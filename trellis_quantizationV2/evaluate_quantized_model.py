#!/usr/bin/env python3
"""
TRELLIS ì–‘ìí™” ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ìê°€ ë¯¸ë¦¬ ì–‘ìí™”í•´ë‘” ì²´í¬í¬ì¸íŠ¸ë“¤ì„ í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python evaluate_quantized_model.py \\
        --model_path quantization_results/trellis_text-base_quantized \\
        --dataset datasets/Toys4k \\
        --CLIP --FD --efficiency \\
        --output_dir evaluation_results \\
        --num_samples 50

ì§€ì› ì˜µì…˜:
    --CLIP: CLIP Score ê³„ì‚° (TRELLIS ë…¼ë¬¸ ë°©ì‹)
    --FD: FrÃ©chet Distance ê³„ì‚° (DINOv2 ê¸°ë°˜)
    --efficiency: íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚° (íŒŒë¼ë¯¸í„°, ë©”ëª¨ë¦¬, ì†ë„)
    --num_samples: í‰ê°€í•  ìƒ˜í”Œ ìˆ˜
    --output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    --report_name: ë³´ê³ ì„œ íŒŒì¼ëª… (ê¸°ë³¸ê°’: ìë™ ìƒì„±)
"""

import os
import sys
import json
import argparse
import tempfile
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

import torch
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm

# TRELLIS ê´€ë ¨ import
try:
    from trellis.pipelines import TrellisImageTo3DPipeline
except ImportError:
    print("âŒ TRELLIS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

# í‰ê°€ ëª¨ë“ˆ import
from metrics_evaluator import get_metrics_evaluator, cleanup_global_evaluator


class QuantizedModelEvaluator:
    """ì–‘ìí™”ëœ TRELLIS ëª¨ë¸ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, dataset_path: str, output_dir: str):
        self.model_path = Path(model_path)
        self.dataset_path = Path(dataset_path) 
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.pipeline = None
        self.evaluator = None
        
        print(f"ğŸ”§ ì–‘ìí™” ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™”")
        print(f"  ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
        print(f"  ë°ì´í„°ì…‹ ê²½ë¡œ: {self.dataset_path}")
        print(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"  ë””ë°”ì´ìŠ¤: {self.device}")
    
    def load_model_config(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì„¤ì • íŒŒì¼ ë¡œë“œ (pipeline.json)"""
        config_path = self.model_path / "pipeline.json"
        
        if not config_path.exists():
            raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print(f"âœ… ëª¨ë¸ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {config_path}")
        return config
    
    def load_pipeline(self) -> TrellisImageTo3DPipeline:
        """ì–‘ìí™”ëœ íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        print(f"ğŸ”§ íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘: {self.model_path}")
        
        try:
            # TRELLIS íŒŒì´í”„ë¼ì¸ ë¡œë“œ (from_pretrained ì‚¬ìš©)
            if self.model_path.is_dir():
                pipeline = TrellisImageTo3DPipeline.from_pretrained(str(self.model_path))
            else:
                raise ValueError(f"ëª¨ë¸ ê²½ë¡œê°€ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {self.model_path}")
            
            pipeline = pipeline.to(self.device)
            print(f"âœ… íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ")
            
            # íŒŒì´í”„ë¼ì¸ êµ¬ì¡° ë¶„ì„
            self.analyze_pipeline_structure(pipeline)
            
            return pipeline
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def analyze_pipeline_structure(self, pipeline):
        """íŒŒì´í”„ë¼ì¸ êµ¬ì¡° ë¶„ì„ ë° ì¶œë ¥"""
        print("\nğŸ“‹ íŒŒì´í”„ë¼ì¸ êµ¬ì¡° ë¶„ì„:")
        
        if hasattr(pipeline, 'models') and isinstance(pipeline.models, dict):
            total_params = 0
            total_size = 0
            
            for model_name, model in pipeline.models.items():
                if model is not None:
                    try:
                        params = sum(p.numel() for p in model.parameters())
                        size = sum(p.numel() * p.element_size() for p in model.parameters())
                        
                        total_params += params
                        total_size += size
                        
                        print(f"  â€¢ {model_name}: {params/1e6:.1f}M íŒŒë¼ë¯¸í„°, {size/(1024*1024):.1f}MB")
                        
                    except Exception as e:
                        print(f"  â€¢ {model_name}: ë¶„ì„ ì‹¤íŒ¨ ({e})")
                else:
                    print(f"  â€¢ {model_name}: None (ë¡œë“œë˜ì§€ ì•ŠìŒ)")
            
            print(f"  ğŸ“Š ì´í•©: {total_params/1e6:.1f}M íŒŒë¼ë¯¸í„°, {total_size/(1024*1024):.1f}MB")
        
        print()
    
    def load_dataset(self, num_samples: int = 100) -> Tuple[List[str], List[Any]]:
        """ë°ì´í„°ì…‹ ë¡œë“œ (Toys4k)"""
        print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {self.dataset_path} (ìƒ˜í”Œ ìˆ˜: {num_samples})")
        
        # Toys4k ë°ì´í„°ì…‹ ì²˜ë¦¬
        if "Toys4k" in str(self.dataset_path):
            return self.load_toys4k_dataset(num_samples)
        else:
            # ê¸°íƒ€ ë°ì´í„°ì…‹ ì²˜ë¦¬ (ì»¤ìŠ¤í…€ êµ¬í˜„)
            return self.load_custom_dataset(num_samples)
    
    def load_toys4k_dataset(self, num_samples: int) -> Tuple[List[str], List[Any]]:
        """Toys4k ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            # dataset_toolkitsì˜ Toys4k ëª¨ë“ˆ í™œìš©
            sys.path.append(str(Path(__file__).parent.parent / "dataset_toolkits"))
            import datasets.Toys4k as Toys4k
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata = Toys4k.get_metadata()
            
            # ëœë¤ ìƒ˜í”Œë§
            if len(metadata) > num_samples:
                metadata = metadata.sample(n=num_samples, random_state=42)
            
            # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„± (íŒŒì¼ëª… ê¸°ë°˜)
            text_prompts = []
            asset_paths = []
            
            for _, row in metadata.iterrows():
                # íŒŒì¼ëª…ì—ì„œ ê°ì²´ëª… ì¶”ì¶œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
                filename = row.get('file_identifier', '')
                object_name = filename.replace('.blend', '').replace('_', ' ')
                prompt = f"a 3D model of {object_name}"
                text_prompts.append(prompt)
                
                # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ (ì¡´ì¬í•  ê²½ìš°)
                if 'local_path' in row:
                    asset_path = self.dataset_path.parent / row['local_path']
                    asset_paths.append(str(asset_path) if asset_path.exists() else None)
                else:
                    asset_paths.append(None)
            
            print(f"âœ… Toys4k ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(text_prompts)}ê°œ ìƒ˜í”Œ")
            return text_prompts, asset_paths
            
        except Exception as e:
            print(f"âš ï¸ Toys4k ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
            return self.generate_default_prompts(num_samples)
    
    def load_custom_dataset(self, num_samples: int) -> Tuple[List[str], List[Any]]:
        """ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ë¡œë“œ"""
        print("ğŸ“‹ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì²˜ë¦¬")
        
        # CSV íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        csv_files = list(self.dataset_path.glob("*.csv"))
        if csv_files:
            try:
                df = pd.read_csv(csv_files[0])
                
                text_prompts = []
                asset_paths = []
                
                for _, row in df.head(num_samples).iterrows():
                    # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
                    text_col = None
                    for col in ['prompt', 'text', 'description', 'caption']:
                        if col in df.columns:
                            text_col = col
                            break
                    
                    if text_col:
                        text_prompts.append(str(row[text_col]))
                    else:
                        text_prompts.append(f"a 3D object {len(text_prompts)}")
                    
                    asset_paths.append(None)  # íŒŒì¼ ê²½ë¡œëŠ” ì¼ë‹¨ None
                
                print(f"âœ… ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(text_prompts)}ê°œ ìƒ˜í”Œ")
                return text_prompts, asset_paths
                
            except Exception as e:
                print(f"âš ï¸ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        return self.generate_default_prompts(num_samples)
    
    def generate_default_prompts(self, num_samples: int) -> Tuple[List[str], List[Any]]:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        default_prompts = [
            "a high quality 3D model",
            "a detailed toy object", 
            "a colorful 3D toy",
            "a realistic miniature model",
            "a professional 3D asset",
            "a well-designed toy figure",
            "a small decorative object",
            "a children's toy model",
            "a collectible figurine",
            "a handcrafted 3D object"
        ]
        
        # ìƒ˜í”Œ ìˆ˜ë§Œí¼ ë°˜ë³µ ìƒì„±
        text_prompts = []
        for i in range(num_samples):
            prompt = default_prompts[i % len(default_prompts)]
            if i >= len(default_prompts):
                prompt += f" variant {i // len(default_prompts) + 1}"
            text_prompts.append(prompt)
        
        asset_paths = [None] * num_samples
        
        print(f"âœ… ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ: {len(text_prompts)}ê°œ ìƒ˜í”Œ")
        return text_prompts, asset_paths
    
    def evaluate_clip_score(self, text_prompts: List[str], num_samples: int) -> Dict[str, float]:
        """CLIP Score í‰ê°€"""
        print(f"ğŸ“ CLIP Score í‰ê°€ ì‹œì‘ (ìƒ˜í”Œ: {num_samples}ê°œ)")
        
        evaluator = get_metrics_evaluator(self.device)
        
        try:
            # í”„ë¡¬í”„íŠ¸ë³„ë¡œ 3D ìƒì„± ë° CLIP í‰ê°€
            generated_assets = []
            successful_prompts = []
            
            for i, prompt in enumerate(tqdm(text_prompts[:num_samples], desc="3D ìƒì„±")):
                try:
                    # ë”ë¯¸ ì´ë¯¸ì§€ ì…ë ¥ (ì‹¤ì œë¡œëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„±)
                    dummy_image = torch.randn(1, 3, 512, 512).to(self.device)
                    
                    with torch.no_grad():
                        output = self.pipeline(dummy_image, num_inference_steps=5)
                    
                    generated_assets.append(output)
                    successful_prompts.append(prompt)
                    
                except Exception as e:
                    print(f"  âš ï¸ ìƒ˜í”Œ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
            
            if len(generated_assets) == 0:
                print("âŒ ìƒì„±ëœ ìì‚°ì´ ì—†ì–´ CLIP í‰ê°€ ë¶ˆê°€")
                return {'clip_score_mean': 0.0, 'clip_score_std': 0.0, 'num_samples': 0}
            
            # TRELLIS ë…¼ë¬¸ ë°©ì‹ CLIP Score ê³„ì‚°
            clip_results = evaluator.compute_clip_score_trellis_paper(
                successful_prompts, generated_assets
            )
            
            print(f"âœ… CLIP Score í‰ê°€ ì™„ë£Œ: í‰ê·  {clip_results['clip_score_mean']:.2f}")
            return clip_results
            
        except Exception as e:
            print(f"âŒ CLIP Score í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'clip_score_mean': 0.0, 'clip_score_std': 0.0, 'num_samples': 0}
    
    def evaluate_frechet_distance(self, text_prompts: List[str], reference_assets: List[Any], 
                                  num_samples: int) -> float:
        """FrÃ©chet Distance í‰ê°€"""
        print(f"ğŸ“ FrÃ©chet Distance í‰ê°€ ì‹œì‘ (ìƒ˜í”Œ: {num_samples}ê°œ)")
        
        evaluator = get_metrics_evaluator(self.device)
        
        try:
            # ìƒì„±ëœ ìì‚°ë“¤
            generated_assets = []
            
            for i, prompt in enumerate(tqdm(text_prompts[:num_samples], desc="FDìš© 3D ìƒì„±")):
                try:
                    dummy_image = torch.randn(1, 3, 512, 512).to(self.device)
                    
                    with torch.no_grad():
                        output = self.pipeline(dummy_image, num_inference_steps=5)
                    
                    generated_assets.append(output)
                    
                except Exception as e:
                    print(f"  âš ï¸ ìƒ˜í”Œ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
            
            if len(generated_assets) == 0:
                print("âŒ ìƒì„±ëœ ìì‚°ì´ ì—†ì–´ FD í‰ê°€ ë¶ˆê°€")
                return 100.0
            
            # ì°¸ì¡° ìì‚°ì´ ì—†ëŠ” ê²½ìš° ìƒì„±ëœ ìì‚° ê°„ ë¹„êµ
            if not reference_assets or len(reference_assets) == 0:
                print("âš ï¸ ì°¸ì¡° ìì‚°ì´ ì—†ì–´ ìƒì„± ìì‚° ë‚´ë¶€ ë‹¤ì–‘ì„±ìœ¼ë¡œ FD ê³„ì‚°")
                mid_point = len(generated_assets) // 2
                reference_assets = generated_assets[:mid_point]
                generated_assets = generated_assets[mid_point:]
            
            # TRELLIS ë…¼ë¬¸ ë°©ì‹ FD ê³„ì‚°
            fd_score = evaluator.compute_frechet_distance_trellis_paper(
                reference_assets, generated_assets
            )
            
            print(f"âœ… FrÃ©chet Distance í‰ê°€ ì™„ë£Œ: {fd_score:.2f}")
            return fd_score
            
        except Exception as e:
            print(f"âŒ FrÃ©chet Distance í‰ê°€ ì‹¤íŒ¨: {e}")
            return 100.0
    
    def evaluate_efficiency(self) -> Dict[str, float]:
        """íš¨ìœ¨ì„± ì§€í‘œ í‰ê°€"""
        print("âš¡ íš¨ìœ¨ì„± ì§€í‘œ í‰ê°€ ì‹œì‘")
        
        evaluator = get_metrics_evaluator(self.device)
        
        try:
            efficiency_metrics = evaluator.compute_efficiency_metrics(
                self.pipeline, "quantized_model"
            )
            
            print("âœ… íš¨ìœ¨ì„± í‰ê°€ ì™„ë£Œ")
            print(f"  íŒŒë¼ë¯¸í„°: {efficiency_metrics.get('parameters_M', 0):.1f}M")
            print(f"  ëª¨ë¸ í¬ê¸°: {efficiency_metrics.get('model_size_MB', 0):.1f}MB") 
            print(f"  GPU ë©”ëª¨ë¦¬: {efficiency_metrics.get('gpu_memory_MB', 0):.1f}MB")
            print(f"  ì¶”ë¡  ì‹œê°„: {efficiency_metrics.get('inference_time_ms', 0):.1f}ms")
            
            return efficiency_metrics
            
        except Exception as e:
            print(f"âŒ íš¨ìœ¨ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return {}
    
    def generate_markdown_report(self, results: Dict[str, Any], 
                                report_name: Optional[str] = None) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±"""
        if report_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = self.model_path.name
            report_name = f"evaluation_report_{model_name}_{timestamp}.md"
        
        report_path = self.output_dir / report_name
        
        # ë³´ê³ ì„œ ì‘ì„±
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# TRELLIS ì–‘ìí™” ëª¨ë¸ í‰ê°€ ë³´ê³ ì„œ\n\n")
            
            # ê¸°ë³¸ ì •ë³´
            f.write(f"## ğŸ“‹ í‰ê°€ ì •ë³´\n\n")
            f.write(f"- **í‰ê°€ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"- **ëª¨ë¸ ê²½ë¡œ**: `{self.model_path}`\n")
            f.write(f"- **ë°ì´í„°ì…‹**: `{self.dataset_path}`\n")
            f.write(f"- **ë””ë°”ì´ìŠ¤**: {self.device}\n\n")
            
            # ëª¨ë¸ êµ¬ì¡°
            if 'model_config' in results:
                config = results['model_config']
                f.write(f"## ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°\n\n")
                f.write(f"```json\n{json.dumps(config, indent=2, ensure_ascii=False)}\n```\n\n")
            
            # íš¨ìœ¨ì„± ì§€í‘œ
            if 'efficiency' in results:
                eff = results['efficiency']
                f.write(f"## âš¡ íš¨ìœ¨ì„± ì§€í‘œ\n\n")
                f.write(f"| ì§€í‘œ | ê°’ |\n")
                f.write(f"|------|----|\n")
                f.write(f"| íŒŒë¼ë¯¸í„° ìˆ˜ | {eff.get('parameters_M', 0):.1f}M |\n")
                f.write(f"| ëª¨ë¸ í¬ê¸° | {eff.get('model_size_MB', 0):.1f}MB |\n")
                f.write(f"| GPU ë©”ëª¨ë¦¬ | {eff.get('gpu_memory_MB', 0):.1f}MB |\n")
                f.write(f"| ì¶”ë¡  ì‹œê°„ | {eff.get('inference_time_ms', 0):.1f}ms |\n\n")
            
            # CLIP Score
            if 'clip_score' in results:
                clip = results['clip_score']
                f.write(f"## ğŸ“ CLIP Score (í…ìŠ¤íŠ¸-3D ì¼ê´€ì„±)\n\n")
                f.write(f"TRELLIS ë…¼ë¬¸ ë°©ì‹ìœ¼ë¡œ ì¸¡ì •ëœ CLIP Score ê²°ê³¼:\n\n")
                f.write(f"- **í‰ê·  ì ìˆ˜**: {clip.get('clip_score_mean', 0):.2f}\n")
                f.write(f"- **í‘œì¤€í¸ì°¨**: {clip.get('clip_score_std', 0):.2f}\n")
                f.write(f"- **ìµœì†Œê°’**: {clip.get('clip_score_min', 0):.2f}\n")
                f.write(f"- **ìµœëŒ€ê°’**: {clip.get('clip_score_max', 0):.2f}\n")
                f.write(f"- **í‰ê°€ ìƒ˜í”Œ ìˆ˜**: {clip.get('num_samples', 0)}ê°œ\n\n")
                
                # ì ìˆ˜ í•´ì„
                score = clip.get('clip_score_mean', 0)
                if score >= 80:
                    interpretation = "ğŸŸ¢ ìš°ìˆ˜ (80+)"
                elif score >= 70:
                    interpretation = "ğŸŸ¡ ì–‘í˜¸ (70-80)"
                elif score >= 60:
                    interpretation = "ğŸŸ  ë³´í†µ (60-70)"
                else:
                    interpretation = "ğŸ”´ ê°œì„  í•„ìš” (<60)"
                
                f.write(f"**í‰ê°€**: {interpretation}\n\n")
            
            # FrÃ©chet Distance  
            if 'frechet_distance' in results:
                fd = results['frechet_distance']
                f.write(f"## ğŸ“ FrÃ©chet Distance (ìƒì„± í’ˆì§ˆ)\n\n")
                f.write(f"DINOv2 ê¸°ë°˜ìœ¼ë¡œ ì¸¡ì •ëœ ìƒì„± í’ˆì§ˆ ë° ë‹¤ì–‘ì„±:\n\n")
                f.write(f"- **FD ì ìˆ˜**: {fd:.2f}\n\n")
                
                # ì ìˆ˜ í•´ì„
                if fd <= 20:
                    interpretation = "ğŸŸ¢ ìš°ìˆ˜ (â‰¤20)"
                elif fd <= 40:
                    interpretation = "ğŸŸ¡ ì–‘í˜¸ (20-40)"
                elif fd <= 60:
                    interpretation = "ğŸŸ  ë³´í†µ (40-60)"
                else:
                    interpretation = "ğŸ”´ ê°œì„  í•„ìš” (>60)"
                
                f.write(f"**í‰ê°€**: {interpretation}\n\n")
            
            # ì¢…í•© í‰ê°€
            f.write(f"## ğŸ¯ ì¢…í•© í‰ê°€\n\n")
            
            if 'efficiency' in results and 'clip_score' in results:
                eff = results['efficiency']
                clip = results['clip_score']
                
                # íš¨ìœ¨ì„±-í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„
                model_size_mb = eff.get('model_size_MB', 0)
                clip_score = clip.get('clip_score_mean', 0)
                
                f.write(f"### íš¨ìœ¨ì„±-í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„\n\n")
                f.write(f"- **ëª¨ë¸ í¬ê¸°**: {model_size_mb:.1f}MB\n")
                f.write(f"- **CLIP ì ìˆ˜**: {clip_score:.2f}\n")
                f.write(f"- **í’ˆì§ˆ/í¬ê¸° ë¹„ìœ¨**: {clip_score/max(model_size_mb/1000, 1):.2f}\n\n")
            
            # ê¶Œì¥ì‚¬í•­
            f.write(f"### ê¶Œì¥ì‚¬í•­\n\n")
            
            if 'clip_score' in results:
                clip_score = results['clip_score'].get('clip_score_mean', 0)
                if clip_score < 70:
                    f.write(f"- ğŸ”´ CLIP Scoreê°€ ë‚®ìŠµë‹ˆë‹¤. ì–‘ìí™” ì •ë„ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ê³ ë ¤í•´ë³´ì„¸ìš”.\n")
                elif clip_score >= 80:
                    f.write(f"- ğŸŸ¢ CLIP Scoreê°€ ìš°ìˆ˜í•©ë‹ˆë‹¤. í˜„ì¬ ì–‘ìí™” ì„¤ì •ì´ ì ì ˆí•©ë‹ˆë‹¤.\n")
            
            if 'efficiency' in results:
                inference_time = results['efficiency'].get('inference_time_ms', 0)
                if inference_time > 2000:
                    f.write(f"- ğŸŸ  ì¶”ë¡  ì‹œê°„ì´ ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì ê·¹ì ì¸ ì–‘ìí™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.\n")
            
            f.write(f"\n---\n")
            f.write(f"*ì´ ë³´ê³ ì„œëŠ” TRELLIS ì–‘ìí™” í‰ê°€ ì‹œìŠ¤í…œì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*\n")
        
        print(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path}")
        return str(report_path)
    
    def run_evaluation(self, metrics: List[str], num_samples: int, 
                      report_name: Optional[str] = None) -> str:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print(f"ğŸš€ ì–‘ìí™” ëª¨ë¸ í‰ê°€ ì‹œì‘")
        print(f"í‰ê°€ ì§€í‘œ: {', '.join(metrics)}")
        print(f"ìƒ˜í”Œ ìˆ˜: {num_samples}")
        print("-" * 50)
        
        results = {}
        
        try:
            # 1. ëª¨ë¸ ë¡œë“œ
            config = self.load_model_config()
            self.pipeline = self.load_pipeline()
            results['model_config'] = config
            
            # 2. ë°ì´í„°ì…‹ ë¡œë“œ
            text_prompts, reference_assets = self.load_dataset(num_samples)
            
            # 3. í‰ê°€ê¸° ì´ˆê¸°í™”
            self.evaluator = get_metrics_evaluator(self.device)
            
            # 4. ê° ì§€í‘œë³„ í‰ê°€
            if 'efficiency' in metrics:
                results['efficiency'] = self.evaluate_efficiency()
            
            if 'CLIP' in metrics:
                results['clip_score'] = self.evaluate_clip_score(text_prompts, num_samples)
            
            if 'FD' in metrics:
                results['frechet_distance'] = self.evaluate_frechet_distance(
                    text_prompts, reference_assets, num_samples
                )
            
            # 5. ê²°ê³¼ ì €ì¥ (JSON)
            results_json_path = self.output_dir / "evaluation_results.json"
            with open(results_json_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            # 6. ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
            report_path = self.generate_markdown_report(results, report_name)
            
            print(f"\nğŸ‰ í‰ê°€ ì™„ë£Œ!")
            print(f"ğŸ“Š JSON ê²°ê³¼: {results_json_path}")
            print(f"ğŸ“„ ë³´ê³ ì„œ: {report_path}")
            
            return report_path
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise
        
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if self.evaluator:
                cleanup_global_evaluator()
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()


def main():
    parser = argparse.ArgumentParser(
        description="TRELLIS ì–‘ìí™” ëª¨ë¸ í‰ê°€ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ í‰ê°€ (ëª¨ë“  ì§€í‘œ)
  python evaluate_quantized_model.py \\
      --model_path quantization_results/trellis_text-base_quantized \\
      --dataset datasets/Toys4k \\
      --CLIP --FD --efficiency
  
  # CLIP Scoreë§Œ í‰ê°€
  python evaluate_quantized_model.py \\
      --model_path my_model \\
      --dataset my_dataset \\
      --CLIP --num_samples 20
        """
    )
    
    parser.add_argument('--model_path', type=str, required=True,
                       help='ì–‘ìí™”ëœ ëª¨ë¸ ê²½ë¡œ (pipeline.jsonì´ ìˆëŠ” ë””ë ‰í† ë¦¬)')
    parser.add_argument('--dataset', type=str, required=True,
                       help='ë°ì´í„°ì…‹ ê²½ë¡œ')
    
    # í‰ê°€ ì§€í‘œ ì„ íƒ
    parser.add_argument('--CLIP', action='store_true',
                       help='CLIP Score ê³„ì‚° (í…ìŠ¤íŠ¸-3D ì¼ê´€ì„±)')
    parser.add_argument('--FD', action='store_true', 
                       help='FrÃ©chet Distance ê³„ì‚° (ìƒì„± í’ˆì§ˆ)')
    parser.add_argument('--efficiency', action='store_true',
                       help='íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚° (íŒŒë¼ë¯¸í„°, ë©”ëª¨ë¦¬, ì†ë„)')
    
    # ê¸°íƒ€ ì˜µì…˜
    parser.add_argument('--num_samples', type=int, default=50,
                       help='í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 50)')
    parser.add_argument('--output_dir', type=str, default='evaluation_results',
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: evaluation_results)')
    parser.add_argument('--report_name', type=str, default=None,
                       help='ë³´ê³ ì„œ íŒŒì¼ëª… (ê¸°ë³¸ê°’: ìë™ ìƒì„±)')
    
    args = parser.parse_args()
    
    # í‰ê°€ ì§€í‘œ í™•ì¸
    metrics = []
    if args.CLIP:
        metrics.append('CLIP')
    if args.FD:
        metrics.append('FD') 
    if args.efficiency:
        metrics.append('efficiency')
    
    if not metrics:
        print("âŒ ìµœì†Œ í•˜ë‚˜ì˜ í‰ê°€ ì§€í‘œë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. (--CLIP, --FD, --efficiency)")
        sys.exit(1)
    
    # í‰ê°€ ì‹¤í–‰
    try:
        evaluator = QuantizedModelEvaluator(
            model_path=args.model_path,
            dataset_path=args.dataset, 
            output_dir=args.output_dir
        )
        
        report_path = evaluator.run_evaluation(
            metrics=metrics,
            num_samples=args.num_samples,
            report_name=args.report_name
        )
        
        print(f"\nâœ… í‰ê°€ ì™„ë£Œ! ë³´ê³ ì„œ: {report_path}")
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()