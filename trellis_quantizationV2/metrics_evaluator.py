"""
TRELLIS ì–‘ìí™” ì‹¤í—˜ì„ ìœ„í•œ ê³µí†µ í‰ê°€ ì§€í‘œ ëª¨ë“ˆ (TRELLIS ë…¼ë¬¸ ë°©ë²•ë¡  ì ìš©)

ì´ ëª¨ë“ˆì€ TRELLIS ë…¼ë¬¸ì˜ í‰ê°€ ë°©ë²•ë¡ ì— ë”°ë¼ ë‹¤ìŒ ì§€í‘œë¥¼ ì œê³µí•©ë‹ˆë‹¤:

1. CLIP Score (TRELLIS ë…¼ë¬¸ ë°©ì‹):
   - ê° ìƒì„±ëœ 3D ìì‚°ì— ëŒ€í•´ 8ê°œ ì´ë¯¸ì§€ ë Œë”ë§ (Yaw 45ë„ ê°„ê²©, Pitch 30Â°, ë°˜ê²½ 2)
   - ë Œë”ë§ëœ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì˜ CLIP ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
   - í‰ê·  ìœ ì‚¬ë„ì— 100ì„ ê³±í•˜ì—¬ ì ìˆ˜í™”

2. FrÃ©chet Distance (FD) with DINOv2:
   - ì°¸ì¡° ë°ì´í„°ì…‹ê³¼ ìƒì„±ëœ ìì‚°ë“¤ì˜ DINOv2 íŠ¹ì§• ë¶„í¬ ë¹„êµ
   - ì‹¤ì œ ë°ì´í„°ì™€ ìƒì„± ë°ì´í„°ì˜ í’ˆì§ˆ ë° ë‹¤ì–‘ì„± í‰ê°€

3. íš¨ìœ¨ì„± ì§€í‘œ: íŒŒë¼ë¯¸í„° ìˆ˜, ëª¨ë¸ í¬ê¸°, GPU ë©”ëª¨ë¦¬, ì¶”ë¡  ì‹œê°„

ì‚¬ìš©ë²•:
    evaluator = MetricsEvaluator()
    clip_score = evaluator.compute_clip_score_trellis_paper(text_prompts, generated_3d_outputs)
    fd_score = evaluator.compute_frechet_distance_trellis_paper(reference_assets, generated_assets)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import psutil
import tempfile
from typing import List, Dict, Tuple, Any, Union
from PIL import Image
from transformers import CLIPTextModel, CLIPVisionModel, CLIPProcessor, AutoTokenizer
from torchvision import transforms
from scipy.linalg import sqrtm
from pathlib import Path
from .trellis_render_utils import (
    render_pipeline_output_trellis_paper,
    load_rendered_images,
    create_trellis_paper_views
)


class MetricsEvaluator:
    """TRELLIS ì–‘ìí™” ì‹¤í—˜ì„ ìœ„í•œ ê³µí†µ í‰ê°€ ì§€í‘œ í´ë˜ìŠ¤"""
    
    def __init__(self, device: str = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # ëª¨ë¸ë“¤ì„ lazy loadingìœ¼ë¡œ ì´ˆê¸°í™”
        self._clip_text_model = None
        self._clip_vision_model = None
        self._clip_processor = None
        self._clip_tokenizer = None
        self._dinov2_model = None
        self._dinov2_transform = None
        # ë Œë”ë§ ìœ í‹¸ë¦¬í‹°ëŠ” í•¨ìˆ˜ë¡œ ì§ì ‘ ì‚¬ìš©
        
        print(f"ğŸ“Š MetricsEvaluator ì´ˆê¸°í™”ë¨ (device: {self.device})")
    
    def _init_clip_model(self, clip_model_name: str = "openai/clip-vit-large-patch14"):
        """CLIP ëª¨ë¸ ì´ˆê¸°í™” (í…ìŠ¤íŠ¸ì™€ ë¹„ì „ ëª¨ë¸ ëª¨ë‘)"""
        if self._clip_text_model is None or self._clip_vision_model is None:
            print(f"ğŸ”§ CLIP ëª¨ë¸ ë¡œë”© ì¤‘: {clip_model_name}")
            
            # CLIP í…ìŠ¤íŠ¸ ë° ë¹„ì „ ëª¨ë¸ ë¡œë“œ
            self._clip_text_model = CLIPTextModel.from_pretrained(clip_model_name)
            self._clip_vision_model = CLIPVisionModel.from_pretrained(clip_model_name)
            self._clip_processor = CLIPProcessor.from_pretrained(clip_model_name)
            self._clip_tokenizer = AutoTokenizer.from_pretrained(clip_model_name)
            
            self._clip_text_model.eval()
            self._clip_vision_model.eval()
            self._clip_text_model = self._clip_text_model.to(self.device)
            self._clip_vision_model = self._clip_vision_model.to(self.device)
            
            print("âœ… CLIP ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def _init_dinov2_model(self, model_name: str = "dinov2_vitl14"):
        """DINOv2 ëª¨ë¸ ì´ˆê¸°í™” (extract_feature.py ë°©ì‹ ì°¸ê³ )"""
        if self._dinov2_model is None:
            print(f"ğŸ”§ DINOv2 ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
            
            # extract_feature.pyì˜ ë°©ì‹ì„ ì°¸ê³ 
            self._dinov2_model = torch.hub.load('facebookresearch/dinov2', model_name)
            self._dinov2_model.eval()
            self._dinov2_model = self._dinov2_model.to(self.device)
            
            # DINOv2ìš© ì „ì²˜ë¦¬ (extract_feature.py ì°¸ê³ )
            self._dinov2_transform = transforms.Compose([
                transforms.Resize((518, 518)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
            print("âœ… DINOv2 ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    # ë Œë”ë§ì€ trellis_render_utils í•¨ìˆ˜ ì§ì ‘ ì‚¬ìš©
    
    def encode_text_clip(self, text_list: List[str]) -> torch.Tensor:
        """í…ìŠ¤íŠ¸ë¥¼ CLIPìœ¼ë¡œ ì¸ì½”ë”© (ì •ê·œí™”ëœ íŠ¹ì§• ë²¡í„° ë°˜í™˜)"""
        self._init_clip_model()
        
        encoding = self._clip_tokenizer(
            text_list, 
            max_length=77, 
            padding='max_length', 
            truncation=True, 
            return_tensors='pt'
        )
        tokens = encoding['input_ids'].to(self.device)
        
        with torch.no_grad():
            text_features = self._clip_text_model(input_ids=tokens).pooler_output
            # L2 ì •ê·œí™”
            text_features = F.normalize(text_features, p=2, dim=-1)
        
        return text_features
    
    def encode_images_clip(self, images: List[Image.Image]) -> torch.Tensor:
        """ì´ë¯¸ì§€ë“¤ì„ CLIPìœ¼ë¡œ ì¸ì½”ë”© (ì •ê·œí™”ëœ íŠ¹ì§• ë²¡í„° ë°˜í™˜)"""
        self._init_clip_model()
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        inputs = self._clip_processor(images=images, return_tensors="pt")
        pixel_values = inputs['pixel_values'].to(self.device)
        
        with torch.no_grad():
            image_features = self._clip_vision_model(pixel_values=pixel_values).pooler_output
            # L2 ì •ê·œí™”
            image_features = F.normalize(image_features, p=2, dim=-1)
        
        return image_features
    
    def extract_dinov2_features(self, images: List[Union[torch.Tensor, Image.Image]]) -> torch.Tensor:
        """ì´ë¯¸ì§€ì—ì„œ DINOv2 íŠ¹ì§• ì¶”ì¶œ"""
        self._init_dinov2_model()
        
        if not images:
            raise ValueError("ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        processed_images = []
        for img in images:
            if isinstance(img, torch.Tensor):
                # ì´ë¯¸ tensorì¸ ê²½ìš°
                if img.dim() == 4:  # batch dimensionì´ ìˆëŠ” ê²½ìš°
                    img = img.squeeze(0)
                if img.shape[0] == 3:  # CHW í˜•ì‹
                    img = transforms.ToPILImage()(img)
                else:  # HWC í˜•ì‹
                    img = transforms.ToPILImage()(img.permute(2, 0, 1))
            
            # PIL Imageë¡œ ë³€í™˜ í›„ ì „ì²˜ë¦¬
            if isinstance(img, Image.Image):
                processed_img = self._dinov2_transform(img)
                processed_images.append(processed_img)
        
        if not processed_images:
            raise ValueError("ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë°°ì¹˜ë¡œ ë¬¶ê¸°
        batch_tensor = torch.stack(processed_images).to(self.device)
        
        with torch.no_grad():
            features = self._dinov2_model(batch_tensor)
        
        return features
    
    def compute_clip_score_trellis_paper(self, text_prompts: List[str], generated_3d_assets: List[Any], 
                                        temp_dir: str = None) -> Dict[str, float]:
        """
        TRELLIS ë…¼ë¬¸ ë°©ì‹ì˜ CLIP Score ê³„ì‚°
        
        ì¸¡ì • ë°©ì‹:
        1. ê° ìƒì„±ëœ 3D ìì‚°ì— ëŒ€í•´ 8ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë Œë”ë§
           - ì¹´ë©”ë¼ ì„¤ì •: Yaw 45ë„ ê°„ê²©(0Â°, 45Â°, ..., 315Â°), Pitch 30Â°, ë°˜ê²½ 2
        2. ë Œë”ë§ëœ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì˜ CLIP íŠ¹ì§• ì¶”ì¶œ
        3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í›„ í‰ê· í•˜ê³  100ì„ ê³±í•´ì„œ ì ìˆ˜í™”
        
        Args:
            text_prompts: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            generated_3d_assets: ìƒì„±ëœ 3D ìì‚°ë“¤ (íŒŒì´í”„ë¼ì¸ ì¶œë ¥)
            temp_dir: ì„ì‹œ ë””ë ‰í† ë¦¬ (ë Œë”ë§ ê²°ê³¼ ì €ì¥)
        
        Returns:
            Dict: CLIP ì ìˆ˜ í†µê³„
        """
        try:
            print("ğŸ“ TRELLIS ë…¼ë¬¸ ë°©ì‹ CLIP Score ê³„ì‚° ì¤‘...")
            
            if temp_dir is None:
                temp_dir = tempfile.mkdtemp()
            
            self._init_renderer()
            
            all_clip_scores = []
            
            for i, (prompt, asset) in enumerate(zip(text_prompts, generated_3d_assets)):
                print(f"  ìì‚° {i+1}/{len(text_prompts)} ì²˜ë¦¬ ì¤‘...")
                
                try:
                    # 1. 3D ìì‚°ì„ 8ê°œ ë·°ë¡œ ë Œë”ë§
                    asset_output_dir = f"{temp_dir}/asset_{i}"
                    rendered_images = render_pipeline_output_trellis_paper(
                        asset, asset_output_dir, f"asset_{i}"
                    )
                    
                    if len(rendered_images) == 0:
                        print(f"    âš ï¸ ìì‚° {i+1} ë Œë”ë§ ì‹¤íŒ¨")
                        continue
                    
                    # ì´ë¯¸ì§€ ë¡œë“œ
                    images = load_rendered_images(rendered_images)
                    
                    if len(images) == 0:
                        print(f"    âš ï¸ ìì‚° {i+1} ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
                        continue
                    
                    # 2. CLIP íŠ¹ì§• ì¶”ì¶œ
                    text_features = self.encode_text_clip([prompt])  # [1, feature_dim]
                    image_features = self.encode_images_clip(images)  # [N_views, feature_dim]
                    
                    # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    similarities = torch.matmul(image_features, text_features.T)  # [N_views, 1]
                    similarities = similarities.squeeze(-1)  # [N_views]
                    
                    # 4. ëª¨ë“  ë·°ì˜ ìœ ì‚¬ë„ í‰ê·  ê³„ì‚°
                    avg_similarity = similarities.mean().item()
                    
                    # 5. TRELLIS ë…¼ë¬¸ ë°©ì‹: 100ì„ ê³±í•´ì„œ ì ìˆ˜í™”
                    clip_score = avg_similarity * 100
                    all_clip_scores.append(clip_score)
                    
                    print(f"    ìì‚° {i+1} CLIP ì ìˆ˜: {clip_score:.2f} ({len(images)}ê°œ ë·°)")
                    
                except Exception as e:
                    print(f"    âš ï¸ ìì‚° {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            if len(all_clip_scores) == 0:
                print("âŒ ëª¨ë“  ìì‚° ì²˜ë¦¬ ì‹¤íŒ¨")
                return {
                    'clip_score_mean': 0.0,
                    'clip_score_std': 0.0,
                    'clip_score_min': 0.0,
                    'clip_score_max': 0.0,
                    'num_samples': 0
                }
            
            # í†µê³„ ê³„ì‚°
            results = {
                'clip_score_mean': np.mean(all_clip_scores),
                'clip_score_std': np.std(all_clip_scores),
                'clip_score_min': np.min(all_clip_scores),
                'clip_score_max': np.max(all_clip_scores),
                'num_samples': len(all_clip_scores)
            }
            
            print(f"âœ… CLIP Score ê³„ì‚° ì™„ë£Œ: í‰ê·  {results['clip_score_mean']:.2f} ({results['num_samples']}ê°œ ìƒ˜í”Œ)")
            return results
            
        except Exception as e:
            print(f"âŒ CLIP Score ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'clip_score_mean': 0.0,
                'clip_score_std': 0.0,
                'clip_score_min': 0.0,
                'clip_score_max': 0.0,
                'num_samples': 0
            }
    
    def compute_frechet_distance(self, real_features: torch.Tensor, generated_features: torch.Tensor) -> float:
        """
        FrÃ©chet Distance ê³„ì‚°
        
        Args:
            real_features: ì‹¤ì œ ì´ë¯¸ì§€ì˜ DINOv2 íŠ¹ì§•
            generated_features: ìƒì„±ëœ ì´ë¯¸ì§€ì˜ DINOv2 íŠ¹ì§•
            
        Returns:
            float: FrÃ©chet Distance ê°’
        """
        try:
            print("ğŸ“ FrÃ©chet Distance ê³„ì‚° ì¤‘...")
            
            # CPUë¡œ ì´ë™
            real_features = real_features.cpu().numpy()
            generated_features = generated_features.cpu().numpy()
            
            # í‰ê· ê³¼ ê³µë¶„ì‚° ê³„ì‚°
            mu_real = np.mean(real_features, axis=0)
            mu_gen = np.mean(generated_features, axis=0)
            
            sigma_real = np.cov(real_features, rowvar=False)
            sigma_gen = np.cov(generated_features, rowvar=False)
            
            # FrÃ©chet Distance ê³„ì‚°
            diff = mu_real - mu_gen
            covmean = sqrtm(sigma_real.dot(sigma_gen))
            
            # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ì²˜ë¦¬
            if np.iscomplexobj(covmean):
                covmean = covmean.real
            
            fd_score = diff.dot(diff) + np.trace(sigma_real + sigma_gen - 2 * covmean)
            
            return float(fd_score)
            
        except Exception as e:
            print(f"âš ï¸ FrÃ©chet Distance ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 100.0  # ë†’ì€ ê°’ ë°˜í™˜ (ë‚˜ìœ í’ˆì§ˆ ì˜ë¯¸)
    
    def compute_frechet_distance_trellis_paper(self, reference_3d_assets: List[Any], 
                                              generated_3d_assets: List[Any],
                                              temp_dir: str = None) -> float:
        """
        TRELLIS ë…¼ë¬¸ ë°©ì‹ì˜ FrÃ©chet Distance ê³„ì‚°
        
        ì¸¡ì • ë°©ì‹:
        1. ì°¸ì¡° ë°ì´í„°ì…‹ê³¼ ìƒì„±ëœ 3D ìì‚°ë“¤ì„ ê°ê° ë Œë”ë§
        2. DINOv2ë¡œ íŠ¹ì§• ì¶”ì¶œ 
        3. ë‘ ë¶„í¬ ê°„ì˜ FrÃ©chet Distance ê³„ì‚°
        
        Args:
            reference_3d_assets: ì°¸ì¡° 3D ìì‚°ë“¤ (ì‹¤ì œ ë°ì´í„°)
            generated_3d_assets: ìƒì„±ëœ 3D ìì‚°ë“¤
            temp_dir: ì„ì‹œ ë””ë ‰í† ë¦¬
            
        Returns:
            float: FrÃ©chet Distance ê°’
        """
        try:
            print("ğŸ“ TRELLIS ë…¼ë¬¸ ë°©ì‹ FrÃ©chet Distance ê³„ì‚° ì¤‘...")
            
            if temp_dir is None:
                temp_dir = tempfile.mkdtemp()
            
            self._init_renderer()
            
            # 1. ì°¸ì¡° ë°ì´í„° ë Œë”ë§ ë° íŠ¹ì§• ì¶”ì¶œ
            print("  ì°¸ì¡° ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
            reference_images = []
            for i, asset in enumerate(reference_3d_assets):
                asset_output_dir = f"{temp_dir}/ref_{i}"
                rendered_images = render_pipeline_output_trellis_paper(
                    asset, asset_output_dir, f"ref_{i}"
                )
                images = load_rendered_images(rendered_images)
                reference_images.extend(images)
            
            # 2. ìƒì„±ëœ ë°ì´í„° ë Œë”ë§ ë° íŠ¹ì§• ì¶”ì¶œ
            print("  ìƒì„±ëœ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
            generated_images = []
            for i, asset in enumerate(generated_3d_assets):
                asset_output_dir = f"{temp_dir}/gen_{i}"
                rendered_images = render_pipeline_output_trellis_paper(
                    asset, asset_output_dir, f"ref_{i}"
                )
                images = load_rendered_images(rendered_images)
                generated_images.extend(images)
            
            # 3. DINOv2 íŠ¹ì§• ì¶”ì¶œ
            if len(reference_images) == 0 or len(generated_images) == 0:
                print("âŒ ë Œë”ë§ëœ ì´ë¯¸ì§€ê°€ ì—†ìŒ")
                return 100.0
                
            reference_features = self.extract_dinov2_features(reference_images)
            generated_features = self.extract_dinov2_features(generated_images)
            
            # 4. FrÃ©chet Distance ê³„ì‚°
            fd_score = self.compute_frechet_distance(reference_features, generated_features)
            
            print(f"âœ… FrÃ©chet Distance ê³„ì‚° ì™„ë£Œ: {fd_score:.2f}")
            print(f"  ì°¸ì¡° ì´ë¯¸ì§€: {len(reference_images)}ê°œ, ìƒì„± ì´ë¯¸ì§€: {len(generated_images)}ê°œ")
            
            return fd_score
            
        except Exception as e:
            print(f"âŒ FrÃ©chet Distance ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 100.0
    
    def compute_frechet_distance_from_images(self, real_images: List[Image.Image], 
                                           generated_images: List[Image.Image]) -> float:
        """
        ì´ë¯¸ì§€ë¡œë¶€í„° FrÃ©chet Distance ê³„ì‚° (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        
        Args:
            real_images: ì‹¤ì œ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸  
            generated_images: ìƒì„±ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            float: FrÃ©chet Distance ê°’
        """
        try:
            # DINOv2 íŠ¹ì§• ì¶”ì¶œ
            real_features = self.extract_dinov2_features(real_images)
            generated_features = self.extract_dinov2_features(generated_images)
            
            # FD ê³„ì‚°
            return self.compute_frechet_distance(real_features, generated_features)
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ê¸°ë°˜ FrÃ©chet Distance ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 100.0
    
    def compute_efficiency_metrics(self, pipeline, model_name: str) -> Dict[str, float]:
        """íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚°"""
        print(f"ğŸ“Š {model_name} íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚° ì¤‘...")
        
        metrics = {}
        
        try:
            # 1. íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            total_params = 0
            if hasattr(pipeline, 'models') and pipeline.models:
                for name, module in pipeline.models.items():
                    if module is not None:
                        params = sum(p.numel() for p in module.parameters())
                        total_params += params
            
            metrics['parameters_M'] = total_params / 1e6
            
            # 2. ëª¨ë¸ í¬ê¸° ê³„ì‚° (MB)
            total_size = 0
            if hasattr(pipeline, 'models') and pipeline.models:
                for name, module in pipeline.models.items():
                    if module is not None:
                        size = sum(p.numel() * p.element_size() for p in module.parameters())
                        total_size += size
            
            metrics['model_size_MB'] = total_size / (1024 * 1024)
            
            # 3. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
                
                try:
                    # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
                    dummy_image = torch.randn(1, 3, 512, 512).to(self.device)
                    
                    with torch.no_grad():
                        pipeline(dummy_image, num_inference_steps=1)
                    
                    metrics['gpu_memory_MB'] = torch.cuda.max_memory_allocated() / (1024 * 1024)
                    
                except Exception as e:
                    print(f"    âš ï¸ GPU ë©”ëª¨ë¦¬ ì¸¡ì • ì‹¤íŒ¨: {e}")
                    metrics['gpu_memory_MB'] = 0
            else:
                metrics['gpu_memory_MB'] = 0
            
            # 4. ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            inference_times = []
            num_runs = 3
            
            try:
                dummy_image = torch.randn(1, 3, 512, 512).to(self.device)
                
                # ì›Œë°ì—…
                with torch.no_grad():
                    pipeline(dummy_image, num_inference_steps=1)
                
                # ì‹¤ì œ ì¸¡ì •
                for _ in range(num_runs):
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                    
                    start_time = time.time()
                    
                    with torch.no_grad():
                        pipeline(dummy_image, num_inference_steps=1)
                    
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                    
                    end_time = time.time()
                    inference_times.append((end_time - start_time) * 1000)  # ms
                
                metrics['inference_time_ms'] = np.mean(inference_times)
                metrics['inference_time_std'] = np.std(inference_times)
                
            except Exception as e:
                print(f"    âš ï¸ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì‹¤íŒ¨: {e}")
                metrics['inference_time_ms'] = 0
                metrics['inference_time_std'] = 0
            
            return metrics
            
        except Exception as e:
            print(f"âŒ íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {
                'parameters_M': 0,
                'model_size_MB': 0,
                'gpu_memory_MB': 0,
                'inference_time_ms': 0,
                'inference_time_std': 0
            }
    
    def evaluate_pipeline_quality_trellis_paper(self, pipeline, text_prompts: List[str], 
                                               reference_assets: List[Any] = None,
                                               num_samples: int = 5, temp_dir: str = None) -> Dict[str, float]:
        """
        TRELLIS ë…¼ë¬¸ ë°©ì‹ì˜ íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í‰ê°€
        
        Args:
            pipeline: TRELLIS íŒŒì´í”„ë¼ì¸
            text_prompts: í‰ê°€ìš© í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
            reference_assets: ì°¸ì¡° 3D ìì‚°ë“¤ (FD ê³„ì‚°ìš©)
            num_samples: ìƒ˜í”Œ ìˆ˜
            temp_dir: ì„ì‹œ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict: í’ˆì§ˆ ì§€í‘œë“¤
        """
        print(f"ğŸ¯ TRELLIS ë…¼ë¬¸ ë°©ì‹ íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í‰ê°€ ì¤‘ (ìƒ˜í”Œ: {num_samples}ê°œ)...")
        
        if temp_dir is None:
            temp_dir = tempfile.mkdtemp()
        
        try:
            generated_assets = []
            used_prompts = []
            
            # ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ìƒì„±
            for i, prompt in enumerate(text_prompts[:num_samples]):
                print(f"  ìƒ˜í”Œ {i+1}/{min(num_samples, len(text_prompts))}: '{prompt[:50]}...'")
                
                try:
                    dummy_image = torch.randn(1, 3, 512, 512).to(self.device)
                    
                    with torch.no_grad():
                        output = pipeline(dummy_image, num_inference_steps=5)
                    
                    generated_assets.append(output)
                    used_prompts.append(prompt)
                    
                except Exception as e:
                    print(f"    âš ï¸ ìƒ˜í”Œ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
            
            if len(generated_assets) == 0:
                print("âŒ ìƒì„±ëœ ìì‚°ì´ ì—†ìŒ")
                return {
                    'clip_score_mean': 0.0,
                    'clip_score_std': 0.0,
                    'frechet_distance': 100.0
                }
            
            results = {}
            
            # 1. TRELLIS ë…¼ë¬¸ ë°©ì‹ CLIP Score ê³„ì‚°
            try:
                clip_results = self.compute_clip_score_trellis_paper(
                    used_prompts, generated_assets, temp_dir
                )
                results.update(clip_results)
            except Exception as e:
                print(f"âš ï¸ CLIP Score ê³„ì‚° ê±´ë„ˆë›°ê¸°: {e}")
                results.update({
                    'clip_score_mean': 0.0,
                    'clip_score_std': 0.0
                })
            
            # 2. TRELLIS ë…¼ë¬¸ ë°©ì‹ FrÃ©chet Distance ê³„ì‚°
            if reference_assets and len(reference_assets) > 0:
                try:
                    fd_score = self.compute_frechet_distance_trellis_paper(
                        reference_assets, generated_assets, temp_dir
                    )
                    results['frechet_distance'] = fd_score
                except Exception as e:
                    print(f"âš ï¸ FD ê³„ì‚° ê±´ë„ˆë›°ê¸°: {e}")
                    results['frechet_distance'] = 100.0
            else:
                print("âš ï¸ ì°¸ì¡° ìì‚°ì´ ì—†ì–´ FD ê³„ì‚° ê±´ë„ˆë›°ê¸°")
                results['frechet_distance'] = 100.0
            
            return results
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                'clip_score_mean': 0.0,
                'clip_score_std': 0.0,
                'frechet_distance': 100.0
            }
    
    def evaluate_pipeline_quality(self, pipeline, text_prompts: List[str], 
                                num_samples: int = 5) -> Dict[str, float]:
        """
        íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ í’ˆì§ˆ í‰ê°€ (ê¸°ì¡´ ë°©ì‹, í˜¸í™˜ì„± ìœ ì§€)
        
        Args:
            pipeline: TRELLIS íŒŒì´í”„ë¼ì¸
            text_prompts: í‰ê°€ìš© í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
            num_samples: ìƒ˜í”Œ ìˆ˜
            
        Returns:
            Dict: í’ˆì§ˆ ì§€í‘œë“¤
        """
        # TRELLIS ë…¼ë¬¸ ë°©ì‹ ì‚¬ìš©
        return self.evaluate_pipeline_quality_trellis_paper(
            pipeline, text_prompts, num_samples=num_samples
        )
    
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        print("ğŸ§¹ MetricsEvaluator ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        
        if self._clip_text_model is not None:
            del self._clip_text_model
            self._clip_text_model = None
        
        if self._clip_vision_model is not None:
            del self._clip_vision_model
            self._clip_vision_model = None
        
        if self._clip_processor is not None:
            del self._clip_processor
            self._clip_processor = None
        
        if self._clip_tokenizer is not None:
            del self._clip_tokenizer
            self._clip_tokenizer = None
        
        if self._dinov2_model is not None:
            del self._dinov2_model
            self._dinov2_model = None
        
        if self._dinov2_transform is not None:
            del self._dinov2_transform
            self._dinov2_transform = None
        
        # ë Œë”ë§ ìœ í‹¸ë¦¬í‹°ëŠ” í•¨ìˆ˜ì´ë¯€ë¡œ ì •ë¦¬ ë¶ˆí•„ìš”
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


# ì „ì—­ í‰ê°€ê¸° ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_global_evaluator = None

def get_metrics_evaluator(device: str = None) -> MetricsEvaluator:
    """ì „ì—­ MetricsEvaluator ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _global_evaluator
    
    if _global_evaluator is None:
        _global_evaluator = MetricsEvaluator(device)
    
    return _global_evaluator


def cleanup_global_evaluator():
    """ì „ì—­ í‰ê°€ê¸° ì •ë¦¬"""
    global _global_evaluator
    
    if _global_evaluator is not None:
        _global_evaluator.cleanup()
        _global_evaluator = None