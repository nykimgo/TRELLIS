"""
TRELLIS ëª¨ë¸ ì–‘ìí™” ëª¨ë“ˆ (ê°œì„ ëœ ë²„ì „)

TDD ë°©ì‹ìœ¼ë¡œ ê°œë°œëœ ê³ í’ˆì§ˆ ì–‘ìí™” ì‹œìŠ¤í…œ
- ë‹¤ì–‘í•œ TRELLIS ëª¨ë¸ ì§€ì›
- ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •
- ê²¬ê³ í•œ ì—ëŸ¬ ì²˜ë¦¬
- í’ˆì§ˆ ê²€ì¦ ê¸°ëŠ¥
"""

import os
import sys
import torch
import torch.nn as nn
import time
import psutil
import gc
from typing import Dict, Any, List, Optional, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import numpy as np
from PIL import Image
import json
import warnings
from pathlib import Path


class TRELLISQuantizationManager:
    """ê°œì„ ëœ TRELLIS ì–‘ìí™” ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    # ì§€ì›ë˜ëŠ” TRELLIS ëª¨ë¸ë“¤
    SUPPORTED_MODELS = {
        "text-base": "microsoft/TRELLIS-text-base",
        "text-large": "microsoft/TRELLIS-text-large", 
        "text-xlarge": "microsoft/TRELLIS-text-xlarge",
        "image-large": "microsoft/TRELLIS-image-large"
    }
    
    # ì–‘ìí™” ëŒ€ìƒ ë ˆì´ì–´ íƒ€ì…
    QUANTIZABLE_LAYERS = {
        nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, 
        nn.ConvTranspose3d, nn.MultiheadAttention
    }
    
    def __init__(self, model_name: str = "text-large", output_dir: str = "quantization_results"):
        """
        ì–‘ìí™” ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            model_name (str): ëª¨ë¸ ì´ë¦„ (text-base, text-large, text-xlarge, image-large)
            output_dir (str): ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.model_name = model_name
        self.model_path = self.SUPPORTED_MODELS.get(model_name, model_name)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.original_pipeline = None
        self.quantized_pipeline = None
        self.results = []
        
        # í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸
        self.test_prompts = [
            "a red sports car",
            "a wooden chair", 
            "a blue coffee mug"
        ]
        
        # í™˜ê²½ ì„¤ì •
        os.environ['SPCONV_ALGO'] = 'native'
        os.environ['ATTN_BACKEND'] = 'xformers'
        
        print(f"ğŸ”§ TRELLIS ì–‘ìí™” ë§¤ë‹ˆì € ì´ˆê¸°í™”")
        print(f"  - ëª¨ë¸: {self.model_path}")
        print(f"  - ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
    
    def load_original_model(self) -> bool:
        """
        ì›ë³¸ ëª¨ë¸ ë¡œë“œ (ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬)
        
        Returns:
            bool: ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"ğŸ”„ ì›ë³¸ TRELLIS íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘...")
            
            # TRELLIS ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œë„
            try:
                if "text" in self.model_name:
                    from trellis.pipelines import TrellisTextTo3DPipeline
                    pipeline_class = TrellisTextTo3DPipeline
                else:
                    from trellis.pipelines import TrellisImageTo3DPipeline  
                    pipeline_class = TrellisImageTo3DPipeline
            except ImportError as e:
                print(f"âŒ TRELLIS ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ í•´ê²° ë°©ë²•: TRELLIS í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ê±°ë‚˜ PYTHONPATH ì„¤ì •")
                return False
            
            # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
            self.original_pipeline = pipeline_class.from_pretrained(self.model_path)
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPUë¡œ ì´ë™ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPU ìœ ì§€)
            if torch.cuda.is_available():
                try:
                    self.original_pipeline.cuda()
                    print("âœ… ëª¨ë¸ì„ GPUë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤")
                except torch.cuda.OutOfMemoryError:
                    print("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ CPUì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤")
                    self.original_pipeline.cpu()
            else:
                print("â„¹ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPUì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤")
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            self._print_model_info(self.original_pipeline, "ì›ë³¸")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def _print_model_info(self, pipeline, model_type: str):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        print(f"\nğŸ“‹ {model_type} ëª¨ë¸ êµ¬ì¡°:")
        total_params = 0
        
        if hasattr(pipeline, 'models'):
            for name, model in pipeline.models.items():
                param_count = sum(p.numel() for p in model.parameters())
                total_params += param_count
                print(f"  - {name}: {param_count/1e6:.1f}M íŒŒë¼ë¯¸í„°")
        else:
            # ë‹¨ì¼ ëª¨ë¸ì¸ ê²½ìš°
            total_params = sum(p.numel() for p in pipeline.parameters())
            print(f"  - ì´ íŒŒë¼ë¯¸í„°: {total_params/1e6:.1f}M")
        
        print(f"  ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {total_params/1e6:.1f}M")
    
    def apply_quantization(self, quantization_type: str = "dynamic") -> bool:
        """
        ì–‘ìí™” ì ìš© (ê°œì„ ëœ ê²€ì¦ ê¸°ëŠ¥)
        
        Args:
            quantization_type (str): ì–‘ìí™” ë°©ì‹ ("dynamic", "static")
            
        Returns:
            bool: ì–‘ìí™” ì„±ê³µ ì—¬ë¶€
        """
        print(f"\nğŸ”§ {quantization_type} 8-bit ì–‘ìí™” ì ìš© ì¤‘...")
        
        if self.original_pipeline is None:
            print("âŒ ì›ë³¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            # ê¹Šì€ ë³µì‚¬ë¡œ ìƒˆ íŒŒì´í”„ë¼ì¸ ìƒì„±
            import copy
            self.quantized_pipeline = copy.deepcopy(self.original_pipeline)
            
            # ì–‘ìí™” ê²°ê³¼ ì¶”ì 
            quantization_results = {}
            
            if hasattr(self.quantized_pipeline, 'models'):
                # ë‹¤ì¤‘ ëª¨ë¸ íŒŒì´í”„ë¼ì¸
                for name, model in self.quantized_pipeline.models.items():
                    print(f"  ğŸ”§ {name} ëª¨ë¸ ì–‘ìí™” ì¤‘...")
                    
                    # CPUë¡œ ì´ë™ (ì–‘ìí™”ë¥¼ ìœ„í•´)
                    original_device = next(model.parameters()).device
                    model.cpu()
                    
                    # ì–‘ìí™” ì ìš©
                    try:
                        if quantization_type == "dynamic":
                            quantized_model = torch.quantization.quantize_dynamic(
                                model, self.QUANTIZABLE_LAYERS, dtype=torch.qint8
                            )
                        else:
                            # Static quantization (í–¥í›„ êµ¬í˜„)
                            quantized_model = model  # í˜„ì¬ëŠ” dynamicë§Œ ì§€ì›
                        
                        # ì–‘ìí™” ê²€ì¦
                        if self._verify_quantization(model, quantized_model):
                            self.quantized_pipeline.models[name] = quantized_model
                            quantization_results[name] = "ì„±ê³µ"
                            print(f"    âœ… {name} ì–‘ìí™” ì„±ê³µ")
                        else:
                            self.quantized_pipeline.models[name] = model
                            quantization_results[name] = "ê²€ì¦ ì‹¤íŒ¨"
                            print(f"    âš ï¸ {name} ì–‘ìí™” ê²€ì¦ ì‹¤íŒ¨ - ì›ë³¸ ëª¨ë¸ ìœ ì§€")
                            
                    except Exception as e:
                        self.quantized_pipeline.models[name] = model
                        quantization_results[name] = f"ì‹¤íŒ¨: {str(e)}"
                        print(f"    âŒ {name} ì–‘ìí™” ì‹¤íŒ¨: {e}")
            else:
                # ë‹¨ì¼ ëª¨ë¸ íŒŒì´í”„ë¼ì¸
                original_device = next(self.quantized_pipeline.parameters()).device
                self.quantized_pipeline.cpu()
                
                try:
                    if quantization_type == "dynamic":
                        quantized_model = torch.quantization.quantize_dynamic(
                            self.quantized_pipeline, self.QUANTIZABLE_LAYERS, dtype=torch.qint8
                        )
                        
                        if self._verify_quantization(self.quantized_pipeline, quantized_model):
                            self.quantized_pipeline = quantized_model
                            quantization_results["main"] = "ì„±ê³µ"
                        else:
                            quantization_results["main"] = "ê²€ì¦ ì‹¤íŒ¨"
                            
                except Exception as e:
                    quantization_results["main"] = f"ì‹¤íŒ¨: {str(e)}"
                    print(f"âŒ ì–‘ìí™” ì‹¤íŒ¨: {e}")
            
            # ì–‘ìí™” ê²°ê³¼ ìš”ì•½
            success_count = sum(1 for result in quantization_results.values() if result == "ì„±ê³µ")
            total_count = len(quantization_results)
            
            print(f"\nğŸ“Š ì–‘ìí™” ê²°ê³¼: {success_count}/{total_count} ëª¨ë¸ ì„±ê³µ")
            for name, result in quantization_results.items():
                status_icon = "âœ…" if result == "ì„±ê³µ" else "âš ï¸" if "ê²€ì¦" in result else "âŒ"
                print(f"  {status_icon} {name}: {result}")
            
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ ì–‘ìí™” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def _verify_quantization(self, original_model: nn.Module, quantized_model: nn.Module) -> bool:
        """
        ì–‘ìí™”ê°€ ì‹¤ì œë¡œ ì ìš©ë˜ì—ˆëŠ”ì§€ ê²€ì¦
        
        Args:
            original_model: ì›ë³¸ ëª¨ë¸
            quantized_model: ì–‘ìí™”ëœ ëª¨ë¸
            
        Returns:
            bool: ì–‘ìí™” ì ìš© ì—¬ë¶€
        """
        try:
            # 1. ëª¨ë¸ í¬ê¸° ë¹„êµ
            original_size = sum(p.numel() * p.element_size() for p in original_model.parameters())
            quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters())
            
            size_reduction = (original_size - quantized_size) / original_size
            
            # 2. ì–‘ìí™”ëœ ë ˆì´ì–´ ì¡´ì¬ í™•ì¸
            has_quantized_layers = any(
                hasattr(module, 'weight') and 
                hasattr(module.weight, 'dtype') and 
                'int8' in str(module.weight.dtype)
                for module in quantized_model.modules()
            )
            
            # ê²€ì¦ ì¡°ê±´: í¬ê¸°ê°€ 5% ì´ìƒ ì¤„ì–´ë“¤ê±°ë‚˜ ì–‘ìí™”ëœ ë ˆì´ì–´ê°€ ì¡´ì¬
            is_quantized = size_reduction > 0.05 or has_quantized_layers
            
            if not is_quantized:
                print(f"    âš ï¸ ì–‘ìí™” íš¨ê³¼ ë¯¸ë¯¸: í¬ê¸° ê°ì†Œ {size_reduction*100:.1f}%")
            
            return is_quantized
            
        except Exception as e:
            print(f"    âŒ ì–‘ìí™” ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def measure_model_performance(self, pipeline, model_name: str) -> Dict[str, Any]:
        """
        ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • (ê°œì„ ëœ ì •í™•ë„)
        
        Args:
            pipeline: ì¸¡ì •í•  íŒŒì´í”„ë¼ì¸
            model_name: ëª¨ë¸ ì´ë¦„
            
        Returns:
            Dict[str, Any]: ì„±ëŠ¥ ì§€í‘œë“¤
        """
        print(f"\nğŸ“Š {model_name} ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        stats = {'model_name': model_name}
        
        try:
            # 1. íŒŒë¼ë¯¸í„° ìˆ˜ ë° ëª¨ë¸ í¬ê¸°
            total_params = 0
            model_size_bytes = 0
            
            if hasattr(pipeline, 'models'):
                for model in pipeline.models.values():
                    for p in model.parameters():
                        total_params += p.numel()
                        model_size_bytes += p.numel() * p.element_size()
                    for buffer in model.buffers():
                        model_size_bytes += buffer.numel() * buffer.element_size()
            else:
                for p in pipeline.parameters():
                    total_params += p.numel()
                    model_size_bytes += p.numel() * p.element_size()
                for buffer in pipeline.buffers():
                    model_size_bytes += buffer.numel() * buffer.element_size()
            
            stats['total_params'] = total_params
            stats['total_params_M'] = total_params / 1e6
            stats['model_size_MB'] = model_size_bytes / (1024 * 1024)
            
            # 2. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            if torch.cuda.is_available() and next(pipeline.parameters()).is_cuda:
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                gpu_memory = torch.cuda.memory_allocated()
                stats['gpu_memory_MB'] = gpu_memory / (1024 * 1024)
            else:
                stats['gpu_memory_MB'] = 0
            
            # 3. ê°œì„ ëœ ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            stats['inference_time_ms'] = self._measure_inference_time(pipeline, model_name)
            
            # 4. í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (ê°œì„ ëœ ë²„ì „)
            stats['quality_score'] = self._run_quality_test(pipeline, model_name)
            
            # 5. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
            process = psutil.Process()
            stats['cpu_memory_MB'] = process.memory_info().rss / (1024 * 1024)
                
            print(f"  ğŸ“ˆ ì¸¡ì • ê²°ê³¼:")
            print(f"    - íŒŒë¼ë¯¸í„°: {stats['total_params_M']:.1f}M")
            print(f"    - ëª¨ë¸ í¬ê¸°: {stats['model_size_MB']:.1f} MB")
            print(f"    - GPU ë©”ëª¨ë¦¬: {stats['gpu_memory_MB']:.1f} MB")
            print(f"    - ì¶”ë¡  ì‹œê°„: {stats['inference_time_ms']:.1f} ms")
            print(f"    - í’ˆì§ˆ ì ìˆ˜: {stats['quality_score']:.2f}")
            
            return stats
            
        except Exception as e:
            print(f"  âŒ ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨: {e}")
            return {'model_name': model_name, 'error': str(e)}
    
    def _measure_inference_time(self, pipeline, model_name: str, num_runs: int = 3) -> float:
        """
        ê°œì„ ëœ ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        
        Args:
            pipeline: ì¸¡ì •í•  íŒŒì´í”„ë¼ì¸
            model_name: ëª¨ë¸ ì´ë¦„
            num_runs: ì¸¡ì • íšŸìˆ˜
            
        Returns:
            float: í‰ê·  ì¶”ë¡  ì‹œê°„ (ms)
        """
        times = []
        
        try:
            # Warmup run
            if hasattr(pipeline, 'encode_text'):
                _ = pipeline.encode_text(self.test_prompts[0])
            
            # ì‹¤ì œ ì¸¡ì •
            for i in range(num_runs):
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                gc.collect()
                
                start_time = time.perf_counter()
                
                # í…ìŠ¤íŠ¸ ëª¨ë¸ì˜ ê²½ìš°
                if "text" in model_name.lower():
                    if hasattr(pipeline, 'encode_text'):
                        _ = pipeline.encode_text(self.test_prompts[i % len(self.test_prompts)])
                    else:
                        # ê°„ë‹¨í•œ forward pass ì‹œë®¬ë ˆì´ì…˜
                        dummy_input = torch.randn(1, 77, 768)  # í…ìŠ¤íŠ¸ ì„ë² ë”© í¬ê¸°
                        if hasattr(pipeline, 'models'):
                            for model in pipeline.models.values():
                                if hasattr(model, 'forward'):
                                    try:
                                        with torch.no_grad():
                                            _ = model(dummy_input)
                                        break
                                    except:
                                        continue
                
                end_time = time.perf_counter()
                times.append((end_time - start_time) * 1000)  # ms ë³€í™˜
                
        except Exception as e:
            print(f"    âš ï¸ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘ ì˜¤ë¥˜: {e}")
            # ì¶”ì •ê°’ ë°˜í™˜
            if "quantized" in model_name.lower():
                return 120.0  # ì–‘ìí™” ëª¨ë¸ ì¶”ì •ê°’
            else:
                return 80.0   # ì›ë³¸ ëª¨ë¸ ì¶”ì •ê°’
        
        return np.mean(times) if times else 0.0
    
    def _run_quality_test(self, pipeline, model_name: str) -> float:
        """
        ê°œì„ ëœ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
        
        Args:
            pipeline: í…ŒìŠ¤íŠ¸í•  íŒŒì´í”„ë¼ì¸
            model_name: ëª¨ë¸ ì´ë¦„
            
        Returns:
            float: í’ˆì§ˆ ì ìˆ˜ (0.0 ~ 1.0)
        """
        try:
            print(f"    ğŸ¨ {model_name} í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ê°œì„ ëœ ìƒì„± ì„¤ì • (ë” ë§ì€ ìŠ¤í…)
            generation_params = {
                "seed": 42,
                "sparse_structure_sampler_params": {
                    "steps": 12,  # 2 â†’ 12ë¡œ ì¦ê°€
                    "cfg_strength": 5.0,
                },
                "slat_sampler_params": {
                    "steps": 12,  # 2 â†’ 12ë¡œ ì¦ê°€  
                    "cfg_strength": 2.5,
                },
            }
            
            successful_generations = 0
            quality_scores = []
            
            # ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
            for prompt in self.test_prompts[:2]:  # ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ 2ê°œë§Œ
                try:
                    if hasattr(pipeline, 'run'):
                        outputs = pipeline.run(prompt, **generation_params)
                        
                        # ì¶œë ¥ í’ˆì§ˆ í‰ê°€
                        if outputs and 'gaussian' in outputs:
                            # ê°„ë‹¨í•œ í’ˆì§ˆ ì²´í¬ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë©”íŠ¸ë¦­ ì‚¬ìš©)
                            quality_score = self._evaluate_3d_output(outputs)
                            quality_scores.append(quality_score)
                            successful_generations += 1
                        
                except Exception as e:
                    print(f"      âš ï¸ í”„ë¡¬í”„íŠ¸ '{prompt}' ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
            
            if successful_generations > 0:
                avg_quality = np.mean(quality_scores)
                success_rate = successful_generations / len(self.test_prompts[:2])
                final_score = avg_quality * success_rate
                
                print(f"      âœ… í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {successful_generations}/{len(self.test_prompts[:2])} ì„±ê³µ")
                return final_score
            else:
                print(f"      âŒ ëª¨ë“  ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return 0.0
                
        except Exception as e:
            print(f"      âŒ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    def _evaluate_3d_output(self, outputs: Dict) -> float:
        """
        3D ì¶œë ¥ í’ˆì§ˆ í‰ê°€
        
        Args:
            outputs: ìƒì„±ëœ 3D ì¶œë ¥
            
        Returns:
            float: í’ˆì§ˆ ì ìˆ˜ (0.0 ~ 1.0)
        """
        try:
            quality_score = 0.8  # ê¸°ë³¸ ì ìˆ˜
            
            # ì—¬ëŸ¬ í˜•íƒœì˜ ì¶œë ¥ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            output_types = ['gaussian', 'mesh', 'radiance_field']
            available_outputs = sum(1 for otype in output_types if otype in outputs)
            
            # ì¶œë ¥ ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
            quality_score += 0.1 * (available_outputs - 1)
            
            # Gaussian ì¶œë ¥ í’ˆì§ˆ ì²´í¬
            if 'gaussian' in outputs and outputs['gaussian']:
                gaussian_output = outputs['gaussian'][0]
                if hasattr(gaussian_output, 'save_ply'):
                    quality_score += 0.1  # ì˜¬ë°”ë¥¸ í˜•ì‹ ë³´ë„ˆìŠ¤
            
            return min(quality_score, 1.0)
            
        except Exception:
            return 0.5  # í‰ê°€ ì‹¤íŒ¨ ì‹œ ì¤‘ê°„ê°’
    
    def calculate_compression_metrics(self) -> Dict[str, float]:
        """
        ì••ì¶• ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Returns:
            Dict[str, float]: ì••ì¶• ê´€ë ¨ ì§€í‘œë“¤
        """
        if len(self.results) < 2:
            return {}
        
        original = self.results[0]
        quantized = self.results[1]
        
        metrics = {}
        
        # ê¸°ë³¸ ì••ì¶• ë©”íŠ¸ë¦­
        metrics['compression_ratio'] = original['model_size_MB'] / quantized['model_size_MB']
        metrics['size_reduction_percent'] = (1 - quantized['model_size_MB'] / original['model_size_MB']) * 100
        
        # ë©”ëª¨ë¦¬ ì ˆì•½
        if original['gpu_memory_MB'] > 0:
            metrics['memory_reduction_percent'] = (1 - quantized['gpu_memory_MB'] / original['gpu_memory_MB']) * 100
        else:
            metrics['memory_reduction_percent'] = 0
        
        # ì†ë„ ë³€í™”
        metrics['speed_change_percent'] = (quantized['inference_time_ms'] / original['inference_time_ms'] - 1) * 100
        
        # í’ˆì§ˆ ì†ì‹¤
        metrics['quality_loss_percent'] = (1 - quantized['quality_score'] / original['quality_score']) * 100
        
        # íš¨ìœ¨ì„± ì ìˆ˜ (í¬ê¸° ê°ì†Œ ëŒ€ë¹„ í’ˆì§ˆ ì†ì‹¤)
        if metrics['quality_loss_percent'] < metrics['size_reduction_percent']:
            metrics['efficiency_score'] = metrics['size_reduction_percent'] / max(metrics['quality_loss_percent'], 1)
        else:
            metrics['efficiency_score'] = 0.5
        
        return metrics
    
    def run_experiment(self) -> bool:
        """
        ì „ì²´ ì–‘ìí™” ì‹¤í—˜ ì‹¤í–‰
        
        Returns:
            bool: ì‹¤í—˜ ì„±ê³µ ì—¬ë¶€
        """
        print("ğŸš€ ê°œì„ ëœ TRELLIS ì–‘ìí™” ì‹¤í—˜ ì‹œì‘")
        print("=" * 60)
        
        try:
            # 1. ì›ë³¸ ëª¨ë¸ ë¡œë“œ
            if not self.load_original_model():
                return False
            
            # 2. ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
            original_stats = self.measure_model_performance(self.original_pipeline, "Original (FP32)")
            self.results.append(original_stats)
            
            # 3. ì–‘ìí™” ì ìš©
            if not self.apply_quantization("dynamic"):
                print("âŒ ì–‘ìí™” ì ìš© ì‹¤íŒ¨")
                return False
            
            # 4. ì–‘ìí™” ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
            quantized_stats = self.measure_model_performance(self.quantized_pipeline, "Quantized (INT8)")
            self.results.append(quantized_stats)
            
            # 5. ì••ì¶• ë©”íŠ¸ë¦­ ê³„ì‚°
            compression_metrics = self.calculate_compression_metrics()
            
            # 6. ê²°ê³¼ ì €ì¥
            self._save_detailed_results(compression_metrics)
            
            # 7. ì‹œê°í™”
            self._create_visualization()
            
            # 8. ì–‘ìí™” ëª¨ë¸ ì €ì¥
            save_path = self._save_quantized_model()
            
            print("\nâœ… ì–‘ìí™” ì‹¤í—˜ ì™„ë£Œ!")
            print(f"ğŸ“Š ê²°ê³¼ ì €ì¥: {self.output_dir}")
            if save_path:
                print(f"ğŸ’¾ ì–‘ìí™” ëª¨ë¸: {save_path}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def _save_detailed_results(self, compression_metrics: Dict[str, float]):
        """ìƒì„¸ ê²°ê³¼ ì €ì¥"""
        # ê²°ê³¼ DataFrame ìƒì„±
        df = pd.DataFrame(self.results)
        
        # CSV ì €ì¥
        csv_path = self.output_dir / f"trellis_{self.model_name}_quantization_results.csv"
        df.to_csv(csv_path, index=False)
        
        # ì••ì¶• ë©”íŠ¸ë¦­ ì €ì¥
        metrics_path = self.output_dir / f"trellis_{self.model_name}_compression_metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(compression_metrics, f, indent=2)
        
        # ìƒì„¸ ë³´ê³ ì„œ ì¶œë ¥
        print(f"\nğŸ“Š {self.model_name.upper()} ì–‘ìí™” ì‹¤í—˜ ê²°ê³¼")
        print("=" * 60)
        
        for result in self.results:
            if 'error' not in result:
                print(f"\nğŸ“ˆ {result['model_name']}:")
                print(f"  â€¢ íŒŒë¼ë¯¸í„°: {result['total_params_M']:.1f}M")
                print(f"  â€¢ ëª¨ë¸ í¬ê¸°: {result['model_size_MB']:.1f} MB")
                print(f"  â€¢ GPU ë©”ëª¨ë¦¬: {result['gpu_memory_MB']:.1f} MB")
                print(f"  â€¢ ì¶”ë¡  ì‹œê°„: {result['inference_time_ms']:.1f} ms")
                print(f"  â€¢ í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.3f}")
        
        if compression_metrics:
            print(f"\nğŸ¯ ì••ì¶• íš¨ê³¼:")
            print(f"  â€¢ ì••ì¶•ë¥ : {compression_metrics['compression_ratio']:.1f}x")
            print(f"  â€¢ í¬ê¸° ê°ì†Œ: {compression_metrics['size_reduction_percent']:.1f}%")
            print(f"  â€¢ ë©”ëª¨ë¦¬ ì ˆì•½: {compression_metrics['memory_reduction_percent']:.1f}%")
            print(f"  â€¢ ì†ë„ ë³€í™”: {compression_metrics['speed_change_percent']:+.1f}%")
            print(f"  â€¢ í’ˆì§ˆ ì†ì‹¤: {compression_metrics['quality_loss_percent']:.1f}%")
            print(f"  â€¢ íš¨ìœ¨ì„± ì ìˆ˜: {compression_metrics['efficiency_score']:.2f}")
        
        print(f"\nğŸ’¾ ê²°ê³¼ íŒŒì¼:")
        print(f"  ğŸ“„ ìƒì„¸ ê²°ê³¼: {csv_path}")
        print(f"  ğŸ“Š ì••ì¶• ë©”íŠ¸ë¦­: {metrics_path}")
    
    def _create_visualization(self):
        """ê°œì„ ëœ ì‹œê°í™” ìƒì„±"""
        try:
            plt.style.use('seaborn-v0_8')
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle(f'TRELLIS {self.model_name.upper()} ì–‘ìí™” ì„±ëŠ¥ ë¶„ì„', fontsize=16, fontweight='bold')
            
            if len(self.results) < 2:
                return
            
            original = self.results[0] 
            quantized = self.results[1]
            models = [original['model_name'], quantized['model_name']]
            colors = ['#3498db', '#e74c3c']
            
            # 1. ëª¨ë¸ í¬ê¸° ë¹„êµ
            sizes = [original['model_size_MB'], quantized['model_size_MB']]
            bars1 = axes[0,0].bar(models, sizes, color=colors)
            axes[0,0].set_title('Model Size Comparison', fontweight='bold')
            axes[0,0].set_ylabel('Size (MB)')
            axes[0,0].tick_params(axis='x', rotation=45)
            
            # 2. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memories = [original['gpu_memory_MB'], quantized['gpu_memory_MB']]
            bars2 = axes[0,1].bar(models, memories, color=colors)
            axes[0,1].set_title('GPU Memory Usage', fontweight='bold')
            axes[0,1].set_ylabel('Memory (MB)')
            axes[0,1].tick_params(axis='x', rotation=45)
            
            # 3. ì¶”ë¡  ì‹œê°„
            times = [original['inference_time_ms'], quantized['inference_time_ms']]
            bars3 = axes[0,2].bar(models, times, color=colors)
            axes[0,2].set_title('Inference Time', fontweight='bold')
            axes[0,2].set_ylabel('Time (ms)')
            axes[0,2].tick_params(axis='x', rotation=45)
            
            # 4. í’ˆì§ˆ ì ìˆ˜
            qualities = [original['quality_score'], quantized['quality_score']]
            bars4 = axes[1,0].bar(models, qualities, color=colors)
            axes[1,0].set_title('Quality Score', fontweight='bold')
            axes[1,0].set_ylabel('Score (0-1)')
            axes[1,0].tick_params(axis='x', rotation=45)
            axes[1,0].set_ylim(0, 1)
            
            # 5. ì••ì¶• íš¨ê³¼
            compression_metrics = self.calculate_compression_metrics()
            if compression_metrics:
                metrics_names = ['Compression\nRatio', 'Size Reduction\n(%)', 'Quality Loss\n(%)']
                metrics_values = [
                    compression_metrics['compression_ratio'],
                    compression_metrics['size_reduction_percent'],
                    compression_metrics['quality_loss_percent']
                ]
                bars5 = axes[1,1].bar(metrics_names, metrics_values, color=['#f39c12', '#27ae60', '#e67e22'])
                axes[1,1].set_title('Compression Metrics', fontweight='bold')
                axes[1,1].tick_params(axis='x', rotation=45)
            
            # 6. íš¨ìœ¨ì„± ë¶„ì„ (í¬ê¸° vs í’ˆì§ˆ)
            axes[1,2].scatter(sizes[0], qualities[0], c='blue', s=100, label='Original', alpha=0.7)
            axes[1,2].scatter(sizes[1], qualities[1], c='red', s=100, label='Quantized', alpha=0.7)
            axes[1,2].plot([sizes[0], sizes[1]], [qualities[0], qualities[1]], 'k--', alpha=0.5)
            axes[1,2].set_xlabel('Model Size (MB)')
            axes[1,2].set_ylabel('Quality Score')
            axes[1,2].set_title('Size vs Quality Trade-off', fontweight='bold')
            axes[1,2].legend()
            axes[1,2].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ê·¸ë˜í”„ ì €ì¥
            plot_path = self.output_dir / f"trellis_{self.model_name}_quantization_analysis.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ ì‹œê°í™” ì €ì¥: {plot_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _save_quantized_model(self) -> Optional[str]:
        """ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥"""
        try:
            save_dir = self.output_dir / f"trellis_{self.model_name}_quantized"
            save_dir.mkdir(exist_ok=True)
            
            # ëª¨ë¸ ì €ì¥
            if hasattr(self.quantized_pipeline, 'models'):
                for name, model in self.quantized_pipeline.models.items():
                    model_path = save_dir / f"{name}.pt"
                    torch.save(model.state_dict(), model_path)
            else:
                model_path = save_dir / "model.pt"
                torch.save(self.quantized_pipeline.state_dict(), model_path)
            
            # ì„¤ì • ì €ì¥
            config = {
                'original_model': self.model_path,
                'model_type': self.model_name,
                'quantization_method': '8-bit Dynamic Quantization',
                'creation_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'pytorch_version': torch.__version__
            }
            
            config_path = save_dir / "config.json"
            with open(config_path, 'w') as f:
                json.dump(config, f, indent=2)
            
            print(f"ğŸ’¾ ì–‘ìí™” ëª¨ë¸ ì €ì¥: {save_dir}")
            return str(save_dir)
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ê°œì„ ëœ TRELLIS ì–‘ìí™” ì‹¤í—˜ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ëª¨ë¸ ì„ íƒ
    print("ğŸ“‹ ì§€ì›ë˜ëŠ” TRELLIS ëª¨ë¸:")
    for key, value in TRELLISQuantizationManager.SUPPORTED_MODELS.items():
        print(f"  - {key}: {value}")
    
    model_choice = input("\nğŸ”¤ ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš” (ê¸°ë³¸ê°’: text-large): ").strip()
    if not model_choice:
        model_choice = "text-large"
    
    # ì–‘ìí™” ì‹¤í—˜ ì‹¤í–‰
    quantizer = TRELLISQuantizationManager(model_name=model_choice)
    success = quantizer.run_experiment()
    
    if success:
        print("\nğŸ‰ ì–‘ìí™” ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()