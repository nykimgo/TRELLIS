#!/usr/bin/env python3
"""
ìë™ í”„ë¡¬í”„íŠ¸ ì¦ê°• ì‹œìŠ¤í…œ
Toys4k ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ LLMì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ì¦ê°•í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import csv
import json
import random
import re
import subprocess
import sys
import os
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import pandas as pd
import numpy as np

class PromptGenerator:
    def __init__(self, metadata_path: str, num_samples: int = 100, use_random: bool = False, csv_file: str = None):
        """
        Args:
            metadata_path: metadata.csv íŒŒì¼ ê²½ë¡œ
            num_samples: ì²˜ë¦¬í•  ëœë¤ ìƒ˜í”Œ ìˆ˜
            use_random: Trueë©´ ëœë¤ ì‹œë“œ ì‚¬ìš©, Falseë©´ ê³ ì • ì‹œë“œ(42) ì‚¬ìš©
            csv_file: ê¸°ì¡´ ìƒ˜í”Œ CSV íŒŒì¼ ê²½ë¡œ (ì´ ê°’ì´ ìˆìœ¼ë©´ metadata_path ëŒ€ì‹  ì‚¬ìš©)
        """
        self.metadata_path = metadata_path
        self.num_samples = num_samples
        self.use_random = use_random
        self.csv_file = csv_file
        self.selected_data = []
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir = Path("./prompt_generation_outputs")
        self.output_dir.mkdir(exist_ok=True)
        
    def load_and_sample_data(self) -> List[Dict]:
        """metadata.csvì—ì„œ ëœë¤ ìƒ˜í”Œë§í•˜ê³  ì§§ì€ ìº¡ì…˜ ì„ íƒ ë˜ëŠ” CSV íŒŒì¼ì—ì„œ ë¡œë“œ"""
        
        # CSV íŒŒì¼ì´ ì§€ì •ë˜ì–´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        if self.csv_file and os.path.exists(self.csv_file):
            return self.load_from_csv(self.csv_file)
        
        print(f"Loading metadata from {self.metadata_path}...")
        
        # CSV ì½ê¸°
        df = pd.read_csv(self.metadata_path)
        print(f"Total records: {len(df)}")
        
        # ì •í™•íˆ num_samples ê°œì˜ ìœ íš¨í•œ ìƒ˜í”Œì„ ì–»ì„ ë•Œê¹Œì§€ ë°˜ë³µ
        selected_data = []
        attempts = 0
        max_attempts = 10
        
        # ì‚¬ìš©ëœ ì¸ë±ìŠ¤ ì¶”ì  (ì¤‘ë³µ ë°©ì§€)
        used_indices = set()
        
        while len(selected_data) < self.num_samples and attempts < max_attempts:
            attempts += 1
            
            # ë‚¨ì€ í•„ìš” ê°œìˆ˜ ê³„ì‚°
            needed = self.num_samples - len(selected_data)
            
            # ì—¬ìœ ë¶„ì„ ë‘ê³  ìƒ˜í”Œë§ (ìœ íš¨í•˜ì§€ ì•Šì€ ê²ƒë“¤ ê³ ë ¤)
            sample_size = min(needed * 2, len(df) - len(used_indices))
            
            if sample_size <= 0:
                print(f"âš ï¸ No more data to sample from. Got {len(selected_data)} valid samples.")
                break
            
            # ì‚¬ìš©ë˜ì§€ ì•Šì€ ì¸ë±ìŠ¤ì—ì„œ ìƒ˜í”Œë§
            available_indices = list(set(df.index) - used_indices)
            if not available_indices:
                print(f"âš ï¸ No more available data. Got {len(selected_data)} valid samples.")
                break
            
            # ëœë¤ ìƒ˜í”Œë§
            if self.use_random:
                sampled_indices = np.random.choice(available_indices, 
                                                 size=min(sample_size, len(available_indices)), 
                                                 replace=False)
            else:
                # ê³ ì • ì‹œë“œ ì‚¬ìš©
                np.random.seed(42 + attempts)  # attemptsë¥¼ ë”í•´ì„œ ë§¤ë²ˆ ë‹¤ë¥¸ ìƒ˜í”Œ ì–»ê¸°
                sampled_indices = np.random.choice(available_indices, 
                                                 size=min(sample_size, len(available_indices)), 
                                                 replace=False)
            
            sampled_df = df.loc[sampled_indices]
            print(f"Attempt {attempts}: Sampling {len(sampled_df)} records to get {needed} more valid samples")
            
            # ìƒ˜í”Œë§ëœ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©ë¨ìœ¼ë¡œ í‘œì‹œ
            used_indices.update(sampled_indices)
            
            for _, row in sampled_df.iterrows():
                if len(selected_data) >= self.num_samples:
                    break
                    
                # captions ì»¬ëŸ¼ì—ì„œ ì§§ì€ ìº¡ì…˜ ì„ íƒ
                captions_str = row['captions']
                
                # NaN/null ê°’ ì²´í¬
                if pd.isna(captions_str) or captions_str is None:
                    continue
                
                # float íƒ€ì… ì²´í¬ (NaNì´ floatë¡œ ì½í ìˆ˜ ìˆìŒ)
                if isinstance(captions_str, float):
                    continue
                
                try:
                    # JSON í˜•ì‹ì˜ captions íŒŒì‹±
                    captions = json.loads(captions_str.replace("'", '"'))
                    
                    # 4-8 ë‹¨ì–´ ê¸¸ì´ì˜ ìº¡ì…˜ í•„í„°ë§
                    short_captions = [
                        cap for cap in captions 
                        if 4 <= len(cap.split()) <= 8
                    ]
                    
                    if short_captions:
                        # ê°€ì¥ ì§§ì€ ìº¡ì…˜ ì„ íƒ
                        selected_caption = min(short_captions, key=lambda x: len(x.split()))
                        
                        selected_data.append({
                            'sha256': row['sha256'],
                            'file_identifier': row['file_identifier'],
                            'original_caption': selected_caption,
                            'all_captions': captions
                        })
                    
                except (json.JSONDecodeError, KeyError, TypeError, AttributeError) as e:
                    continue
        
        sampling_type = "random" if self.use_random else "fixed seed"
        print(f"Final result: {len(selected_data)} valid samples obtained ({sampling_type})")
        
        if len(selected_data) < self.num_samples:
            print(f"âš ï¸ Warning: Only found {len(selected_data)} valid samples out of {self.num_samples} requested")
        
        self.selected_data = selected_data
        print(f"Successfully selected {len(selected_data)} items with short captions")
        
        # CSV íŒŒì¼ë¡œ ìƒ˜í”Œ ì €ì¥ (CSV íŒŒì¼ë¡œë¶€í„° ë¡œë“œí•œ ê²½ìš°ê°€ ì•„ë‹ ë•Œë§Œ)
        if not self.csv_file:
            self.save_samples_to_csv()
        
        return selected_data
    
    def save_samples_to_csv(self):
        """ì„ íƒëœ ìƒ˜í”Œì„ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.selected_data:
            return
        
        # ëœë¤/ê³ ì • ìƒ˜í”Œ êµ¬ë¶„ì„ ìœ„í•œ íŒŒì¼ëª…
        suffix = "random" if self.use_random else "fixed"
        csv_filename = f"sampled_data_{self.num_samples}_{suffix}.csv"
        csv_path = self.output_dir / csv_filename
        
        # DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        df_data = []
        for item in self.selected_data:
            df_data.append({
                'sha256': item['sha256'],
                'file_identifier': item['file_identifier'],
                'original_caption': item['original_caption'],
                'all_captions': json.dumps(item['all_captions'], ensure_ascii=False)
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv(csv_path, index=False, encoding='utf-8')
        print(f"ğŸ“ Saved {len(df_data)} samples to: {csv_path}")
        
        return str(csv_path)
    
    def load_from_csv(self, csv_path: str) -> List[Dict]:
        """CSV íŒŒì¼ì—ì„œ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ"""
        print(f"Loading samples from CSV: {csv_path}")
        
        df = pd.read_csv(csv_path)
        print(f"Loaded {len(df)} records from CSV")
        
        selected_data = []
        for _, row in df.iterrows():
            try:
                # all_captions JSON íŒŒì‹±
                all_captions = json.loads(row['all_captions'])
                
                selected_data.append({
                    'sha256': row['sha256'],
                    'file_identifier': row['file_identifier'],
                    'original_caption': row['original_caption'],
                    'all_captions': all_captions
                })
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Error parsing CSV row: {e}")
                continue
        
        self.selected_data = selected_data
        print(f"Successfully loaded {len(selected_data)} items from CSV")
        return selected_data
    
    def generate_prompts_file(self, output_file: str = "prompts.txt") -> str:
        """prompts.txt íŒŒì¼ ìƒì„±"""
        if not self.selected_data:
            self.load_and_sample_data()
        
        prompt_template = """You are a prompt engineer specialized in converting short user prompts into detailed, descriptive prompts optimized for text-to-3D asset generation models.

Given a short numbered user input list, rewrite each line into a vivid, detailed prompt that:

- Clearly describes the object, scene, or character
- Includes relevant visual details (color, shape, style, posture, materials, environment)
- Avoids ambiguity and is easy for a 3D generation model to interpret
- Uses concise but descriptive language within 40 words or less
- Respond **line by line**, keeping the numbering intact

Please answer in the following format:
"1. <User input>":"<Model response>"

Numbered user input:
"""
        
        # ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        numbered_captions = []
        for i, data in enumerate(self.selected_data, 1):
            numbered_captions.append(f"{i}. {data['original_caption']}")
        
        # ì „ì²´ í”„ë¡¬í”„íŠ¸ ìƒì„±
        full_prompt = prompt_template + "\n".join(numbered_captions)
        
        # íŒŒì¼ ì €ì¥
        output_path = self.output_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(full_prompt)
        
        print(f"Generated prompts file: {output_path}")
        return str(output_path)
    
    def run_ollama_models(self, prompts_file: str, models: List[str]) -> Dict[str, str]:
        """ì—¬ëŸ¬ Ollama ëª¨ë¸ ì‹¤í–‰"""
        results = {}
        
        print(f"Running {len(models)} Ollama models...")
        
        for model in models:
            print(f"Running model: {model}")
            
            # ì¶œë ¥ íŒŒì¼ëª… ìƒì„± (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
            safe_model_name = re.sub(r'[^\w\-_\.]', '_', model)
            output_file = self.output_dir / f"detailed_prompt_{safe_model_name}.txt"
            
            try:
                # ollama ì‹¤í–‰
                cmd = f"ollama run {model} < {prompts_file}"
                result = subprocess.run(
                    cmd, 
                    shell=True, 
                    capture_output=True, 
                    text=True, 
                    timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                )
                
                if result.returncode == 0:
                    # ê²°ê³¼ ì €ì¥
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(result.stdout)
                    
                    results[model] = str(output_file)
                    print(f"âœ“ {model} completed: {output_file}")
                else:
                    print(f"âœ— {model} failed: {result.stderr}")
                    
            except subprocess.TimeoutExpired:
                print(f"âœ— {model} timed out")
            except Exception as e:
                print(f"âœ— {model} error: {e}")
        
        return results
    
    def parse_llm_output(self, file_path: str) -> List[Tuple[str, str]]:
        """LLM ì¶œë ¥ íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ (ì›ë³¸, ì¦ê°•) ìŒ ì¶”ì¶œ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ë‹¤ì–‘í•œ í˜•ì‹ ì²˜ë¦¬
        pairs = []
        
        # íŒ¨í„´ 1: "ë²ˆí˜¸. ì›ë³¸":"ì¦ê°•"
        pattern1 = r'"(\d+\.\s*[^"]+)"\s*:\s*"([^"]+)"'
        matches1 = re.findall(pattern1, content, re.MULTILINE | re.DOTALL)
        
        for original, enhanced in matches1:
            # ë²ˆí˜¸ ì œê±°
            clean_original = re.sub(r'^\d+\.\s*', '', original).strip()
            enhanced_clean = self._remove_quotes(enhanced)
            pairs.append((clean_original, enhanced_clean))
        
        # íŒ¨í„´ 2: ë²ˆí˜¸. "ì›ë³¸":"ì¦ê°•" (ë”°ì˜´í‘œ ì—†ëŠ” ë²ˆí˜¸)
        if not pairs:
            pattern2 = r'\d+\.\s*"([^"]+)"\s*:\s*"([^"]+)"'
            matches2 = re.findall(pattern2, content, re.MULTILINE | re.DOTALL)
            # í…œí”Œë¦¿ í…ìŠ¤íŠ¸ í•„í„°ë§
            filtered_matches = []
            for orig, enh in matches2:
                if not ('<User input>' in orig or '<Model response>' in enh):
                    filtered_matches.append((orig.strip(), self._remove_quotes(enh)))
            pairs = filtered_matches
        
        # íŒ¨í„´ 3: qwen3 í˜•ì‹ ì²˜ë¦¬ - ë²ˆí˜¸. "ì¦ê°•ëœ ë‚´ìš©" (ASCII ë”°ì˜´í‘œ)
        if not pairs:
            pattern3 = r'(\d+)\.\s*"([^"]+)"'
            matches3 = re.findall(pattern3, content, re.MULTILINE | re.DOTALL)
            
            if matches3:
                # ì›ë³¸ ë°ì´í„°ì™€ ë§¤ì¹­í•˜ê¸° ìœ„í•´ ì¸ë±ìŠ¤ ì‚¬ìš©
                for i, (num_str, enhanced) in enumerate(matches3):
                    num = int(num_str) - 1  # 0-based indexë¡œ ë³€í™˜
                    if num < len(self.selected_data):
                        original = self.selected_data[num]['original_caption']
                        enhanced_clean = self._remove_quotes(enhanced)
                        pairs.append((original.strip(), enhanced_clean))
        
        # íŒ¨í„´ 4: gemma3 í˜•ì‹ ì²˜ë¦¬ - ë²ˆí˜¸. "ì¦ê°•ëœ ë‚´ìš©" (Unicode ë”°ì˜´í‘œ ì§€ì›)
        if not pairs:
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                # ë²ˆí˜¸ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ ì°¾ê¸° (ì¤‘ë³µ ë²ˆí˜¸ ì²˜ë¦¬: "1. 1. content" í˜•ì‹)
                match = re.match(r'^(\d+)\.\s*(?:\d+\.\s*)?(.+)$', line)
                if match:
                    num_str, rest = match.groups()
                    # ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ë‚´ìš©ì¸ì§€ í™•ì¸í•˜ê³  ì œê±° (ì‹œì‘ê³¼ ëì´ ë‹¤ë¥¸ Unicode ë”°ì˜´í‘œì¼ ìˆ˜ ìˆìŒ)
                    start_quotes = ('"', '"', '"')  # ASCII, left double, right double
                    end_quotes = ('"', '"', '"')    # ASCII, left double, right double  
                    if (rest.startswith(start_quotes) and rest.endswith(end_quotes)):
                        enhanced = rest[1:-1]  # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë¬¸ì ì œê±°
                        enhanced_clean = self._remove_quotes(enhanced)
                        num = int(num_str) - 1  # 0-based indexë¡œ ë³€í™˜
                        if num < len(self.selected_data):
                            original = self.selected_data[num]['original_caption']
                            pairs.append((original.strip(), enhanced_clean))
                    elif not any(char in rest for char in ':'):  # ë”°ì˜´í‘œ ì—†ì´ ì§ì ‘ í…ìŠ¤íŠ¸ì¸ ê²½ìš° (ì½œë¡  ì—†ìŒ í™•ì¸)
                        enhanced = rest.strip()
                        enhanced_clean = self._remove_quotes(enhanced)
                        num = int(num_str) - 1
                        if num < len(self.selected_data):
                            original = self.selected_data[num]['original_caption']
                            pairs.append((original.strip(), enhanced_clean))
        
        # íŒ¨í„´ 5: DeepSeek í˜•ì‹ ì²˜ë¦¬ - í°ë”°ì˜´í‘œ ì•ˆì˜ ë‚´ìš©ì„ ë¼ì¸ë³„ë¡œ ë¶„ì„
        if not pairs:
            lines = content.split('\n')
            in_quoted_section = False
            
            for line in lines:
                line = line.strip()
                
                # í°ë”°ì˜´í‘œë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ ì°¾ê¸° (ê²°ê³¼ ì„¹ì…˜ ì‹œì‘)
                if line.startswith('"') and not in_quoted_section:
                    in_quoted_section = True
                    line = line[1:]  # ì²« ë²ˆì§¸ í°ë”°ì˜´í‘œ ì œê±°
                
                if in_quoted_section:
                    # ë§ˆì§€ë§‰ í°ë”°ì˜´í‘œë¡œ ëë‚˜ëŠ” ê²½ìš° (ê²°ê³¼ ì„¹ì…˜ ì¢…ë£Œ)
                    if line.endswith('"') and not line.endswith('""'):
                        line = line[:-1]  # ë§ˆì§€ë§‰ í°ë”°ì˜´í‘œ ì œê±°
                        in_quoted_section = False
                    
                    # ë²ˆí˜¸. 'ì›ë³¸': ì¦ê°•ëœë‚´ìš© íŒ¨í„´ ì°¾ê¸°
                    match = re.match(r"(\d+)\.\s*'([^']+)':\s*(.+)", line)
                    if match:
                        num_str, original, enhanced = match.groups()
                        original_clean = original.strip()
                        enhanced_clean = enhanced.strip().rstrip('.')
                        pairs.append((original_clean, enhanced_clean))
        
        # íŒ¨í„´ 6: ê°„ë‹¨í•œ ë¼ì¸ë³„ ì²˜ë¦¬ (ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„)
        if not pairs:
            lines = content.split('\n')
            for line in lines:
                if ':' in line and line.strip():
                    # ë²ˆí˜¸ì™€ ë”°ì˜´í‘œ ì œê±° í›„ ì½œë¡ ìœ¼ë¡œ ë¶„í• 
                    clean_line = re.sub(r'^\d+\.\s*', '', line.strip())
                    clean_line = clean_line.replace('"', '')
                    
                    if ':' in clean_line:
                        parts = clean_line.split(':', 1)
                        if len(parts) == 2:
                            enhanced_clean = self._remove_quotes(parts[1])
                            pairs.append((parts[0].strip(), enhanced_clean))
        
        print(f"Parsed {len(pairs)} prompt pairs from {file_path}")
        return pairs
    
    def create_excel_output_enhanced(self, llm_results: Dict[str, str]) -> str:
        """í–¥ìƒëœ í†µí•© Excel íŒŒì¼ ìƒì„±"""
        try:
            from enhanced_normalizer import EnhancedNormalizer
            
            # í†µí•© ì •ê·œí™” ì‹¤í–‰ (ì¤‘ê°„ íŒŒì¼ë“¤ì´ ì €ì¥ë¨)
            normalizer = EnhancedNormalizer()
            normalized_data = normalizer.normalize_with_best_model(llm_results)
            
            if not normalized_data:
                print("No normalized data available, falling back to legacy method")
                return self.create_excel_output_legacy(llm_results)
            
            # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ê° ì—”íŠ¸ë¦¬ì— llm_modelì´ ìˆëŠ”ì§€ í™•ì¸
            # llm_modelì´ ì—†ê±°ë‚˜ ì˜ëª»ëœ ê²½ìš° legacy ë°©ë²• ì‚¬ìš©
            has_valid_source_models = all(
                entry.get('llm_model', 'unknown') != 'unknown' 
                for entry in normalized_data
            )
            
            if not has_valid_source_models:
                print("Source model mapping failed, using legacy method to avoid duplicates...")
                return self.create_excel_output_legacy(llm_results)
            
            # ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±
            metadata_dict = {data['original_caption']: data for data in self.selected_data}
            
            # í†µí•© Excel íŒŒì¼ ìƒì„±
            models = list(llm_results.keys())  # llm_resultsì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
            excel_path = self._create_output_path(models)
            result_path = normalizer.create_unified_excel(normalized_data, metadata_dict, str(excel_path), llm_results)
            
            return result_path
            
        except Exception as e:
            print(f"Enhanced Excel creation failed: {e}")
            print("Falling back to legacy method...")
            return self.create_excel_output_legacy(llm_results)

    def create_excel_output_legacy(self, llm_results: Dict[str, str]) -> str:
        """ê¸°ì¡´ Excel íŒŒì¼ ìƒì„± ë°©ì‹ (fallback)"""
        all_rows = []
        object_name_cache = {}  # ì›ë³¸ í”„ë¡¬í”„íŠ¸ë³„ë¡œ object_name ìºì‹œ
        metadata_cache = {}  # ì›ë³¸ í”„ë¡¬í”„íŠ¸ë³„ë¡œ metadata ìºì‹œ
        
        # ì²« ë²ˆì§¸ íŒ¨ìŠ¤: ëª¨ë“  ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ê³  ìºì‹œ êµ¬ì¶•
        for model, file_path in llm_results.items():
            try:
                pairs = self.parse_llm_output(file_path)
                
                for i, (original, enhanced) in enumerate(pairs):
                    original_lower = original.strip().lower()
                    
                    # ì›ë³¸ ë°ì´í„°ì—ì„œ ë§¤ì¹­ë˜ëŠ” í•­ëª© ì°¾ê¸°
                    matched_data = None
                    for data in self.selected_data:
                        if data['original_caption'].strip().lower() == original_lower:
                            matched_data = data
                            break
                    
                    # metadata ìºì‹œì— ì €ì¥ (ë§¤ì¹­ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´)
                    if matched_data and original_lower not in metadata_cache:
                        metadata_cache[original_lower] = matched_data
                    
                    # object_name ì¶”ì¶œ ë° ìºì‹œ (í•­ìƒ ì‹œë„, Unknownì´ ì•„ë‹Œ ê²½ìš°ë§Œ ìºì‹œ)
                    if original_lower not in object_name_cache:
                        object_name = self._extract_object_name(original)
                        if object_name != "Unknown":
                            object_name_cache[original_lower] = object_name
                            
            except Exception as e:
                print(f"Error in first pass processing {model}: {e}")
                continue
        
        # ë‘ ë²ˆì§¸ íŒ¨ìŠ¤: ì‹¤ì œ í–‰ ìƒì„± (ìºì‹œ í™œìš©)
        for model, file_path in llm_results.items():
            try:
                pairs = self.parse_llm_output(file_path)
                
                # ëª¨ë¸ ì •ë³´ ì¶”ì¶œ
                model_parts = model.split(':')
                model_name = model_parts[0]
                model_size = model_parts[1] if len(model_parts) > 1 else "unknown"
                
                for i, (original, enhanced) in enumerate(pairs):
                    original_lower = original.strip().lower()
                    
                    # ìºì‹œì—ì„œ object_name ê°€ì ¸ì˜¤ê¸°
                    object_name = object_name_cache.get(original_lower, "Unknown")
                    
                    # Unknownì¸ ê²½ìš° ìºì‹œì—ì„œ ìœ ì‚¬í•œ í”„ë¡¬í”„íŠ¸ì˜ ì¢‹ì€ object_name ì°¾ê¸°
                    if object_name == "Unknown":
                        for cached_prompt, cached_name in object_name_cache.items():
                            if self._prompts_similar(original_lower, cached_prompt) and cached_name != "Unknown":
                                object_name = cached_name
                                print(f"Shared object_name '{cached_name}' from similar prompt for '{original[:30]}...'")
                                break
                    
                    # ìºì‹œì—ì„œ metadata ê°€ì ¸ì˜¤ê¸°
                    matched_data = metadata_cache.get(original_lower, None)
                    
                    # enhanced text ì²˜ë¦¬ - ìˆ«ì ì œê±° ë° ë”°ì˜´í‘œ ì¶”ê°€
                    enhanced_clean = self._clean_enhanced_text(enhanced, model_name)
                    
                    row = {
                        'category': model_name,
                        'llm_model': model,
                        'parameters': model_size,
                        'size': 'unknown',
                        'GPU_usage': 'unknown',
                        'object_name': object_name,
                        'seed': '',
                        'params': '',
                        'matched_image': '',
                        'object_name_clean': object_name,
                        'user_prompt': original,
                        'text_prompt': enhanced_clean,
                        'sha256': matched_data['sha256'] if matched_data else '',
                        'file_identifier': matched_data['file_identifier'] if matched_data else ''
                    }
                    all_rows.append(row)
                    
            except Exception as e:
                print(f"Error processing {model}: {e}")
                continue
        
        # DataFrame ìƒì„± ë° Excel ì €ì¥
        df = pd.DataFrame(all_rows)
        excel_path = self._create_output_path(list(llm_results.keys()))
        df.to_excel(excel_path, index=False)
        
        print(f"Legacy Excel file created: {excel_path}")
        print(f"Total rows: {len(all_rows)}")
        
        return str(excel_path)
    
    def _remove_quotes(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì‹œì‘ê³¼ ëì˜ ìŒë”°ì˜´í‘œ ì œê±° (ì¼ê´€ì„±ì„ ìœ„í•´)"""
        text = text.strip()
        quote_chars = ('"', '"', '"')  # ASCII ë° Unicode ìŒë”°ì˜´í‘œ
        
        # ì‹œì‘ê³¼ ëì´ ëª¨ë‘ ìŒë”°ì˜´í‘œì¸ ê²½ìš° ì œê±°
        if text.startswith(quote_chars) and text.endswith(quote_chars):
            return text[1:-1].strip()
        
        return text
    
    def _extract_object_name(self, prompt: str) -> str:
        """ê°„ë‹¨í•œ ê°ì²´ëª… ì¶”ì¶œ (ë‚˜ì¤‘ì— ë” ì •êµí•œ LLMìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥)"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        words = prompt.lower().split()
        
        # ì¼ë°˜ì ì¸ ê°ì²´ í‚¤ì›Œë“œë“¤
        object_keywords = [
            'table', 'chair', 'bed', 'lamp', 'book', 'cup', 'bowl', 'plate', 
            'phone', 'television', 'computer', 'car', 'house', 'tree', 'flower',
            'cabinet', 'drawer', 'shelf', 'mirror', 'clock', 'bottle', 'box',
            'plane', 'airplane', 'fan', 'frosted'
        ]
        
        for word in words:
            if word in object_keywords:
                return word.capitalize()
        
        # íŠ¹ë³„í•œ ê²½ìš° ì²˜ë¦¬
        if 'plane' in prompt.lower() or 'airplane' in prompt.lower():
            return 'Pink-brown'
        if 'fan' in prompt.lower() and 'wooden' in prompt.lower():
            return 'Fan'
        if 'bottle' in prompt.lower() and ('frosted' in prompt.lower() or 'cross' in prompt.lower()):
            return 'Bottle'
        
        # ì²« ë²ˆì§¸ ëª…ì‚¬ë¡œ ì¶”ì •ë˜ëŠ” ë‹¨ì–´ ë°˜í™˜
        return words[0].capitalize() if words else "Unknown"
    
    def _create_output_path(self, models: List[str]) -> Path:
        """CSV íŒŒì¼ëª…ê³¼ ëª¨ë¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        # CSV íŒŒì¼ëª…ì—ì„œ ê¸°ë³¸ ì´ë¦„ ì¶”ì¶œ
        if self.csv_file:
            csv_name = Path(self.csv_file).stem  # í™•ì¥ì ì œê±°
        else:
            csv_name = "metadata_sample"
        
        # ëª¨ë¸ ê°œìˆ˜ ê³„ì‚°
        num_models = len(models)
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° í¬ê¸° ë¶„ì„í•˜ì—¬ ë²”ìœ„ ê²°ì •
        model_range = self._get_model_range(models)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (CSV íŒŒì¼ëª… ê¸°ë°˜)
        output_subdir = self.output_dir / csv_name
        output_subdir.mkdir(exist_ok=True)
        
        filename = f"prompt_results_{num_models}_{model_range}"
        # íŒŒì¼ëª… ìƒì„±
        for model_name in models:
            if ':' in model_name:
                model_cat = model_name.split(':', 1)[0]
            filename += f"_{model_cat}"
        filename += ".xlsx"
        return output_subdir / filename
    
    def _extract_model_params(self, model_name: str) -> float:
        """ëª¨ë¸ëª…ì—ì„œ íŒŒë¼ë¯¸í„° í¬ê¸° ì¶”ì¶œ (ì˜ˆ: qwen3:32b-q8_0 -> 32)"""
        try:
            # ':' ì´í›„ ë¶€ë¶„ ì¶”ì¶œ
            if ':' in model_name:
                param_part = model_name.split(':', 1)[1]
                # 'b' ì•ì˜ ìˆ«ì ì¶”ì¶œ
                import re
                match = re.search(r'(\d+(?:\.\d+)?)b', param_part, re.IGNORECASE)
                if match:
                    return float(match.group(1))
        except:
            pass
        return 0
    
    def _get_model_range(self, models: List[str]) -> str:
        """ëª¨ë¸ë“¤ì˜ íŒŒë¼ë¯¸í„° ë²”ìœ„ë¥¼ ë¶„ì„í•˜ì—¬ small/medium/large ë°˜í™˜"""
        params = []
        for model in models:
            param_size = self._extract_model_params(model)
            if param_size > 0:
                params.append(param_size)
        
        if not params:
            return "unknown"
        
        max_param = max(params)
        
        if max_param <= 10:
            return "small"
        elif max_param <= 20:
            return "medium"
        else:
            return "large"
    
    def _clean_enhanced_text(self, text: str, model_name: str) -> str:
        """enhanced text ì •ë¦¬ - ìˆ«ì ì œê±° ë° ë”°ì˜´í‘œ ì¶”ê°€"""
        # ê¸°ë³¸ ì •ë¦¬
        cleaned = text.strip()
        
        # gemma3ì˜ ê²½ìš° ì•ì˜ ìˆ«ì ì œê±° (ì˜ˆ: "1. A petite..." -> "A petite...")
        if model_name == 'gemma3':
            cleaned = re.sub(r'^\d+\.\s*', '', cleaned)
        
        # qwen3ì™€ deepseekì˜ ê²½ìš° ë”°ì˜´í‘œ ì¶”ê°€
        if model_name in ['qwen3', 'deepseek-r1']:
            # ì´ë¯¸ ë”°ì˜´í‘œê°€ ìˆìœ¼ë©´ ì œê±° í›„ ë‹¤ì‹œ ì¶”ê°€
            cleaned = cleaned.strip('"\'')
            cleaned = f'"{cleaned}"'
        
        return cleaned
    
    def _prompts_similar(self, prompt1: str, prompt2: str) -> bool:
        """ë‘ í”„ë¡¬í”„íŠ¸ì˜ ìœ ì‚¬ë„ í™•ì¸"""
        words1 = set(prompt1.lower().split())
        words2 = set(prompt2.lower().split())
        
        if not words1 or not words2:
            return False
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        similarity = intersection / union
        
        return similarity > 0.7  # 70% ì´ìƒ ìœ ì‚¬
    
    def run_full_pipeline(self, models: List[str]) -> str:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=== Starting Automated Prompt Generation Pipeline ===")
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ìƒ˜í”Œë§
        self.load_and_sample_data()
        
        # 2. prompts.txt ìƒì„±
        prompts_file = self.generate_prompts_file()
        
        # 3. Ollama ëª¨ë¸ë“¤ ì‹¤í–‰
        llm_results = self.run_ollama_models(prompts_file, models)
        
        if not llm_results:
            print("No successful LLM results. Aborting.")
            return ""
        
        # 4. í–¥ìƒëœ Excel íŒŒì¼ ìƒì„± (í†µí•© ì •ê·œí™” + ê°ì²´ëª… ìƒì„±)
        excel_path = self.create_excel_output_enhanced(llm_results)
        
        print("=== Pipeline Completed ===")
        print(f"Results saved to: {excel_path}")
        print(f"Output directory: {self.output_dir}")
        
        return excel_path

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê¸°ë³¸ ì„¤ì •
    metadata_path = "datasets/HSSD/metadata.csv"
    num_samples = 100
    
    # ì‚¬ìš©í•  Ollama ëª¨ë¸ë“¤ (ì‚¬ìš©ìê°€ ì„¤ì¹˜ëœ ëª¨ë¸ë¡œ ìˆ˜ì • í•„ìš”)
    models = [
        "gemma3:1b",
        "gemma3:12b", 
        "qwen3:0.6b",
        "qwen3:1.7b",
        "qwen3:14b",
        "deepseek-r1:1.5b"
    ]
    
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1:
        metadata_path = sys.argv[1]
    if len(sys.argv) > 2:
        num_samples = int(sys.argv[2])
    
    print(f"Metadata path: {metadata_path}")
    print(f"Number of samples: {num_samples}")
    print(f"Models to use: {models}")
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    generator = PromptGenerator(metadata_path, num_samples)
    result_path = generator.run_full_pipeline(models)
    
    if result_path:
        print(f"\nâœ“ Success! Results saved to: {result_path}")
    else:
        print("\nâœ— Pipeline failed!")

if __name__ == "__main__":
    main()