#!/usr/bin/env python3
"""
í–¥ìƒëœ LLM ì¶œë ¥ ì •ê·œí™” ë„êµ¬
JSON í˜•ì‹ìœ¼ë¡œ í†µí•©ëœ ì •ê·œí™” ë° ê°ì²´ëª… ìƒì„±
"""

import re
import json
import os
import subprocess
from pathlib import Path
from typing import List, Dict, Tuple
import pandas as pd

class EnhancedNormalizer:
    def __init__(self):
        self.unified_prompt = """
You are an expert text processing assistant. Your task is to process LLM-generated prompt enhancement outputs and extract structured information.

Given raw LLM output containing original prompts and their enhanced versions, extract and return the data in this exact JSON format for each pair:

{"object_name": "<main_object>", "user_prompt": "<original_short_prompt>", "text_prompt": "<enhanced_detailed_prompt>"}

Rules:
1. object_name: Extract the main object (1-3 words, singular form, no articles)
2. user_prompt: Original short prompt (clean, no numbering)  
3. text_prompt: Enhanced detailed prompt (clean, descriptive)
4. Return ONLY valid JSON objects, one per line
5. No explanations, headers, or additional text

Examples:
Input: 1. "Dark wooden table":"A dark wooden side table with carved legs..."
Output: {"object_name": "Table", "user_prompt": "Dark wooden table", "text_prompt": "A dark wooden side table with carved legs and a smooth surface finish"}

Input: 2. "Red sports car":"A sleek red sports car with aerodynamic design..."  
Output: {"object_name": "Car", "user_prompt": "Red sports car", "text_prompt": "A sleek red sports car with aerodynamic design and chrome details"}

Now process this input:

"""

    def get_best_model(self, available_models: List[str]) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ì—ì„œ íŒŒë¼ë¯¸í„° í¬ê¸°ê°€ ê°€ì¥ í° ëª¨ë¸ ì„ íƒ"""
        def extract_params(model_name):
            try:
                if ':' in model_name:
                    param_part = model_name.split(':', 1)[1]
                    match = re.search(r'(\d+(?:\.\d+)?)b', param_part, re.IGNORECASE)
                    if match:
                        return float(match.group(1))
            except:
                pass
            return 0
        
        if not available_models:
            return None
            
        best_model = max(available_models, key=extract_params)
        best_params = extract_params(best_model)
        
        print(f"Selected best normalization model: {best_model} ({best_params}B params)")
        return best_model

    def normalize_with_best_model(self, llm_results: Dict[str, str]) -> List[Dict]:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ëª¨ë“  LLM ì¶œë ¥ì„ ì •ê·œí™” (ì™¸ë¶€ API ìš°ì„ )"""
        
        # 1. ì™¸ë¶€ API ìš°ì„  ì‹œë„
        try:
            from external_api_normalizer import ExternalAPINormalizer
            
            api_normalizer = ExternalAPINormalizer()
            
            # í™œì„±í™”ëœ APIê°€ ìˆëŠ”ì§€ í™•ì¸
            has_enabled_api = any(api["enabled"] for api in [
                api_normalizer.config["groq"], 
                api_normalizer.config["openai"], 
                api_normalizer.config["claude"], 
                api_normalizer.config["gemini"]
            ])
            
            if has_enabled_api:
                print("ğŸš€ Trying external API for fast normalization...")
                result = api_normalizer.normalize_with_external_api(llm_results)
                if result:
                    return result
            else:
                print("No external APIs configured, falling back to local Ollama")
                
        except Exception as e:
            print(f"External API failed: {e}")
            print("Falling back to local Ollama...")
        
        # 2. ë¡œì»¬ Ollama ëª¨ë¸ë¡œ fallback
        return self.normalize_with_ollama(llm_results)

    def normalize_with_ollama(self, llm_results: Dict[str, str]) -> List[Dict]:
        """ë¡œì»¬ Ollama ëª¨ë¸ë¡œ ì •ê·œí™”"""
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
            if result.returncode != 0:
                print("Error: Cannot access Ollama")
                return self.fallback_normalize(llm_results)
            
            available_models = []
            lines = result.stdout.strip().split('\n')[1:]  # Skip header
            for line in lines:
                if line.strip():
                    model_name = line.split()[0]
                    available_models.append(model_name)
                    
        except Exception as e:
            print(f"Error checking models: {e}")
            return self.fallback_normalize(llm_results)
        
        # ìµœê³  ëª¨ë¸ ì„ íƒ
        best_model = self.get_best_model(available_models)
        if not best_model:
            print("No suitable model found, using fallback normalization")
            return self.fallback_normalize(llm_results)
        
        print(f"Using local Ollama model: {best_model}")
        
        # ëª¨ë“  LLM ì¶œë ¥ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        combined_content = ""
        for model, file_path in llm_results.items():
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                combined_content += f"\n=== Output from {model} ===\n{content}\n"
        
        # í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±
        full_prompt = self.unified_prompt + combined_content
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        temp_prompt_file = Path(list(llm_results.values())[0]).parent / "temp_unified_prompt.txt"
        with open(temp_prompt_file, 'w', encoding='utf-8') as f:
            f.write(full_prompt)
        
        try:
            # ìµœê³  ëª¨ë¸ë¡œ ì •ê·œí™” ì‹¤í–‰
            cmd = f"ollama run {best_model} < {temp_prompt_file}"
            result = subprocess.run(
                cmd,
                shell=True,
                capture_output=True,
                text=True,
                timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                # JSON ì¶œë ¥ íŒŒì‹±
                normalized_data = self.parse_json_output(result.stdout)
                
                # ê²°ê³¼ ì €ì¥
                output_file = temp_prompt_file.parent / f"unified_normalized_{best_model.replace(':', '_')}.txt"
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(result.stdout)
                
                print(f"âœ“ Unified normalization completed: {len(normalized_data)} entries")
                print(f"âœ“ Raw output saved to: {output_file}")
                
                return normalized_data
            else:
                print(f"âœ— Ollama normalization failed: {result.stderr}")
                return self.fallback_normalize(llm_results)
                
        except subprocess.TimeoutExpired:
            print("âœ— Ollama normalization timed out")
            return self.fallback_normalize(llm_results)
        except Exception as e:
            print(f"âœ— Error during Ollama normalization: {e}")
            return self.fallback_normalize(llm_results)
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if temp_prompt_file.exists():
                temp_prompt_file.unlink()

    def parse_json_output(self, output: str) -> List[Dict]:
        """JSON ì¶œë ¥ íŒŒì‹±"""
        normalized_data = []
        
        lines = output.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line and line.startswith('{') and line.endswith('}'):
                try:
                    data = json.loads(line)
                    
                    # í•„ìˆ˜ í•„ë“œ í™•ì¸
                    if all(key in data for key in ['object_name', 'user_prompt', 'text_prompt']):
                        normalized_data.append({
                            'object_name': str(data['object_name']).strip(),
                            'user_prompt': str(data['user_prompt']).strip(),
                            'text_prompt': str(data['text_prompt']).strip()
                        })
                        
                except json.JSONDecodeError:
                    continue
        
        print(f"Successfully parsed {len(normalized_data)} JSON entries")
        return normalized_data

    def fallback_normalize(self, llm_results: Dict[str, str]) -> List[Dict]:
        """LLM ì‹¤íŒ¨ì‹œ rule-based ì •ê·œí™”"""
        print("Using fallback rule-based normalization")
        
        from object_name_generator import ObjectNameGenerator
        obj_generator = ObjectNameGenerator()
        
        normalized_data = []
        
        for model, file_path in llm_results.items():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ê¸°ì¡´ ì •ê·œí™” íŒ¨í„´ë“¤
                patterns = [
                    r'"(\d+\.\s*[^"]+)"\s*:\s*"([^"]+)"',
                    r'\d+\.\s*"([^"]+)"\s*:\s*"([^"]+)"',
                    r'\d+\.\s*([^:]+):\s*([^\n]+)',
                    r'"([^"]+)"\s*:\s*"([^"]+)"'
                ]
                
                for pattern in patterns:
                    matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
                    if matches:
                        for original, enhanced in matches:
                            clean_original = re.sub(r'^\d+\.\s*', '', original).strip().strip('"\'')
                            clean_enhanced = enhanced.strip().strip('"\'')
                            
                            if clean_original and clean_enhanced:
                                object_name = obj_generator._fallback_object_extraction(clean_original)
                                
                                normalized_data.append({
                                    'object_name': object_name,
                                    'user_prompt': clean_original,
                                    'text_prompt': clean_enhanced
                                })
                        break
                        
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
                continue
        
        print(f"Fallback normalization completed: {len(normalized_data)} entries")
        return normalized_data

    def create_unified_excel(self, normalized_data: List[Dict], metadata_dict: Dict, output_path: str, llm_results: Dict[str, str] = None) -> str:
        """í†µí•©ëœ Excel íŒŒì¼ ìƒì„± - ëª¨ë¸ë³„ë¡œ ì¤‘ë³µ ìƒì„±"""
        
        # ì›ë³¸ LLM ê²°ê³¼ì—ì„œ ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ë§¤í•‘ ìƒì„±
        model_prompt_map = {}
        if llm_results:
            print(f"Creating model mapping from {len(llm_results)} LLM result files...")
            
            for model, file_path in llm_results.items():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    print(f"Parsing {model} results...")
                    
                    # ê¸°ë³¸ íŒ¨í„´ìœ¼ë¡œ íŒŒì‹±í•˜ì—¬ í•´ë‹¹ ëª¨ë¸ì˜ í”„ë¡¬í”„íŠ¸ë“¤ ì¶”ì¶œ
                    patterns = [
                        r'"(\d+\.\s*[^"]+)"\s*:\s*"([^"]+)"',
                        r'\d+\.\s*"([^"]+)"\s*:\s*"([^"]+)"',
                        r'\d+\.\s*([^:]+):\s*([^\n]+)',
                        r'"([^"]+)"\s*:\s*"([^"]+)"'
                    ]
                    
                    found_matches = 0
                    for pattern in patterns:
                        matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
                        if matches:
                            for original, enhanced in matches:
                                clean_original = re.sub(r'^\d+\.\s*', '', original).strip().strip('"\'').lower()
                                model_prompt_map[clean_original] = model
                                found_matches += 1
                            break
                    
                    print(f"  Found {found_matches} prompts from {model}")
                            
                except Exception as e:
                    print(f"Warning: Could not parse {model} for model mapping: {e}")
            
            print(f"Total mapped prompts: {len(model_prompt_map)}")
            if model_prompt_map:
                print("Sample mappings:")
                for i, (prompt, model) in enumerate(list(model_prompt_map.items())[:3]):
                    print(f"  '{prompt[:30]}...' â†’ {model}")
        
        # ë©”íƒ€ë°ì´í„°ì™€ ë§¤ì¹­
        for entry in normalized_data:
            user_prompt = entry['user_prompt'].lower().strip()
            
            # ì›ë³¸ ë°ì´í„°ì—ì„œ ë§¤ì¹­ ì‹œë„
            for original_data in metadata_dict.values():
                if original_data['original_caption'].lower().strip() == user_prompt:
                    entry.update({
                        'sha256': original_data['sha256'],
                        'file_identifier': original_data['file_identifier']
                    })
                    break
            else:
                # ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ë¹ˆ ê°’
                entry.update({
                    'sha256': '',
                    'file_identifier': ''
                })
            
            # LLM ëª¨ë¸ ì •ë³´ ë§¤ì¹­ (ì •í™• ë§¤ì¹­ ìš°ì„ , ê·¸ ë‹¤ìŒ ìœ ì‚¬ ë§¤ì¹­)
            matched_model = model_prompt_map.get(user_prompt, None)
            
            if not matched_model and model_prompt_map:
                # ì •í™• ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ìœ ì‚¬ ë§¤ì¹­ ì‹œë„
                best_match = None
                best_ratio = 0
                
                for mapped_prompt, model in model_prompt_map.items():
                    # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨)
                    user_words = set(user_prompt.lower().split())
                    mapped_words = set(mapped_prompt.lower().split())
                    
                    if user_words and mapped_words:
                        intersection = len(user_words & mapped_words)
                        union = len(user_words | mapped_words)
                        ratio = intersection / union
                        
                        if ratio > best_ratio and ratio > 0.6:  # 60% ì´ìƒ ìœ ì‚¬
                            best_ratio = ratio
                            best_match = model
                
                if best_match:
                    matched_model = best_match
                    print(f"Fuzzy matched '{user_prompt[:30]}...' to {best_match} (similarity: {best_ratio:.2f})")
            
            entry['source_model'] = matched_model or 'unknown'
        
        df_data = []
        # DataFrame ìƒì„± - ê° ëª¨ë¸ë³„ë¡œ ëª¨ë“  ì—”íŠ¸ë¦¬ë¥¼ ì¤‘ë³µ ìƒì„±                                                                                                                                                                                                                                                                      â”‚                                                                                                                                                                                                                                                                                                              â”‚
        models_used = set()                                                                                                                                                                                                                                                                                                         â”‚
        # ë¨¼ì € ì›ë³¸ LLM ê²°ê³¼ì—ì„œ ì‚¬ìš©ëœ ëª¨ë¸ë“¤ íŒŒì•…                                                                                                                                                                                                                                                                                 â”‚
        if llm_results:
            models_used = set(llm_results.keys())  
        # ê° ì •ê·œí™”ëœ ì—”íŠ¸ë¦¬ì— ëŒ€í•´ ì˜¬ë°”ë¥¸ ëª¨ë¸ ì •ë³´ í• ë‹¹
        for model in models_used:
            category, parameters = self.extract_model_info(model)
            for entry in normalized_data:
                row = {
                    'category': category,
                    'llm_model': model,  # í•´ë‹¹ user_promptë¥¼ ìƒì„±í•œ ì›ë³¸ ëª¨ë¸ëª… ì‚¬ìš©
                    'parameters': parameters,
                    'size': 'unknown',
                    'GPU_usage': 'unknown',
                    'object_name': entry['object_name'],
                    'seed': '',
                    'params': '',
                    'matched_image': '',
                    'object_name_clean': entry['object_name'],
                    'user_prompt': entry['user_prompt'],
                    'text_prompt': entry['text_prompt'],
                    'sha256': entry.get('sha256', ''),
                    'file_identifier': entry.get('file_identifier', '')
                }
                df_data.append(row)
        
        # ë§Œì•½ ëª¨ë¸ì´ ì—†ë‹¤ë©´ unknownìœ¼ë¡œ ì²˜ë¦¬ (ì´ ë¶€ë¶„ì€ ìœ„ ë¡œì§ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ë¯€ë¡œ ì‚¬ì‹¤ìƒ ë¶ˆí•„ìš”)
        if not models_used:
            for entry in normalized_data:
                row = {
                    'category': 'unknown',
                    'llm_model': 'unknown',
                    'parameters': 'unknown',
                    'size': 'unknown',
                    'GPU_usage': 'unknown',
                    'object_name': entry['object_name'],
                    'seed': '',
                    'params': '',
                    'matched_image': '',
                    'object_name_clean': entry['object_name'],
                    'user_prompt': entry['user_prompt'],
                    'text_prompt': entry['text_prompt'],
                    'sha256': entry.get('sha256', ''),
                    'file_identifier': entry.get('file_identifier', '')
                }
                df_data.append(row)
        
        # Excel íŒŒì¼ ì €ì¥
        df = pd.DataFrame(df_data)
        df.to_excel(output_path, index=False)
        
        print(f"âœ“ Unified Excel file created: {output_path}")
        print(f"âœ“ Total entries: {len(df_data)}")
        
        # ëª¨ë¸ ë¶„í¬ í‘œì‹œ
        model_dist = pd.Series([row['llm_model'] for row in df_data]).value_counts().to_dict()
        print(f"âœ“ Original generation model distribution:")
        for model, count in model_dist.items():
            print(f"  - {model}: {count} prompts")
        
        return output_path

    def extract_model_info(self, model_name: str) -> tuple:
        """ëª¨ë¸ëª…ì—ì„œ ì¹´í…Œê³ ë¦¬ì™€ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ì¶œ"""
        if model_name == 'unknown' or not model_name:
            return 'unknown', 'unknown'
        
        try:
            # ì˜ˆ: "qwen3:14b-q8_0" -> category="qwen3", parameters="14b"
            if ':' in model_name:
                category = model_name.split(':', 1)[0]
                param_part = model_name.split(':', 1)[1]
                
                # íŒŒë¼ë¯¸í„° í¬ê¸° ì¶”ì¶œ (ì˜ˆ: "14b-q8_0" -> "14b")
                import re
                param_match = re.search(r'(\d+(?:\.\d+)?b)', param_part, re.IGNORECASE)
                parameters = param_match.group(1) if param_match else param_part.split('-')[0]
                
                return category, parameters
            else:
                return model_name, 'unknown'
                
        except Exception:
            return model_name, 'unknown'

    def extract_api_model_params(self, model_name: str) -> str:
        """ì™¸ë¶€ API ëª¨ë¸ëª…ì—ì„œ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ì¶œ"""
        if not model_name:
            return 'unknown'
        
        try:
            # ë‹¤ì–‘í•œ API ëª¨ë¸ëª… íŒ¨í„´ ì²˜ë¦¬
            model_lower = model_name.lower()
            
            # Groq/Llama ëª¨ë¸: llama-3.1-70b-versatile -> 70b
            if 'llama' in model_lower:
                import re
                match = re.search(r'(\d+(?:\.\d+)?b)', model_name, re.IGNORECASE)
                return match.group(1) if match else 'unknown'
            
            # OpenAI ëª¨ë¸: gpt-4o-mini -> 4o-mini
            elif 'gpt' in model_lower:
                if 'gpt-4o-mini' in model_lower:
                    return '4o-mini'
                elif 'gpt-4o' in model_lower:
                    return '4o'
                elif 'gpt-4' in model_lower:
                    return '4'
                elif 'gpt-3.5' in model_lower:
                    return '3.5'
                else:
                    return 'unknown'
            
            # Claude ëª¨ë¸: claude-3-5-haiku-20241022 -> 3.5-haiku  
            elif 'claude' in model_lower:
                if 'claude-3-5-sonnet' in model_lower:
                    return '3.5-sonnet'
                elif 'claude-3-5-haiku' in model_lower:
                    return '3.5-haiku'
                elif 'claude-3-opus' in model_lower:
                    return '3-opus'
                elif 'claude-3' in model_lower:
                    return '3'
                else:
                    return 'unknown'
            
            # Gemini ëª¨ë¸: gemini-1.5-flash -> 1.5-flash
            elif 'gemini' in model_lower:
                if 'gemini-1.5-pro' in model_lower:
                    return '1.5-pro'
                elif 'gemini-1.5-flash' in model_lower:
                    return '1.5-flash'
                elif 'gemini-1.5' in model_lower:
                    return '1.5'
                else:
                    return 'unknown'
            
            else:
                return 'unknown'
                
        except Exception:
            return 'unknown'

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    normalizer = EnhancedNormalizer()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_output = '''
1. "Dark wooden table":"A dark wooden side table with carved legs and smooth surface"
2. "Red sports car":"A sleek red sports car with aerodynamic design and chrome details"
    '''
    
    # ì„ì‹œ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸
    test_file = "test_unified_output.txt"
    with open(test_file, 'w') as f:
        f.write(test_output)
    
    llm_results = {"test_model": test_file}
    result = normalizer.normalize_with_best_model(llm_results)
    
    print(f"Test result: {result}")
    
    # ì •ë¦¬
    if os.path.exists(test_file):
        os.remove(test_file)

if __name__ == "__main__":
    main()