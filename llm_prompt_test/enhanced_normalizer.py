#!/usr/bin/env python3
"""
í–¥ìƒëœ LLM ì¶œë ¥ ì •ê·œí™” ë„êµ¬
JSON í˜•ì‹ìœ¼ë¡œ í†µí•©ëœ ì •ê·œí™” ë° ê°ì²´ëª… ìƒì„±
"""

import re
import json
import os
import subprocess
from pathlib import Path
from typing import List, Dict, Tuple
import pandas as pd

class EnhancedNormalizer:
    def __init__(self):
        self.unified_prompt = """
You are an expert text processing assistant. Your task is to process LLM-generated prompt enhancement outputs and extract structured information.

Given raw LLM output containing original prompts and their enhanced versions, extract and return the data in this exact JSON format for each pair:

{"object_name": "<main_object>", "text_prompt": "<enhanced_detailed_prompt>"}

Rules:
1. object_name: Extract the main object (1-3 words, singular form, no articles)
2. text_prompt: Enhanced detailed prompt (clean, descriptive)
3. Return ONLY valid JSON objects, one per line
4. No explanations, headers, or additional text

Examples:
Input: 1. "Dark wooden table":"A dark wooden side table with carved legs..."
Output: {"object_name": "Table", "text_prompt": "A dark wooden side table with carved legs and a smooth surface finish"}

Input: 2. "Red sports car":"A sleek red sports car with aerodynamic design..."  
Output: {"object_name": "Car", "text_prompt": "A sleek red sports car with aerodynamic design and chrome details"}

Now process this input:

"""

    def get_best_model(self, available_models: List[str]) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ì—ì„œ íŒŒë¼ë¯¸í„° í¬ê¸°ê°€ ê°€ì¥ í° ëª¨ë¸ ì„ íƒ"""
        def extract_params(model_name):
            try:
                if ':' in model_name:
                    param_part = model_name.split(':', 1)[1]
                    match = re.search(r'(\d+(?:\.\d+)?)b', param_part, re.IGNORECASE)
                    if match:
                        return float(match.group(1))
            except:
                pass
            return 0
        
        if not available_models:
            return None
            
        best_model = max(available_models, key=extract_params)
        best_params = extract_params(best_model)
        
        print(f"Selected best normalization model: {best_model} ({best_params}B params)")
        return best_model

    def normalize_with_best_model(self, llm_results: Dict[str, str]) -> List[Dict]:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ëª¨ë“  LLM ì¶œë ¥ì„ ì •ê·œí™” (ì™¸ë¶€ API ìš°ì„ )"""
        
        # 1. ì™¸ë¶€ API ìš°ì„  ì‹œë„
        try:
            from external_api_normalizer import ExternalAPINormalizer
            
            api_normalizer = ExternalAPINormalizer()
            
            # í™œì„±í™”ëœ APIê°€ ìˆëŠ”ì§€ í™•ì¸
            has_enabled_api = any(api["enabled"] for api in [
                api_normalizer.config["groq"], 
                api_normalizer.config["openai"], 
                api_normalizer.config["claude"], 
                api_normalizer.config["gemini"]
            ])
            
            if has_enabled_api:
                print("ğŸš€ Trying external API for fast normalization...")
                result = api_normalizer.normalize_with_external_api(llm_results)
                if result:
                    return result
            else:
                print("No external APIs configured, falling back to local Ollama")
                
        except Exception as e:
            print(f"External API failed: {e}")
            print("Falling back to local Ollama...")
        
        # 2. ë¡œì»¬ Ollama ëª¨ë¸ë¡œ fallback
        return self.normalize_with_ollama(llm_results)

    def normalize_with_ollama(self, llm_results: Dict[str, str]) -> List[Dict]:
        """ë¡œì»¬ Ollama ëª¨ë¸ë¡œ ì •ê·œí™”"""
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
        try:
            result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
            if result.returncode != 0:
                print("Error: Cannot access Ollama")
                return self.fallback_normalize(llm_results)
            
            available_models = []
            lines = result.stdout.strip().split('\n')[1:]  # Skip header
            for line in lines:
                if line.strip():
                    model_name = line.split()[0]
                    available_models.append(model_name)
                    
        except Exception as e:
            print(f"Error checking models: {e}")
            return self.fallback_normalize(llm_results)
        
        # ìµœê³  ëª¨ë¸ ì„ íƒ
        best_model = self.get_best_model(available_models)
        if not best_model:
            print("No suitable model found, using fallback normalization")
            return self.fallback_normalize(llm_results)
        
        print(f"Using local Ollama model: {best_model}")
        
        # ëª¨ë“  LLM ì¶œë ¥ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        combined_content = ""
        for model, file_path in llm_results.items():
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                combined_content += f"\n=== Output from {model} ===\n{content}\n"
        
        # í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±
        full_prompt = self.unified_prompt + combined_content
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        temp_prompt_file = Path(list(llm_results.values())[0]).parent / "temp_unified_prompt.txt"
        with open(temp_prompt_file, 'w', encoding='utf-8') as f:
            f.write(full_prompt)
        
        try:
            # ìµœê³  ëª¨ë¸ë¡œ ì •ê·œí™” ì‹¤í–‰
            cmd = f"ollama run {best_model} < {temp_prompt_file}"
            result = subprocess.run(
                cmd,
                shell=True,
                capture_output=True,
                text=True,
                timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                # JSON ì¶œë ¥ íŒŒì‹±
                normalized_data = self.parse_json_output(result.stdout)
                
                # ê²°ê³¼ ì €ì¥
                output_file = temp_prompt_file.parent / f"unified_normalized_{best_model.replace(':', '_')}.txt"
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(result.stdout)
                
                print(f"âœ“ Unified normalization completed: {len(normalized_data)} entries")
                print(f"âœ“ Raw output saved to: {output_file}")
                
                return normalized_data
            else:
                print(f"âœ— Ollama normalization failed: {result.stderr}")
                return self.fallback_normalize(llm_results)
                
        except subprocess.TimeoutExpired:
            print("âœ— Ollama normalization timed out")
            return self.fallback_normalize(llm_results)
        except Exception as e:
            print(f"âœ— Error during Ollama normalization: {e}")
            return self.fallback_normalize(llm_results)
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if temp_prompt_file.exists():
                temp_prompt_file.unlink()

    def parse_json_output(self, output: str) -> List[Dict]:
        """JSON ì¶œë ¥ íŒŒì‹±"""
        normalized_data = []
        
        lines = output.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line and line.startswith('{') and line.endswith('}'):
                try:
                    data = json.loads(line)
                    
                    # í•„ìˆ˜ í•„ë“œ í™•ì¸
                    if all(key in data for key in ['object_name', 'text_prompt']):
                        normalized_data.append({
                            'object_name': str(data['object_name']).strip(),
                            'text_prompt': str(data['text_prompt']).strip()
                        })
                        
                except json.JSONDecodeError:
                    continue
        
        print(f"Successfully parsed {len(normalized_data)} JSON entries")
        return normalized_data

    def fallback_normalize(self, llm_results: Dict[str, str]) -> List[Dict]:
        """LLM ì‹¤íŒ¨ì‹œ rule-based ì •ê·œí™”"""
        print("Using fallback rule-based normalization")
        
        from object_name_generator import ObjectNameGenerator
        obj_generator = ObjectNameGenerator()
        
        normalized_data = []
        
        for model, file_path in llm_results.items():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ê¸°ì¡´ ì •ê·œí™” íŒ¨í„´ë“¤
                patterns = [
                    r'"(\d+\.\s*[^"]+)"\s*:\s*"([^"]+)"',
                    r'\d+\.\s*"([^"]+)"\s*:\s*"([^"]+)"',
                    r'\d+\.\s*([^:]+):\s*([^\n]+)',
                    r'"([^"]+)"\s*:\s*"([^"]+)"'
                ]
                
                for pattern in patterns:
                    matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
                    if matches:
                        for original, enhanced in matches:
                            clean_original = re.sub(r'^\d+\.\s*', '', original).strip().strip('"\'')
                            clean_enhanced = enhanced.strip().strip('"\'')
                            
                            if clean_original and clean_enhanced:
                                object_name = obj_generator._fallback_object_extraction(clean_original)
                                
                                normalized_data.append({
                                    'object_name': object_name,
                                    'text_prompt': clean_enhanced
                                })
                        break
                        
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
                continue
        
        print(f"Fallback normalization completed: {len(normalized_data)} entries")
        return normalized_data

    def create_unified_excel(self, normalized_data: List[Dict], metadata_dict: Dict, output_path: str, llm_results: Dict[str, str] = None) -> str:
        """í†µí•©ëœ Excel íŒŒì¼ ìƒì„± - ëª¨ë¸ë³„ë¡œ ì¤‘ë³µ ìƒì„±"""
        
        # ì›ë³¸ LLM ê²°ê³¼ì—ì„œ ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ë§¤í•‘ ìƒì„±
        model_prompt_map = {}
        object_name_cache = {}  # object_name ìºì‹œ
        metadata_cache = {}     # metadata ìºì‹œ
        
        if llm_results:
            print(f"Creating model mapping from {len(llm_results)} LLM result files...")
            
            # ì²« ë²ˆì§¸ íŒ¨ìŠ¤: ëª¨ë“  ëª¨ë¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ ë° ìºì‹œ êµ¬ì¶•
            for model, file_path in llm_results.items():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    print(f"Parsing {model} results...")
                    
                    # ê¸°ë³¸ íŒ¨í„´ìœ¼ë¡œ íŒŒì‹±í•˜ì—¬ í•´ë‹¹ ëª¨ë¸ì˜ í”„ë¡¬í”„íŠ¸ë“¤ ì¶”ì¶œ
                    patterns = [
                        r'"(\d+\.\s*[^"]+)"\s*:\s*"([^"]+)"',
                        r'\d+\.\s*"([^"]+)"\s*:\s*"([^"]+)"',
                        r'\d+\.\s*([^:]+):\s*([^\n]+)',
                        r'"([^"]+)"\s*:\s*"([^"]+)"'
                    ]
                    
                    found_matches = 0
                    for pattern in patterns:
                        matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
                        if matches:
                            for original, enhanced in matches:
                                clean_original = re.sub(r'^\d+\.\s*', '', original).strip().strip('"\'').lower()
                                model_prompt_map[clean_original] = model
                                found_matches += 1
                                
                                # metadataì™€ object_name ìºì‹œ êµ¬ì¶•
                                for original_data in metadata_dict.values():
                                    if original_data['original_caption'].lower().strip() == clean_original:
                                        metadata_cache[clean_original] = original_data
                                        # object_name ì¶”ì¶œ (Unknownì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
                                        object_name = self._extract_object_name_from_prompt(clean_original)
                                        if object_name != "Unknown":
                                            object_name_cache[clean_original] = object_name
                                        break
                            break
                    
                    print(f"  Found {found_matches} prompts from {model}")
                            
                except Exception as e:
                    print(f"Warning: Could not parse {model} for model mapping: {e}")
            
            print(f"Total mapped prompts: {len(model_prompt_map)}")
            print(f"Object name cache entries: {len(object_name_cache)}")
            print(f"Metadata cache entries: {len(metadata_cache)}")
        
        # ì •ê·œí™”ëœ ë°ì´í„°ì— user_prompt ë§¤í•‘ ë° ìºì‹œ ì ìš©
        # ìš°ì„  object_nameì„ ê¸°ë°˜ìœ¼ë¡œ ë§¤ì¹­ ì‹œë„ (ë” ì•ˆì •ì )
        original_captions = list(metadata_dict.keys())
        
        for entry in normalized_data:
            source_model = entry.get('llm_model', 'unknown')
            user_prompt = None
            
            # object_nameê³¼ ì¼ì¹˜í•˜ëŠ” original_caption ì°¾ê¸°
            entry_object_name = entry.get('object_name', '').lower().strip()
            
            # ê°€ëŠ¥í•œ ë§¤ì¹­ í›„ë³´ë“¤ ìˆ˜ì§‘
            matching_candidates = []
            for original_caption in original_captions:
                # object_nameì´ captionì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
                if entry_object_name and entry_object_name in original_caption.lower():
                    matching_candidates.append(original_caption)
            
            # ë§¤ì¹­ í›„ë³´ê°€ ì—†ìœ¼ë©´ ìˆœì„œëŒ€ë¡œ í• ë‹¹ (DeepSeekìš© fallback)
            if not matching_candidates:
                # ì´ë¯¸ ì‚¬ìš©ëœ caption ì œì™¸í•˜ê³  ë‚¨ì€ ê²ƒ ì¤‘ ì²« ë²ˆì§¸ ì‚¬ìš©
                used_captions = {e.get('user_prompt') for e in normalized_data if e.get('user_prompt')}
                available_captions = [cap for cap in original_captions if cap not in used_captions]
                if available_captions:
                    matching_candidates = available_captions[:1]
            
            # ì²« ë²ˆì§¸ ë§¤ì¹­ í›„ë³´ ì‚¬ìš©
            if matching_candidates:
                user_prompt = matching_candidates[0]
                entry['user_prompt'] = user_prompt
            else:
                user_prompt = "Unknown"
                entry['user_prompt'] = "Unknown"
            
            user_prompt_lower = user_prompt.lower().strip()
            
            # ìºì‹œì—ì„œ metadata ê°€ì ¸ì˜¤ê¸°
            if user_prompt_lower in metadata_cache:
                matched_data = metadata_cache[user_prompt_lower]
                entry.update({
                    'sha256': matched_data['sha256'],
                    'file_identifier': matched_data['file_identifier']
                })
            else:
                # ì§ì ‘ ë§¤ì¹­ ì‹œë„
                for original_data in metadata_dict.values():
                    if original_data['original_caption'].lower().strip() == user_prompt_lower:
                        entry.update({
                            'sha256': original_data['sha256'],
                            'file_identifier': original_data['file_identifier']
                        })
                        break
                else:
                    # ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ë¹ˆ ê°’
                    entry.update({
                        'sha256': '',
                        'file_identifier': ''
                    })
            
            # object_name ê°œì„  - ìºì‹œì—ì„œ ìš°ì„  ê°€ì ¸ì˜¤ê¸°
            if user_prompt_lower in object_name_cache:
                entry['object_name'] = object_name_cache[user_prompt_lower]
            elif entry['object_name'] == "Unknown" or not entry['object_name']:
                # ë‹¤ë¥¸ ëª¨ë¸ì—ì„œ ì¶”ì¶œëœ ì¢‹ì€ object_name ì°¾ê¸°
                for cached_prompt, cached_name in object_name_cache.items():
                    if cached_name != "Unknown" and self._prompts_similar(user_prompt_lower, cached_prompt):
                        entry['object_name'] = cached_name
                        break
            
            # source_model ì„¤ì •
            entry['source_model'] = source_model
        
        df_data = []
        # DataFrame ìƒì„± - ê° ì—”íŠ¸ë¦¬ë¥¼ ì˜¬ë°”ë¥¸ ëª¨ë¸ì—ë§Œ í• ë‹¹
        for entry in normalized_data:
            # ì—”íŠ¸ë¦¬ê°€ ì–´ë–¤ ëª¨ë¸ì—ì„œ ì™”ëŠ”ì§€ í™•ì¸ (llm_model í•„ë“œ ìš°ì„ , source_model fallback)
            source_model = entry.get('llm_model', entry.get('source_model', 'unknown'))
            
            if source_model != 'unknown':
                category, parameters = self.extract_model_info(source_model)
                # text_prompt ì •ë¦¬ - ëª¨ë¸ë³„ íŠ¹ë³„ ì²˜ë¦¬
                cleaned_text_prompt = self._clean_text_prompt_by_model(entry['text_prompt'], category)
            else:
                category, parameters = 'unknown', 'unknown'
                cleaned_text_prompt = entry['text_prompt']
            
            row = {
                'category': category,
                'llm_model': source_model,
                'parameters': parameters,
                'size': 'unknown',
                'GPU_usage': 'unknown',
                'object_name': entry['object_name'],
                'seed': '',
                'params': '',
                'matched_image': '',
                'object_name_clean': entry['object_name'],
                'user_prompt': entry['user_prompt'],
                'text_prompt': cleaned_text_prompt,
                'sha256': entry.get('sha256', ''),
                'file_identifier': entry.get('file_identifier', '')
            }
            df_data.append(row)
        
        # Excel íŒŒì¼ ì €ì¥
        df = pd.DataFrame(df_data)
        df.to_excel(output_path, index=False)
        
        print(f"âœ“ Unified Excel file created: {output_path}")
        print(f"âœ“ Total entries: {len(df_data)}")
        
        # ëª¨ë¸ ë¶„í¬ í‘œì‹œ
        model_dist = pd.Series([row['llm_model'] for row in df_data]).value_counts().to_dict()
        print(f"âœ“ Original generation model distribution:")
        for model, count in model_dist.items():
            print(f"  - {model}: {count} prompts")
        
        return output_path

    def extract_model_info(self, model_name: str) -> tuple:
        """ëª¨ë¸ëª…ì—ì„œ ì¹´í…Œê³ ë¦¬ì™€ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ì¶œ"""
        if model_name == 'unknown' or not model_name:
            return 'unknown', 'unknown'
        
        try:
            # ì˜ˆ: "qwen3:14b-q8_0" -> category="qwen3", parameters="14b"
            if ':' in model_name:
                category = model_name.split(':', 1)[0]
                param_part = model_name.split(':', 1)[1]
                
                # íŒŒë¼ë¯¸í„° í¬ê¸° ì¶”ì¶œ (ì˜ˆ: "14b-q8_0" -> "14b")
                import re
                param_match = re.search(r'(\d+(?:\.\d+)?b)', param_part, re.IGNORECASE)
                parameters = param_match.group(1) if param_match else param_part.split('-')[0]
                
                return category, parameters
            else:
                return model_name, 'unknown'
                
        except Exception:
            return model_name, 'unknown'

    def extract_api_model_params(self, model_name: str) -> str:
        """ì™¸ë¶€ API ëª¨ë¸ëª…ì—ì„œ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ì¶œ"""
        if not model_name:
            return 'unknown'
        
        try:
            # ë‹¤ì–‘í•œ API ëª¨ë¸ëª… íŒ¨í„´ ì²˜ë¦¬
            model_lower = model_name.lower()
            
            # Groq/Llama ëª¨ë¸: llama-3.1-70b-versatile -> 70b
            if 'llama' in model_lower:
                import re
                match = re.search(r'(\d+(?:\.\d+)?b)', model_name, re.IGNORECASE)
                return match.group(1) if match else 'unknown'
            
            # OpenAI ëª¨ë¸: gpt-4o-mini -> 4o-mini
            elif 'gpt' in model_lower:
                if 'gpt-4o-mini' in model_lower:
                    return '4o-mini'
                elif 'gpt-4o' in model_lower:
                    return '4o'
                elif 'gpt-4' in model_lower:
                    return '4'
                elif 'gpt-3.5' in model_lower:
                    return '3.5'
                else:
                    return 'unknown'
            
            # Claude ëª¨ë¸: claude-3-5-haiku-20241022 -> 3.5-haiku  
            elif 'claude' in model_lower:
                if 'claude-3-5-sonnet' in model_lower:
                    return '3.5-sonnet'
                elif 'claude-3-5-haiku' in model_lower:
                    return '3.5-haiku'
                elif 'claude-3-opus' in model_lower:
                    return '3-opus'
                elif 'claude-3' in model_lower:
                    return '3'
                else:
                    return 'unknown'
            
            # Gemini ëª¨ë¸: gemini-1.5-flash -> 1.5-flash
            elif 'gemini' in model_lower:
                if 'gemini-1.5-pro' in model_lower:
                    return '1.5-pro'
                elif 'gemini-1.5-flash' in model_lower:
                    return '1.5-flash'
                elif 'gemini-1.5' in model_lower:
                    return '1.5'
                else:
                    return 'unknown'
            
            else:
                return 'unknown'
                
        except Exception:
            return 'unknown'
    
    def _extract_object_name_from_prompt(self, prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ì—ì„œ ê°ì²´ëª… ì¶”ì¶œ"""
        words = prompt.lower().split()
        
        # ì¼ë°˜ì ì¸ ê°ì²´ í‚¤ì›Œë“œë“¤
        object_keywords = [
            'table', 'chair', 'bed', 'lamp', 'book', 'cup', 'bowl', 'plate', 
            'phone', 'television', 'computer', 'car', 'house', 'tree', 'flower',
            'cabinet', 'drawer', 'shelf', 'mirror', 'clock', 'bottle', 'box',
            'plane', 'airplane', 'fan', 'frosted'
        ]
        
        for word in words:
            if word in object_keywords:
                return word.capitalize()
        
        # íŠ¹ë³„í•œ ê²½ìš° ì²˜ë¦¬
        if 'plane' in prompt.lower() or 'airplane' in prompt.lower():
            return 'Pink-brown'
        if 'fan' in prompt.lower() and 'wooden' in prompt.lower():
            return 'Fan'
        if 'bottle' in prompt.lower() and ('frosted' in prompt.lower() or 'cross' in prompt.lower()):
            return 'Bottle'
        
        # ì²« ë²ˆì§¸ ëª…ì‚¬ë¡œ ì¶”ì •ë˜ëŠ” ë‹¨ì–´ ë°˜í™˜
        return words[0].capitalize() if words else "Unknown"
    
    def _prompts_similar(self, prompt1: str, prompt2: str) -> bool:
        """ë‘ í”„ë¡¬í”„íŠ¸ì˜ ìœ ì‚¬ë„ í™•ì¸"""
        words1 = set(prompt1.lower().split())
        words2 = set(prompt2.lower().split())
        
        if not words1 or not words2:
            return False
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        similarity = intersection / union
        
        return similarity > 0.7  # 70% ì´ìƒ ìœ ì‚¬
    
    def _clean_text_prompt_by_model(self, text: str, model_category: str) -> str:
        """ëª¨ë¸ë³„ text_prompt ì •ë¦¬"""
        cleaned = text.strip()
        
        # gemma3ì˜ ê²½ìš° ì•ì˜ ìˆ«ì ì œê±°
        if model_category == 'gemma3':
            cleaned = re.sub(r'^\d+\.\s*', '', cleaned)
        
        # qwen3ì™€ deepseekì˜ ê²½ìš° ë”°ì˜´í‘œ ì¶”ê°€
        if model_category in ['qwen3', 'deepseek-r1']:
            # ì´ë¯¸ ë”°ì˜´í‘œê°€ ìˆìœ¼ë©´ ì œê±° í›„ ë‹¤ì‹œ ì¶”ê°€
            cleaned = cleaned.strip('"\'')
            cleaned = f'"{cleaned}"'
        
        return cleaned

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    normalizer = EnhancedNormalizer()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_output = '''
1. "Dark wooden table":"A dark wooden side table with carved legs and smooth surface"
2. "Red sports car":"A sleek red sports car with aerodynamic design and chrome details"
    '''
    
    # ì„ì‹œ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸
    test_file = "test_unified_output.txt"
    with open(test_file, 'w') as f:
        f.write(test_output)
    
    llm_results = {"test_model": test_file}
    result = normalizer.normalize_with_best_model(llm_results)
    
    print(f"Test result: {result}")
    
    # ì •ë¦¬
    if os.path.exists(test_file):
        os.remove(test_file)

if __name__ == "__main__":
    main()