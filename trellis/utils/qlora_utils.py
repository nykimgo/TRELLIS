import torch
import torch.nn as nn
from typing import List
from contextlib import contextmanager
import types
import math

try:
    import bitsandbytes as bnb  # type: ignore
except Exception:  # pragma: no cover - optional dependency
    bnb = None

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

from ..modules import sparse as sp
from ..modules.sparse.linear import SparseLinear

from ..utils.elastic_utils import ElasticModuleMixin  # Mixin í´ë˜ìŠ¤ import

import torch.nn.functional as F

class SparseDropout(nn.Module):
    """SparseTensorì™€ í˜¸í™˜ë˜ëŠ” ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´."""
    def __init__(self, p=0.5):
        super().__init__()
        self.p = p

    def forward(self, input):
        # í•™ìŠµ ì¤‘ì´ ì•„ë‹ˆê±°ë‚˜ ë“œë¡­ì•„ì›ƒ í™•ë¥ ì´ 0ì´ë©´ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ
        if not self.training or self.p == 0:
            return input
        
        # ì…ë ¥ì´ SparseTensorì¸ ê²½ìš°, ê·¸ ë‚´ìš©ë¬¼(feats)ì—ë§Œ ë“œë¡­ì•„ì›ƒ ì ìš©
        if isinstance(input, sp.SparseTensor):
            new_feats = F.dropout(input.feats, p=self.p, training=self.training)
            return input.replace(feats=new_feats)
        
        # ì¼ë°˜ í…ì„œëŠ” ê·¸ëŒ€ë¡œ ë“œë¡­ì•„ì›ƒ ì ìš©
        return F.dropout(input, p=self.p, training=self.training)

def _replace_lora_dropout_modules(module: nn.Module):
    """
    LoraLayerë¥¼ ìˆœíšŒí•˜ë©° ë‚´ë¶€ì˜ Dropout ëª¨ë“ˆì„ SparseDropoutìœ¼ë¡œ êµì²´í•©ë‹ˆë‹¤.
    """
    from peft.tuners.lora import LoraLayer
    
    for child_module in module.children():
        # LoraLayerë¥¼ ì°¾ìŒ
        if isinstance(child_module, LoraLayer):
            # lora_dropout ì†ì„±ê³¼ ModuleDictê°€ ìˆëŠ”ì§€ í™•ì¸
            if hasattr(child_module, 'lora_dropout') and isinstance(child_module.lora_dropout, nn.ModuleDict):
                for adapter_name, dropout_layer in child_module.lora_dropout.items():
                    # nn.Dropout ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì°¾ì•„ì„œ
                    if isinstance(dropout_layer, nn.Dropout):
                        p = dropout_layer.p
                        # ìš°ë¦¬ê°€ ë§Œë“  SparseDropoutìœ¼ë¡œ êµì²´
                        child_module.lora_dropout[adapter_name] = SparseDropout(p)
        else:
            # ë‹¤ë¥¸ ëª¨ë“  ëª¨ë“ˆì— ëŒ€í•´ ì¬ê·€ì ìœ¼ë¡œ í•¨ìˆ˜ í˜¸ì¶œ
            _replace_lora_dropout_modules(child_module)

def _new_lora_linear_forward(self, x):
    """
    SparseTensorë¥¼ ì¸ì‹í•˜ëŠ” ìƒˆë¡œìš´ nn.Linear.forward ë©”ì„œë“œ.
    'self'ëŠ” nn.Linearì˜ ì¸ìŠ¤í„´ìŠ¤ì…ë‹ˆë‹¤.
    """
    if isinstance(x, sp.SparseTensor):
        # ì…ë ¥ì´ SparseTensorì´ë©´, ê·¸ ë‚´ìš©ë¬¼(feats)ì—ë§Œ linear ì—°ì‚°ì„ ì ìš©
        return x.replace(feats=F.linear(x.feats, self.weight, self.bias))
    # ì¼ë°˜ í…ì„œëŠ” ì›ë˜ì˜ linear ì—°ì‚°ì„ ê·¸ëŒ€ë¡œ ìˆ˜í–‰
    return F.linear(x, self.weight, self.bias)

def _patch_lora_linear_layers(module: nn.Module):
    """
    ëª¨ë¸ì„ ìˆœíšŒí•˜ë©° ëª¨ë“  LoraLayerì˜ lora_A, lora_B ì„ í˜• ê³„ì¸µì˜
    forward ë©”ì„œë“œë¥¼ SparseTensorë¥¼ ì¸ì‹í•˜ëŠ” ë²„ì „ìœ¼ë¡œ êµì²´(ëª½í‚¤ íŒ¨ì¹˜)í•©ë‹ˆë‹¤.
    """
    from peft.tuners.lora import LoraLayer
    import types
    
    for child_module in module.children():
        # LoraLayer (QuantizedLinear, SparseLinear ë“±ì„ ê°ì‹¸ëŠ”)ë¥¼ ì°¾ìŒ
        if isinstance(child_module, LoraLayer):
            # lora_A ëª¨ë“ˆì´ ìˆë‹¤ë©´, ëª¨ë“  ì–´ëŒ‘í„°ì— ëŒ€í•´ íŒ¨ì¹˜ ì ìš©
            if hasattr(child_module, 'lora_A'):
                for adapter in child_module.lora_A.values():
                    adapter.forward = types.MethodType(_new_lora_linear_forward, adapter)
            # lora_B ëª¨ë“ˆì´ ìˆë‹¤ë©´, ëª¨ë“  ì–´ëŒ‘í„°ì— ëŒ€í•´ íŒ¨ì¹˜ ì ìš©
            if hasattr(child_module, 'lora_B'):
                for adapter in child_module.lora_B.values():
                    adapter.forward = types.MethodType(_new_lora_linear_forward, adapter)
        else:
            # ë‹¤ë¥¸ ëª¨ë“  ëª¨ë“ˆì— ëŒ€í•´ ì¬ê·€ì ìœ¼ë¡œ í•¨ìˆ˜ í˜¸ì¶œ
            _patch_lora_linear_layers(child_module)




class SparseLinear4bit(bnb.nn.Linear4bit):  # type: ignore
    """4bit quantized linear layer that accepts a :class:`SparseTensor`."""

    def forward(self, input: sp.SparseTensor):  # type: ignore[override]
        return input.replace(super().forward(input.feats))


def _replace_linear(module: nn.Module) -> None:
    """Recursively replace Linear/SparseLinear with 4bit versions."""
    if bnb is None:
        return
    for name, child in list(module.named_children()):
        if isinstance(child, SparseLinear):
            new_linear = SparseLinear4bit(
                child.in_features,
                child.out_features,
                bias=child.bias is not None,
            )
            new_linear.weight.data.copy_(child.weight.data)
            if child.bias is not None:
                new_linear.bias.data.copy_(child.bias.data)
            setattr(module, name, new_linear)
        elif isinstance(child, nn.Linear):
            new_linear = bnb.nn.Linear4bit(
                child.in_features,
                child.out_features,
                bias=child.bias is not None,
            )
            new_linear.weight.data.copy_(child.weight.data)
            if child.bias is not None:
                new_linear.bias.data.copy_(child.bias.data)
            setattr(module, name, new_linear)
        else:
            _replace_linear(child)


def _get_target_modules_for_trellis(model_type: str = "structured_latent_flow") -> List[str]:
    """TRELLIS ğ“–_L ëª¨ë¸ì— íŠ¹í™”ëœ íƒ€ê²Ÿ ëª¨ë“ˆ íŒ¨í„´"""
    if model_type == "structured_latent_flow":
        return [
            # Self/Cross attention projections in transformer blocks
            "blocks.*.attn.q_proj",
            "blocks.*.attn.k_proj", 
            "blocks.*.attn.v_proj",
            "blocks.*.attn.o_proj",
            # FFN layers in transformer blocks
            "blocks.*.mlp.mlp.0",  # First linear layer in FFN
            "blocks.*.mlp.mlp.2",  # Second linear layer in FFN
            # Input/Output blocks attention and FFN
            "input_blocks.*.attn.*_proj",
            "input_blocks.*.mlp.*",
            "out_blocks.*.attn.*_proj", 
            "out_blocks.*.mlp.*",
            # Main input/output projections
            "input_layer",
            "out_layer",
            # ResBlock skip connections (SparseLinear)
            "*.skip_connection",
        ]
    else:
        # ë‹¤ë¥¸ ëª¨ë¸ íƒ€ì…ì„ ìœ„í•œ ì¼ë°˜ì ì¸ íŒ¨í„´
        return ["*.proj", "*.linear", "*.mlp.*"]


def _find_linear_module_names(model: nn.Module, use_patterns: bool = True, model_type: str = "structured_latent_flow") -> List[str]:
    """Finds the names of all linear and sparse linear modules in the model."""
    if use_patterns:
        # TRELLISì— íŠ¹í™”ëœ íŒ¨í„´ ì‚¬ìš©
        target_patterns = _get_target_modules_for_trellis(model_type)
        names = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, SparseLinear)):
                # íŒ¨í„´ ë§¤ì¹­ í™•ì¸
                for pattern in target_patterns:
                    import fnmatch
                    if fnmatch.fnmatch(name, pattern):
                        names.append(name)
                        break
        return names
    else:
        # ê¸°ì¡´ ë°©ì‹: ëª¨ë“  Linear ë ˆì´ì–´
        names = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, SparseLinear)):
                names.append(name)
        return names


class ElasticPeftModel(ElasticModuleMixin):
    """
    PeftModelì˜ ElasticModuleMixin í˜¸í™˜ ë˜í¼
    ì €ìˆ˜ì¤€ Elastic ìƒí˜¸ì‘ìš©ì„ ì™„ì „íˆ ì§€ì›
    DDP/Peft/Samplerì˜ ë³µì¡í•œ ì¸ì ì „ë‹¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” 'ì¸ì ë°©í™”ë²½' ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, peft_model, original_model):
        super().__init__()  # <--- ìƒì†ë°›ì€ Mixinì˜ __init__ ë©”ì„œë“œ í˜¸ì¶œ
        # PeftModelì˜ ëª¨ë“  ì†ì„±ê³¼ ë©”ì„œë“œë¥¼ ìœ„ì„
        self.__dict__['_peft_model'] = peft_model
        self.__dict__['_original_model'] = original_model
        '''
        # DDP í˜¸í™˜ì„±ì„ ìœ„í•œ ì„¤ì •
        self._ddp_params_and_buffers_to_ignore = set()
        
        # Frozen íŒŒë¼ë¯¸í„°ë“¤ì„ DDPì—ì„œ ë¬´ì‹œí•˜ë„ë¡ ì„¤ì •
        for name, param in peft_model.named_parameters():
            if not param.requires_grad:
                self._ddp_params_and_buffers_to_ignore.add(name)
        '''
        
    def __getattr__(self, name):
        """PeftModelì˜ ì†ì„±/ë©”ì„œë“œì— ì ‘ê·¼"""
        return getattr(self._peft_model, name)
    
    def __setattr__(self, name, value):
        """ì†ì„± ì„¤ì •ì„ PeftModelì— ìœ„ì„"""
        if name.startswith('_') or name in ['_peft_model', '_original_model']:
            self.__dict__[name] = value
        else:
            setattr(self._peft_model, name, value)
    
    def _get_input_size(self, x, *args, **kwargs):
        """ì…ë ¥ í¬ê¸° ê³„ì‚°"""
        if hasattr(x, 'feats'):  # SparseTensor
            return x.feats.shape[0]
        return x.shape[0]

    @contextmanager
    def with_mem_ratio(self, mem_ratio=1.0):
        """
        ë©”ëª¨ë¦¬ ë¹„ìœ¨ì— ë”°ë¥¸ gradient checkpointing ì„¤ì •
        ì›ë³¸ ëª¨ë¸ì˜ blocksì— ì§ì ‘ ì ‘ê·¼í•˜ì—¬ ì œì–´
        """
        if mem_ratio == 1.0:
            yield 1.0
            return
        
        # ì›ë³¸ ëª¨ë¸ì—ì„œ blocks ì ‘ê·¼
        base_model = self._peft_model.base_model.model  # PeftModel -> base_model -> ì‹¤ì œ ëª¨ë¸
        
        if not hasattr(base_model, 'blocks'):
            # blocksê°€ ì—†ìœ¼ë©´ ì›ë³¸ ëª¨ë¸ì˜ with_mem_ratio ì‚¬ìš©
            if hasattr(self._original_model, 'with_mem_ratio'):
                with self._original_model.with_mem_ratio(mem_ratio) as exact_ratio:
                    yield exact_ratio
            else:
                yield mem_ratio
            return
        
        # SparseTransformerElasticMixinê³¼ ë™ì¼í•œ ë¡œì§
        num_blocks = len(base_model.blocks)
        num_checkpoint_blocks = min(math.ceil((1 - mem_ratio) * num_blocks) + 1, num_blocks)
        exact_mem_ratio = 1 - (num_checkpoint_blocks - 1) / num_blocks
        
        # ì›ë³¸ ìƒíƒœ ì €ì¥
        original_checkpoints = []
        for i in range(num_blocks):
            original_checkpoints.append(base_model.blocks[i].use_checkpoint)
            base_model.blocks[i].use_checkpoint = i < num_checkpoint_blocks
        
        try:
            yield exact_mem_ratio
        finally:
            # ì›ë³¸ ìƒíƒœ ë³µì›
            for i in range(num_blocks):
                base_model.blocks[i].use_checkpoint = original_checkpoints[i]
    
    def register_memory_controller(self, memory_controller):
        """ë©”ëª¨ë¦¬ ì»¨íŠ¸ë¡¤ëŸ¬ ë“±ë¡"""
        self._memory_controller = memory_controller
        # ì›ë³¸ ëª¨ë¸ì—ë„ ì „ë‹¬ (í˜¸í™˜ì„±)
        if hasattr(self._original_model, 'register_memory_controller'):
            self._original_model.register_memory_controller(memory_controller)

    def __call__(self, *args, **kwargs):
        """
        ì–´ë–¤ í˜•íƒœì˜ ì¸ìê°€ ë“¤ì–´ì˜¤ë“  forwardë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
        """
        return self.forward(*args, **kwargs)

    def forward(self, *args, **kwargs):
        """
        'ì¸ì ë°€ìˆ˜' ì‘ì „ì˜ ì‹œì‘ì .
        x, t, condë¥¼ 'smuggled_args' ê°€ë°©ì— ë‹´ì•„ Peftê°€ ê±´ë“œë¦¬ì§€ ëª»í•˜ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.
        """
        # Samplerë¡œë¶€í„° ë°›ì€ ì¸ìë¥¼ ì¶”ì¶œ
        x = args[0]
        t = args[1]
        cond = args[2]

        # ëª¨ë“  ì¸ìë¥¼ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ì— í¬ì¥
        smuggled_args = {'x': x, 't': t, 'cond': cond}

        # --- Elastic ë¡œì§ (ì´ì „ê³¼ ë™ì¼) ---
        if (self._memory_controller is None or
            not torch.is_grad_enabled() or
            not self.training):
            # **í•µì‹¬**: ì¸ìë“¤ì„ 'smuggled_args' í‚¤ì›Œë“œ ì¸ì í•˜ë‚˜ë¡œ ì „ë‹¬
            return self._peft_model(smuggled_args=smuggled_args)
        else:
            # Elastic forward
            input_size = self._get_input_size(x)
            mem_ratio = self._memory_controller.get_mem_ratio(input_size)
            with self.with_mem_ratio(mem_ratio) as exact_mem_ratio:
                # **í•µì‹¬**: ì¸ìë“¤ì„ 'smuggled_args' í‚¤ì›Œë“œ ì¸ì í•˜ë‚˜ë¡œ ì „ë‹¬
                ret = self._peft_model(smuggled_args=smuggled_args)
            self._memory_controller.update_run_states(input_size, exact_mem_ratio)
            return ret

    def state_dict(self, *args, **kwargs):
        """state_dictë¥¼ PeftModelì— ìœ„ì„"""
        return self._peft_model.state_dict(*args, **kwargs)
    
    def load_state_dict(self, *args, **kwargs):
        """load_state_dictë¥¼ PeftModelì— ìœ„ì„"""
        return self._peft_model.load_state_dict(*args, **kwargs)
    
    def parameters(self, *args, **kwargs):
        """parametersë¥¼ PeftModelì— ìœ„ì„"""
        return self._peft_model.parameters(*args, **kwargs)
    
    def named_parameters(self, *args, **kwargs):
        """named_parametersë¥¼ PeftModelì— ìœ„ì„"""
        return self._peft_model.named_parameters(*args, **kwargs)
    
    def train(self, mode=True):
        """train ëª¨ë“œë¥¼ PeftModelì— ìœ„ì„"""
        return self._peft_model.train(mode)
    
    def eval(self):
        """eval ëª¨ë“œë¥¼ PeftModelì— ìœ„ì„"""
        return self._peft_model.eval()
    
    def to(self, *args, **kwargs):
        """ë””ë°”ì´ìŠ¤ ì´ë™ì„ PeftModelì— ìœ„ì„"""
        result = self._peft_model.to(*args, **kwargs)
        return self  # ë˜í¼ ìì²´ë¥¼ ë°˜í™˜
    
    def cuda(self, *args, **kwargs):
        """CUDA ì´ë™ì„ PeftModelì— ìœ„ì„"""
        self._peft_model.cuda(*args, **kwargs)
        return self


def apply_qlora(
    model: nn.Module,
    r: int = 8,
    lora_alpha: int = 16,
    lora_dropout: float = 0.0,
    quantize: bool = True,
    use_target_patterns: bool = True,
    model_type: str = "structured_latent_flow",
    verbose: bool = True,
) -> nn.Module:
    """Apply QLoRA to a model.

    Args:
        model: Model to modify in-place.
        r: LoRA rank.
        lora_alpha: LoRA alpha scaling.
        lora_dropout: LoRA dropout.
        quantize: If ``True`` and bitsandbytes is available, replace linear
            layers with 4-bit quantized versions before attaching LoRA adapters.
        use_target_patterns: If ``True``, use TRELLIS-specific target patterns.
        model_type: Type of model for pattern selection.
        verbose: If ``True``, print detailed information.
    Returns:
        The modified model.
    """
    
    # ì›ë³¸ ë””ë°”ì´ìŠ¤ ì €ì¥
    original_device = next(model.parameters()).device
    
    # Elastic ì†ì„± ì²´í¬
    is_elastic = isinstance(model, ElasticModuleMixin)
    original_model = model  # ì›ë³¸ ì°¸ì¡° ì €ì¥
    
    if is_elastic and verbose:
        print("Warning: Elastic model detected. Creating full Elastic-compatible wrapper...")
    
    if quantize and bnb is not None:
        _replace_linear(model)
        model = prepare_model_for_kbit_training(model)

    target_modules = _find_linear_module_names(model, use_target_patterns, model_type)
    
    # ë””ë²„ê¹…ì„ ìœ„í•œ íƒ€ê²Ÿ ëª¨ë“ˆ ì¶œë ¥
    if verbose:
        print(f"QLoRA will be applied to {len(target_modules)} modules:")
        for module_name in target_modules[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
            print(f"  - {module_name}")
        if len(target_modules) > 10:
            print(f"  ... and {len(target_modules) - 10} more modules")
    
    lora_config = LoraConfig(
        r=r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules,
        bias="none",
        task_type="FEATURE_EXTRACTION",  # 3D ìƒì„± ëª¨ë¸ì— ì í•©í•œ task_type
    )
    peft_model = get_peft_model(model, lora_config)
    
    # LoRA ë ˆì´ì–´ì˜ Dropoutì„ Sparse-aware ë²„ì „ìœ¼ë¡œ êµì²´
    _replace_lora_dropout_modules(peft_model)
    # LoRA ë ˆì´ì–´ì˜ Linearë¥¼ Sparse-aware ë²„ì „ìœ¼ë¡œ êµì²´ (ëª½í‚¤ íŒ¨ì¹˜)
    _patch_lora_linear_layers(peft_model)

    # Elastic í˜¸í™˜ì„±ì´ í•„ìš”í•œ ê²½ìš° ì™„ì „í•œ ë˜í¼ ìƒì„±
    if is_elastic:
        elastic_model = ElasticPeftModel(peft_model, original_model)
        model = elastic_model
        
        '''
        if verbose:
            print("   âœ… Full Elastic compatibility wrapper created")
            print("   ğŸ”§ Supports: blocks manipulation, memory controller, checkpointing")
        '''
    else:
        model = peft_model
    
    # QLoRA ì ìš© í›„ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì›ë³¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    model = model.to(original_device)
    
    # DDP í˜¸í™˜ì„±ì„ ìœ„í•´ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ê°™ì€ ë””ë°”ì´ìŠ¤ì— ìˆëŠ”ì§€ í™•ì¸
    devices = {p.device for p in model.parameters()}
    if len(devices) > 1:
        '''if verbose:
            print(f"Warning: Parameters on multiple devices: {devices}")
            print("Moving all parameters to CUDA...")'''
        model = model.cuda()
    
    return model