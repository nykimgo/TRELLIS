
"""
TRELLIS ëª¨ë¸ ì €ì¥ ë° ê²°ê³¼ ê´€ë¦¬ í´ë˜ìŠ¤

ê¸°ëŠ¥:
- CSV/JSON ê²°ê³¼ ì €ì¥
- ì–‘ìí™”ëœ ëª¨ë¸ì„ TRELLIS í˜•ì‹ìœ¼ë¡œ ì €ì¥
- ì‹œê°í™” ê·¸ë˜í”„ ìƒì„±
"""

import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd
import matplotlib.pyplot as plt
import torch

# ìƒëŒ€ ì„í¬íŠ¸ ë¬¸ì œ í•´ê²°
try:
    from performance_measurer import PerformanceMeasurer
except ImportError:
    from .performance_measurer import PerformanceMeasurer


class ModelSaver:
    """ëª¨ë¸ ì €ì¥ ë° ê²°ê³¼ ê´€ë¦¬"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.measurer = PerformanceMeasurer()
    
    def save_results(self, results: List[Dict[str, Any]], model_name: str):
        """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
        try:
            print("ğŸ’¾ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì¤‘...")
            
            # CSV ì €ì¥
            df = pd.DataFrame(results)
            csv_path = self.output_dir / f"trellis_{model_name}_results.csv"
            df.to_csv(csv_path, index=False)
            print(f"  âœ… CSV: {csv_path}")
            
            # ì••ì¶• ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì €ì¥
            compression_metrics = self.measurer.calculate_compression_metrics(results)
            if compression_metrics:
                metrics_path = self.output_dir / f"trellis_{model_name}_metrics.json"
                with open(metrics_path, 'w') as f:
                    json.dump(compression_metrics, f, indent=2)
                print(f"  âœ… ë©”íŠ¸ë¦­: {metrics_path}")
                
                # ê²°ê³¼ ì¶œë ¥
                self._print_compression_results(compression_metrics)
            
            # ì‹œê°í™” ìƒì„±
            self._create_visualization(df, model_name)
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def save_quantized_model(self, pipeline, model_name: str, original_path: str) -> Optional[str]:
        """ì–‘ìí™”ëœ ëª¨ë¸ì„ TRELLIS í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        try:
            print("ğŸ’¾ ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥ ì¤‘...")
            
            # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            save_dir = self.output_dir / f"trellis_{model_name}_quantized"
            save_dir.mkdir(parents=True, exist_ok=True)
            ckpts_dir = save_dir / "ckpts"
            ckpts_dir.mkdir(parents=True, exist_ok=True)
            
            saved_components = []
            
            # ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ë“¤ì„ safetensorsë¡œ ì €ì¥
            if hasattr(pipeline, 'models') and pipeline.models:
                for comp_name, model in pipeline.models.items():
                    if model is not None:
                        success = self._save_component(comp_name, model, ckpts_dir)
                        if success:
                            file_path = ckpts_dir / f"{comp_name}_quantized_int8.safetensors"
                            file_size_mb = file_path.stat().st_size / (1024 * 1024)
                            saved_components.append(f"{comp_name}: {file_size_mb:.1f}MB")
            
            # pipeline.json ìƒì„±
            self._create_pipeline_config(save_dir, original_path, saved_components)
            
            # README ìƒì„±
            self._create_readme(save_dir, model_name, original_path, saved_components)
            
            if saved_components:
                print(f"  âœ… {len(saved_components)}ê°œ ì»´í¬ë„ŒíŠ¸ ì €ì¥ ì™„ë£Œ")
                return str(save_dir)
            else:
                print("  âš ï¸ ì €ì¥ëœ ì»´í¬ë„ŒíŠ¸ ì—†ìŒ")
                return None
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def _save_component(self, name: str, model, ckpts_dir: Path) -> bool:
        """ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì €ì¥"""
        try:
            model_path = ckpts_dir / f"{name}_quantized_int8.safetensors"
            
            # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
            model_cpu = model.cpu()
            state_dict = model_cpu.state_dict()
            torch.save(state_dict, model_path)
            
            # GPUë¡œ ë³µê·€
            if torch.cuda.is_available():
                model.cuda()
            
            return True
            
        except Exception as e:
            print(f"    âŒ {name} ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _create_pipeline_config(self, save_dir: Path, original_path: str, saved_components: List[str]):
        """pipeline.json ì„¤ì • íŒŒì¼ ìƒì„±"""
        try:
            # ì›ë³¸ ì„¤ì • ì½ê¸°
            original_config_path = Path(original_path) / "pipeline.json"
            if original_config_path.exists():
                with open(original_config_path, 'r') as f:
                    config = json.load(f)
                
                # ëª¨ë¸ ê²½ë¡œë¥¼ ì–‘ìí™” ë²„ì „ìœ¼ë¡œ ìˆ˜ì •
                if 'args' in config and 'models' in config['args']:
                    for model_name in config['args']['models']:
                        config['args']['models'][model_name] = f"ckpts/{model_name}_quantized_int8.safetensors"
                
                # ì–‘ìí™” ì •ë³´ ì¶”ê°€
                config['quantization_info'] = {
                    'method': 'dynamic_int8',
                    'original_model': original_path,
                    'quantized_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'saved_components': [comp.split(':')[0] for comp in saved_components]
                }
                
                # ì €ì¥
                config_path = save_dir / "pipeline.json"
                with open(config_path, 'w') as f:
                    json.dump(config, f, indent=2)
                
                print(f"    âœ… pipeline.json ìƒì„±")
                
        except Exception as e:
            print(f"    âš ï¸ pipeline.json ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _create_readme(self, save_dir: Path, model_name: str, original_path: str, saved_components: List[str]):
        """README.md ìƒì„±"""
        readme_path = save_dir / "README.md"
        with open(readme_path, 'w') as f:
            f.write(f"""# TRELLIS {model_name.upper()} Quantized Model (INT8)

## ğŸ“‹ ëª¨ë¸ ì •ë³´
- **ì›ë³¸ ëª¨ë¸**: {original_path}
- **ì–‘ìí™” ë°©ë²•**: Dynamic INT8 Quantization
- **ì–‘ìí™” ì¼ì‹œ**: {time.strftime('%Y-%m-%d %H:%M:%S')}
- **ì €ì¥ëœ ì»´í¬ë„ŒíŠ¸**: {len(saved_components)}ê°œ

## ğŸš€ ì‚¬ìš© ë°©ë²•

```python
from trellis.pipelines import TrellisTextTo3DPipeline

# ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
pipeline = TrellisTextTo3DPipeline.from_pretrained("{save_dir}")
pipeline.cuda()

# 3D ìƒì„±
outputs = pipeline.run("a red sports car", seed=42)

# ê²°ê³¼ ì €ì¥
from trellis.utils import postprocessing_utils
glb = postprocessing_utils.to_glb(outputs['gaussian'][0], outputs['mesh'][0])
glb.export("output.glb")
```

## ğŸ“Š ì €ì¥ëœ ì»´í¬ë„ŒíŠ¸
""")
            for component in saved_components:
                f.write(f"- {component}\n")
            
            f.write(f"""
## âš ï¸ ì£¼ì˜ì‚¬í•­
- INT8 ì–‘ìí™”ë¡œ ì¸í•´ ì›ë³¸ ëŒ€ë¹„ ì•½ê°„ì˜ í’ˆì§ˆ ì €í•˜ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì¶”ë¡  ì†ë„ëŠ” ê°œì„ ë©ë‹ˆë‹¤
""")
    
    def _create_visualization(self, df: pd.DataFrame, model_name: str):
        """ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"""
        try:
            if len(df) < 2:
                return
            
            # ì—ëŸ¬ê°€ ìˆëŠ” í–‰ ì œì™¸
            df_clean = df[~df.get('error', pd.Series(False, index=df.index)).notna()]
            if df_clean.empty:
                return
            
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
            
            # 1. íŒŒë¼ë¯¸í„° ìˆ˜
            ax1.bar(df_clean['model_name'], df_clean['total_params_M'])
            ax1.set_title('Model Parameters (M)')
            ax1.set_ylabel('Parameters (Millions)')
            
            # 2. ëª¨ë¸ í¬ê¸°
            ax2.bar(df_clean['model_name'], df_clean['model_size_MB'])
            ax2.set_title('Model Size (MB)')
            ax2.set_ylabel('Size (MB)')
            
            # 3. GPU ë©”ëª¨ë¦¬
            ax3.bar(df_clean['model_name'], df_clean['gpu_memory_MB'])
            ax3.set_title('GPU Memory Usage (MB)')
            ax3.set_ylabel('Memory (MB)')
            
            # 4. ì¶”ë¡  ì‹œê°„
            ax4.bar(df_clean['model_name'], df_clean['inference_time_ms'])
            ax4.set_title('Inference Time (ms)')
            ax4.set_ylabel('Time (ms)')
            
            plt.tight_layout()
            
            # ì €ì¥
            plot_path = self.output_dir / f"trellis_{model_name}_comparison.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"  âœ… ì‹œê°í™”: {plot_path}")
            
        except Exception as e:
            print(f"  âš ï¸ ì‹œê°í™” ì‹¤íŒ¨: {e}")
    
    def _print_compression_results(self, metrics: Dict[str, float]):
        """ì••ì¶• ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ¯ ì••ì¶• íš¨ê³¼:")
        print(f"  â€¢ ì••ì¶•ë¥ : {metrics['compression_ratio']:.1f}x")
        print(f"  â€¢ í¬ê¸° ê°ì†Œ: {metrics['size_reduction_percent']:.1f}%")
        print(f"  â€¢ ë©”ëª¨ë¦¬ ì ˆì•½: {metrics['memory_reduction_percent']:.1f}%")
        print(f"  â€¢ ì†ë„ ë³€í™”: {metrics['speed_change_percent']:+.1f}%")
        print(f"  â€¢ í’ˆì§ˆ ì†ì‹¤: {metrics['quality_loss_percent']:.1f}%")
        print(f"  â€¢ íš¨ìœ¨ì„± ì ìˆ˜: {metrics['efficiency_score']:.3f}")