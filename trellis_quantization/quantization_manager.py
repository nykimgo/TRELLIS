"""
TRELLIS ì–‘ìí™” ê´€ë¦¬ í´ë˜ìŠ¤

ì£¼ìš” ê¸°ëŠ¥:
- ëª¨ë¸ ë¡œë“œ ë° êµ¬ì¡° ë¶„ì„
- Dynamic INT8 ì–‘ìí™” ì ìš©
- ì„±ëŠ¥ ì¸¡ì • ë° ë¹„êµ
"""

import os
import torch
import torch.nn as nn
import time
import gc
from typing import Dict, Any, List, Tuple, Optional
from pathlib import Path

# ìƒëŒ€ ì„í¬íŠ¸ ë¬¸ì œ í•´ê²°
try:
    from model_analyzer import ModelAnalyzer
    from performance_measurer import PerformanceMeasurer
    from model_saver import ModelSaver
except ImportError:
    # ì ˆëŒ€ ì„í¬íŠ¸ë¡œ ì‹œë„
    from .model_analyzer import ModelAnalyzer
    from .performance_measurer import PerformanceMeasurer
    from .model_saver import ModelSaver


class ActivationQuantWrapper(nn.Module):
    """ê°„ë‹¨í•œ ëŒ€ì¹­ í™œì„±í™” ì–‘ìí™” ë˜í¼"""

    def __init__(self, module: nn.Module):
        super().__init__()
        self.module = module

    @staticmethod
    def _quant_tensor(t: torch.Tensor) -> torch.Tensor:
        if not torch.is_floating_point(t):
            return t
        scale = t.abs().max() / 127 if t.numel() > 0 else 1.0
        if scale == 0:
            return t
        q = torch.clamp((t / scale).round(), -128, 127).to(torch.int8)
        dq = q.to(torch.float32) * scale
        return dq

    def forward(self, *args, **kwargs):
        q_args = [self._quant_tensor(a) if isinstance(a, torch.Tensor) else a for a in args]
        q_kwargs = {k: self._quant_tensor(v) if isinstance(v, torch.Tensor) else v for k, v in kwargs.items()}
        out = self.module(*q_args, **q_kwargs)
        if isinstance(out, torch.Tensor):
            return self._quant_tensor(out)
        if isinstance(out, (list, tuple)):
            return type(out)(self._quant_tensor(o) if isinstance(o, torch.Tensor) else o for o in out)


class TRELLISQuantizationManager:
    """TRELLIS ì–‘ìí™” ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    # ì§€ì›ë˜ëŠ” TRELLIS ëª¨ë¸ë“¤
    SUPPORTED_MODELS = {
        "text-base": "/home/sr/TRELLIS/microsoft/TRELLIS-text-base",
        "text-large": "/home/sr/TRELLIS/microsoft/TRELLIS-text-large", 
        "text-xlarge": "/home/sr/TRELLIS/microsoft/TRELLIS-text-xlarge",
        "image-large": "/home/sr/TRELLIS/microsoft/TRELLIS-image-large"
    }
    
    # ì–‘ìí™” ëŒ€ìƒ ë ˆì´ì–´ íƒ€ì…
    QUANTIZABLE_LAYERS = {
        nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, 
        nn.ConvTranspose3d, nn.MultiheadAttention
    }
    
    def __init__(
        self,
        model_name: str = "text-base",
        output_dir: str = "quantization_results",
        modules_to_quantize: Optional[List[str]] = None,
        quantize_weights: bool = True,
        quantize_activations: bool = False,
    ):
        """ì´ˆê¸°í™”

        Args:
            model_name: ì‚¬ìš©í•  TRELLIS ëª¨ë¸ ì´ë¦„
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            modules_to_quantize: ì–‘ìí™”í•  ëª¨ë“ˆ ì´ë¦„ ë¦¬ìŠ¤íŠ¸. Noneì´ë©´ ì „ì²´ ëª¨ë“ˆ ëŒ€ìƒ
            quantize_weights: ê°€ì¤‘ì¹˜ INT8 ì–‘ìí™” ì—¬ë¶€
            quantize_activations: í™œì„±í™” INT8 ì–‘ìí™” ì—¬ë¶€
        """
        self.model_name = model_name
        self.model_path = self.SUPPORTED_MODELS.get(model_name, model_name)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ì–‘ìí™” ì„¤ì •
        self.modules_to_quantize = modules_to_quantize
        self.quantize_weights = quantize_weights
        self.quantize_activations = quantize_activations

        
        # ìƒíƒœ ë³€ìˆ˜
        self.original_pipeline = None
        self.quantized_pipeline = None
        self.model_components = []  # (name, module) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        self.results = []
        
        # í—¬í¼ í´ë˜ìŠ¤ë“¤
        self.analyzer = ModelAnalyzer()
        self.measurer = PerformanceMeasurer()
        self.saver = ModelSaver(self.output_dir)
        
        # í™˜ê²½ ì„¤ì •
        os.environ['SPCONV_ALGO'] = 'native'
        os.environ['ATTN_BACKEND'] = 'xformers'
        
        print(f"ğŸ”§ TRELLIS ì–‘ìí™” ë§¤ë‹ˆì € ì´ˆê¸°í™”")
        print(f"  ğŸ“‚ ëª¨ë¸: {self.model_path}")
        print(f"  ğŸ“ ì¶œë ¥: {self.output_dir}")

        if self.modules_to_quantize:
            print(f"  ğŸ¯ ëŒ€ìƒ ëª¨ë“ˆ: {self.modules_to_quantize}")
        print(f"  âš™ï¸ ê°€ì¤‘ì¹˜ ì–‘ìí™”: {self.quantize_weights}, í™œì„±í™” ì–‘ìí™”: {self.quantize_activations}")
    
    def load_original_model(self) -> bool:
        """ì›ë³¸ ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"ğŸ”„ TRELLIS íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘...")
            
            # ê²½ë¡œ í™•ì¸
            if not os.path.exists(self.model_path):
                print(f"âŒ ëª¨ë¸ ê²½ë¡œ ì—†ìŒ: {self.model_path}")
                self._show_available_models()
                return False
            
            # TRELLIS ëª¨ë“ˆ ì„í¬íŠ¸
            try:
                if "text" in self.model_name:
                    from trellis.pipelines import TrellisTextTo3DPipeline
                    pipeline_class = TrellisTextTo3DPipeline
                else:
                    from trellis.pipelines import TrellisImageTo3DPipeline  
                    pipeline_class = TrellisImageTo3DPipeline
            except ImportError as e:
                print(f"âŒ TRELLIS ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ TRELLIS í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ê±°ë‚˜ PYTHONPATH ì„¤ì •")
                return False
            
            # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
            self.original_pipeline = pipeline_class.from_pretrained(self.model_path)
            
            # GPUë¡œ ì´ë™
            if torch.cuda.is_available():
                try:
                    self.original_pipeline.cuda()
                    print("âœ… ëª¨ë¸ì„ GPUë¡œ ë¡œë“œ")
                except RuntimeError as e:
                    if "out of memory" in str(e).lower():
                        print("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - CPU ì‚¬ìš©")
                        torch.cuda.empty_cache()
                    else:
                        raise e
            
            # ëª¨ë¸ êµ¬ì¡° ë¶„ì„
            self.model_components = self.analyzer.analyze_pipeline(self.original_pipeline)

            # íŠ¹ì • ëª¨ë“ˆë§Œ ì„ íƒì ìœ¼ë¡œ ì–‘ìí™”
            if self.modules_to_quantize:
                self.model_components = [
                    (n, m) for n, m in self.model_components if n in self.modules_to_quantize
                ]
                print(f"ğŸ¯ ì„ íƒëœ ëª¨ë“ˆ ìˆ˜: {len(self.model_components)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def apply_quantization(self) -> bool:
        """ì–‘ìí™” ì ìš©"""
        try:
            print("ğŸ”§ INT8 ì–‘ìí™” ì ìš© ì¤‘...")
            
            if not self.model_components:
                print("âŒ ì–‘ìí™”í•  ì»´í¬ë„ŒíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            gc.collect()
            
            success_count = 0
            total_count = len(self.model_components)
            
            for name, original_module in self.model_components:
                success = self._quantize_component(name, original_module)
                if success:
                    success_count += 1
            
            # ì–‘ìí™”ëœ íŒŒì´í”„ë¼ì¸ ì„¤ì • (ì°¸ì¡° ë³µì‚¬)
            self.quantized_pipeline = self.original_pipeline
            
            torch.cuda.empty_cache()
            gc.collect()
            
            print(f"ğŸ“Š ì–‘ìí™” ê²°ê³¼: {success_count}/{total_count} ì„±ê³µ")
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ ì–‘ìí™” ì‹¤íŒ¨: {e}")
            return False
    
    def _quantize_component(self, name: str, module: nn.Module) -> bool:
        """ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì–‘ìí™”"""
        try:
            print(f"  ğŸ”§ {name} ì–‘ìí™” ì¤‘...")
            
            # íŒŒë¼ë¯¸í„° í™•ì¸
            original_params = sum(p.numel() for p in module.parameters())
            if original_params == 0:
                print(f"    âš ï¸ íŒŒë¼ë¯¸í„° ì—†ìŒ - ê±´ë„ˆëœ€")
                return False
            
            # CPUë¡œ ì´ë™ í›„ í•„ìš” ì‹œ ê°€ì¤‘ì¹˜ ì–‘ìí™”
            module.cpu()
            torch.cuda.empty_cache()
            
            quantized_module = module
            size_reduction = 0.0

            if self.quantize_weights:
                quantized_module = torch.quantization.quantize_dynamic(
                    module,
                    self.QUANTIZABLE_LAYERS,
                    dtype=torch.qint8
                )
                original_size = sum(p.numel() * p.element_size() for p in module.parameters())
                quantized_size = sum(p.numel() * p.element_size() for p in quantized_module.parameters())
                if original_size > 0:
                    size_reduction = ((original_size - quantized_size) / original_size) * 100

            # í™œì„±í™” ì–‘ìí™” ë˜í•‘
            if self.quantize_activations:
                quantized_module = ActivationQuantWrapper(quantized_module)

            # GPUë¡œ ë³µê·€ ë° êµì²´
            if torch.cuda.is_available():
                quantized_module.cuda()
            
            if hasattr(self.original_pipeline, 'models') and name in self.original_pipeline.models:
                self.original_pipeline.models[name] = quantized_module
            elif hasattr(self.original_pipeline, name):
                setattr(self.original_pipeline, name, quantized_module)
            else:
                print(f"    âš ï¸ ì»´í¬ë„ŒíŠ¸ êµì²´ ì‹¤íŒ¨")
                return False
            
            print(f"    âœ… ì™„ë£Œ - í¬ê¸° ê°ì†Œ: {size_reduction:.1f}%")
            return True
            
        except Exception as e:
            print(f"    âŒ ì‹¤íŒ¨: {e}")
            return False
    
    def run_experiment(self) -> bool:
        """ì „ì²´ ì‹¤í—˜ ì‹¤í–‰"""
        try:
            print("ğŸš€ TRELLIS ì–‘ìí™” ì‹¤í—˜ ì‹œì‘")
            print("=" * 50)
            
            # 1. ì›ë³¸ ëª¨ë¸ ë¡œë“œ
            if not self.load_original_model():
                return False
            
            # 2. ì›ë³¸ ì„±ëŠ¥ ì¸¡ì •
            original_metrics = self.measurer.measure_performance(
                self.original_pipeline, "Original (FP32)", self.model_components
            )
            self.results.append(original_metrics)
            
            # 3. ì–‘ìí™” ì ìš©
            if not self.apply_quantization():
                return False
            
            # 4. ì–‘ìí™” ì„±ëŠ¥ ì¸¡ì •
            quantized_metrics = self.measurer.measure_performance(
                self.quantized_pipeline, "Quantized (INT8)", self.model_components
            )
            self.results.append(quantized_metrics)
            
            # 5. ê²°ê³¼ ì €ì¥
            self.saver.save_results(self.results, self.model_name)
            
            # 6. ì–‘ìí™” ëª¨ë¸ ì €ì¥
            quantized_path = self.saver.save_quantized_model(
                self.quantized_pipeline, self.model_name, self.model_path
            )
            
            print("\nâœ… ì‹¤í—˜ ì™„ë£Œ!")
            if quantized_path:
                print(f"ğŸ’¾ ì–‘ìí™” ëª¨ë¸: {quantized_path}")
            print(f"ğŸ“Š ê²°ê³¼ í™•ì¸: {self.output_dir}/")
            
            return True
            
        except Exception as e:
            print(f"âŒ ì‹¤í—˜ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False
    
    def _show_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í‘œì‹œ"""
        print("ğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
        for name, path in self.SUPPORTED_MODELS.items():
            exists = "âœ…" if os.path.exists(path) else "âŒ"
            print(f"  {exists} {name}: {path}")