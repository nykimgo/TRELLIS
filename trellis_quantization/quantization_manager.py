"""
TRELLIS ì–‘ìí™” ê´€ë¦¬ í´ë˜ìŠ¤

ì£¼ìš” ê¸°ëŠ¥:
- ëª¨ë¸ ë¡œë“œ ë° êµ¬ì¡° ë¶„ì„
- Dynamic INT8 ì–‘ìí™” ì ìš©
- ì„±ëŠ¥ ì¸¡ì • ë° ë¹„êµ
"""

import os
import torch
import torch.nn as nn
import time
import gc
from typing import Dict, Any, List, Tuple, Optional
from pathlib import Path

# ìƒëŒ€ ì„í¬íŠ¸ ë¬¸ì œ í•´ê²°
try:
    from model_analyzer import ModelAnalyzer
    from performance_measurer import PerformanceMeasurer
    from model_saver import ModelSaver
except ImportError:
    # ì ˆëŒ€ ì„í¬íŠ¸ë¡œ ì‹œë„
    from .model_analyzer import ModelAnalyzer
    from .performance_measurer import PerformanceMeasurer
    from .model_saver import ModelSaver


class TRELLISQuantizationManager:
    """TRELLIS ì–‘ìí™” ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    # ì§€ì›ë˜ëŠ” TRELLIS ëª¨ë¸ë“¤
    SUPPORTED_MODELS = {
        "text-base": "/home/sr/TRELLIS/microsoft/TRELLIS-text-base",
        "text-large": "/home/sr/TRELLIS/microsoft/TRELLIS-text-large", 
        "text-xlarge": "/home/sr/TRELLIS/microsoft/TRELLIS-text-xlarge",
        "image-large": "/home/sr/TRELLIS/microsoft/TRELLIS-image-large"
    }
    
    # ì–‘ìí™” ëŒ€ìƒ ë ˆì´ì–´ íƒ€ì…
    QUANTIZABLE_LAYERS = {
        nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, 
        nn.ConvTranspose3d, nn.MultiheadAttention
    }
    
    def __init__(self, model_name: str = "text-base", output_dir: str = "quantization_results"):
        """ì´ˆê¸°í™”"""
        self.model_name = model_name
        self.model_path = self.SUPPORTED_MODELS.get(model_name, model_name)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ìƒíƒœ ë³€ìˆ˜
        self.original_pipeline = None
        self.quantized_pipeline = None
        self.model_components = []  # (name, module) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        self.results = []
        
        # í—¬í¼ í´ë˜ìŠ¤ë“¤
        self.analyzer = ModelAnalyzer()
        self.measurer = PerformanceMeasurer()
        self.saver = ModelSaver(self.output_dir)
        
        # í™˜ê²½ ì„¤ì •
        os.environ['SPCONV_ALGO'] = 'native'
        os.environ['ATTN_BACKEND'] = 'xformers'
        
        print(f"ğŸ”§ TRELLIS ì–‘ìí™” ë§¤ë‹ˆì € ì´ˆê¸°í™”")
        print(f"  ğŸ“‚ ëª¨ë¸: {self.model_path}")
        print(f"  ğŸ“ ì¶œë ¥: {self.output_dir}")
    
    def load_original_model(self) -> bool:
        """ì›ë³¸ ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"ğŸ”„ TRELLIS íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘...")
            
            # ê²½ë¡œ í™•ì¸
            if not os.path.exists(self.model_path):
                print(f"âŒ ëª¨ë¸ ê²½ë¡œ ì—†ìŒ: {self.model_path}")
                self._show_available_models()
                return False
            
            # TRELLIS ëª¨ë“ˆ ì„í¬íŠ¸
            try:
                if "text" in self.model_name:
                    from trellis.pipelines import TrellisTextTo3DPipeline
                    pipeline_class = TrellisTextTo3DPipeline
                else:
                    from trellis.pipelines import TrellisImageTo3DPipeline  
                    pipeline_class = TrellisImageTo3DPipeline
            except ImportError as e:
                print(f"âŒ TRELLIS ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ TRELLIS í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ê±°ë‚˜ PYTHONPATH ì„¤ì •")
                return False
            
            # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
            self.original_pipeline = pipeline_class.from_pretrained(self.model_path)
            
            # GPUë¡œ ì´ë™
            if torch.cuda.is_available():
                try:
                    self.original_pipeline.cuda()
                    print("âœ… ëª¨ë¸ì„ GPUë¡œ ë¡œë“œ")
                except RuntimeError as e:
                    if "out of memory" in str(e).lower():
                        print("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - CPU ì‚¬ìš©")
                        torch.cuda.empty_cache()
                    else:
                        raise e
            
            # ëª¨ë¸ êµ¬ì¡° ë¶„ì„
            self.model_components = self.analyzer.analyze_pipeline(self.original_pipeline)
            
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def apply_quantization(self) -> bool:
        """ì–‘ìí™” ì ìš©"""
        try:
            print("ğŸ”§ Dynamic INT8 ì–‘ìí™” ì ìš© ì¤‘...")
            
            if not self.model_components:
                print("âŒ ì–‘ìí™”í•  ì»´í¬ë„ŒíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            gc.collect()
            
            success_count = 0
            total_count = len(self.model_components)
            
            for name, original_module in self.model_components:
                success = self._quantize_component(name, original_module)
                if success:
                    success_count += 1
            
            # ì–‘ìí™”ëœ íŒŒì´í”„ë¼ì¸ ì„¤ì • (ì°¸ì¡° ë³µì‚¬)
            self.quantized_pipeline = self.original_pipeline
            
            torch.cuda.empty_cache()
            gc.collect()
            
            print(f"ğŸ“Š ì–‘ìí™” ê²°ê³¼: {success_count}/{total_count} ì„±ê³µ")
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ ì–‘ìí™” ì‹¤íŒ¨: {e}")
            return False
    
    def _quantize_component(self, name: str, module: nn.Module) -> bool:
        """ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì–‘ìí™”"""
        try:
            print(f"  ğŸ”§ {name} ì–‘ìí™” ì¤‘...")
            
            # íŒŒë¼ë¯¸í„° í™•ì¸
            original_params = sum(p.numel() for p in module.parameters())
            if original_params == 0:
                print(f"    âš ï¸ íŒŒë¼ë¯¸í„° ì—†ìŒ - ê±´ë„ˆëœ€")
                return False
            
            # CPUë¡œ ì´ë™ í›„ ì–‘ìí™”
            module.cpu()
            torch.cuda.empty_cache()
            
            quantized_module = torch.quantization.quantize_dynamic(
                module, 
                self.QUANTIZABLE_LAYERS, 
                dtype=torch.qint8
            )
            
            # í¬ê¸° ê³„ì‚°
            original_size = sum(p.numel() * p.element_size() for p in module.parameters())
            quantized_size = sum(p.numel() * p.element_size() for p in quantized_module.parameters())
            
            size_reduction = ((original_size - quantized_size) / original_size) * 100 if original_size > 0 else 0
            
            # GPUë¡œ ë³µê·€ ë° êµì²´
            if torch.cuda.is_available():
                quantized_module.cuda()
            
            if hasattr(self.original_pipeline, 'models') and name in self.original_pipeline.models:
                self.original_pipeline.models[name] = quantized_module
            elif hasattr(self.original_pipeline, name):
                setattr(self.original_pipeline, name, quantized_module)
            else:
                print(f"    âš ï¸ ì»´í¬ë„ŒíŠ¸ êµì²´ ì‹¤íŒ¨")
                return False
            
            print(f"    âœ… ì™„ë£Œ - í¬ê¸° ê°ì†Œ: {size_reduction:.1f}%")
            return True
            
        except Exception as e:
            print(f"    âŒ ì‹¤íŒ¨: {e}")
            return False
    
    def run_experiment(self) -> bool:
        """ì „ì²´ ì‹¤í—˜ ì‹¤í–‰"""
        try:
            print("ğŸš€ TRELLIS ì–‘ìí™” ì‹¤í—˜ ì‹œì‘")
            print("=" * 50)
            
            # 1. ì›ë³¸ ëª¨ë¸ ë¡œë“œ
            if not self.load_original_model():
                return False
            
            # 2. ì›ë³¸ ì„±ëŠ¥ ì¸¡ì •
            original_metrics = self.measurer.measure_performance(
                self.original_pipeline, "Original (FP32)", self.model_components
            )
            self.results.append(original_metrics)
            
            # 3. ì–‘ìí™” ì ìš©
            if not self.apply_quantization():
                return False
            
            # 4. ì–‘ìí™” ì„±ëŠ¥ ì¸¡ì •
            quantized_metrics = self.measurer.measure_performance(
                self.quantized_pipeline, "Quantized (INT8)", self.model_components
            )
            self.results.append(quantized_metrics)
            
            # 5. ê²°ê³¼ ì €ì¥
            self.saver.save_results(self.results, self.model_name)
            
            # 6. ì–‘ìí™” ëª¨ë¸ ì €ì¥
            quantized_path = self.saver.save_quantized_model(
                self.quantized_pipeline, self.model_name, self.model_path
            )
            
            print("\nâœ… ì‹¤í—˜ ì™„ë£Œ!")
            if quantized_path:
                print(f"ğŸ’¾ ì–‘ìí™” ëª¨ë¸: {quantized_path}")
            print(f"ğŸ“Š ê²°ê³¼ í™•ì¸: {self.output_dir}/")
            
            return True
            
        except Exception as e:
            print(f"âŒ ì‹¤í—˜ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False
    
    def _show_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í‘œì‹œ"""
        print("ğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
        for name, path in self.SUPPORTED_MODELS.items():
            exists = "âœ…" if os.path.exists(path) else "âŒ"
            print(f"  {exists} {name}: {path}")