#!/usr/bin/env python3
"""
TRELLIS ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë„êµ¬

ê¸°ëŠ¥:
1. quick_test() - ì–‘ìí™” ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
2. compare_models() - ì›ë³¸ vs ì–‘ìí™” ëª¨ë¸ ë¹„êµ
3. batch_test() - ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸

ì‚¬ìš©ë²•:
    python model_test.py quick
    python model_test.py compare --prompt "a red car"
    python model_test.py batch
"""

import os
import argparse
import time
from pathlib import Path
import torch

# í™˜ê²½ ì„¤ì •
os.environ['SPCONV_ALGO'] = 'native'

# âš ï¸ ê²½ë¡œ ì„¤ì • - í•„ìš”ì‹œ ìˆ˜ì •í•˜ì„¸ìš”
ORIGINAL_MODEL_PATH = "/home/sr/TRELLIS/microsoft/TRELLIS-text-base"
QUANTIZED_MODEL_PATH = "./quantization_results/trellis_text-base_quantized"

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
TEST_PROMPTS = [
    "a red sports car",
    "a wooden chair", 
    "a blue coffee mug",
    "a small house",
    "a cute cat"
]

QUICK_TEST_PROMPTS = [
    "a red cube",
    "a blue sphere", 
    "a green pyramid"
]


def quick_test():
    """ì–‘ìí™”ëœ ëª¨ë¸ ë¹ ë¥¸ ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ“¥ ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸...")
        print(f"ğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {QUANTIZED_MODEL_PATH}")
        
        from trellis.pipelines import TrellisTextTo3DPipeline
        
        pipeline = TrellisTextTo3DPipeline.from_pretrained(QUANTIZED_MODEL_PATH)
        pipeline.cuda()
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        if hasattr(pipeline, 'models'):
            print("ğŸ“Š ëª¨ë¸ ì»´í¬ë„ŒíŠ¸:")
            for name, model in pipeline.models.items():
                if model is not None:
                    param_count = sum(p.numel() for p in model.parameters())
                    
                    # ì–‘ìí™” ìƒíƒœ í™•ì¸
                    is_quantized = any(
                        hasattr(m, '_packed_params') or 'quantized' in str(type(m)).lower()
                        for m in model.modules()
                    )
                    status = "ğŸ”§INT8" if is_quantized else "ğŸ“FP32"
                    
                    print(f"  - {name}: {param_count/1e6:.1f}M {status}")
        
        print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("ğŸ’¡ ì‹¤ì œ 3D ìƒì„±ì€ 'python model_test.py compare' ì‚¬ìš©")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("\nğŸ”§ í•´ê²° ë°©ë²•:")
        print("1. TRELLIS í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ê³  ìˆëŠ”ì§€ í™•ì¸")
        print("2. QUANTIZED_MODEL_PATH ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
        print("3. ì–‘ìí™” ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸")


def compare_models(prompt: str, seed: int = 42):
    """ì›ë³¸ê³¼ ì–‘ìí™” ëª¨ë¸ ë¹„êµ"""
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸: '{prompt}' (seed: {seed})")
    print(f"ğŸ“‚ ì›ë³¸ ê²½ë¡œ: {ORIGINAL_MODEL_PATH}")
    print(f"ğŸ“‚ ì–‘ìí™” ê²½ë¡œ: {QUANTIZED_MODEL_PATH}")
    
    try:
        from trellis.pipelines import TrellisTextTo3DPipeline
        from trellis.utils import postprocessing_utils
        
        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        safe_name = prompt.replace(' ', '_').replace('/', '_')
        
        # 1. ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print("\nğŸ“¥ ì›ë³¸ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        original_pipeline = TrellisTextTo3DPipeline.from_pretrained(ORIGINAL_MODEL_PATH)
        original_pipeline.cuda()
        
        print("ğŸ”„ ì›ë³¸ ëª¨ë¸ ìƒì„± ì¤‘...")
        original_outputs = original_pipeline.run(prompt, seed=seed)
        
        # ì›ë³¸ ê²°ê³¼ ì €ì¥
        original_glb = postprocessing_utils.to_glb(
            original_outputs['gaussian'][0], 
            original_outputs['mesh'][0],
            simplify=0.95, 
            texture_size=1024
        )
        original_file = f"original_{safe_name}_seed{seed}.glb"
        original_glb.export(original_file)
        print(f"  âœ… ì›ë³¸ ì €ì¥: {original_file}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del original_pipeline
        torch.cuda.empty_cache()
        
        # 2. ì–‘ìí™” ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print("\nğŸ“¥ ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì¤‘...")
        quantized_pipeline = TrellisTextTo3DPipeline.from_pretrained(QUANTIZED_MODEL_PATH)
        quantized_pipeline.cuda()
        
        print("ğŸ”„ ì–‘ìí™” ëª¨ë¸ ìƒì„± ì¤‘...")
        quantized_outputs = quantized_pipeline.run(prompt, seed=seed)
        
        # ì–‘ìí™” ê²°ê³¼ ì €ì¥
        quantized_glb = postprocessing_utils.to_glb(
            quantized_outputs['gaussian'][0],
            quantized_outputs['mesh'][0],
            simplify=0.95,
            texture_size=1024
        )
        quantized_file = f"quantized_{safe_name}_seed{seed}.glb"
        quantized_glb.export(quantized_file)
        print(f"  âœ… ì–‘ìí™” ì €ì¥: {quantized_file}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del quantized_pipeline
        torch.cuda.empty_cache()
        
        # íŒŒì¼ í¬ê¸° ë¹„êµ
        original_size = Path(original_file).stat().st_size / (1024 * 1024)  # MB
        quantized_size = Path(quantized_file).stat().st_size / (1024 * 1024)  # MB
        
        print(f"\nğŸ‰ ë¹„êµ ì™„ë£Œ!")
        print(f"  ğŸ“¦ ì›ë³¸: {original_file} ({original_size:.1f}MB)")
        print(f"  ğŸ“¦ ì–‘ìí™”: {quantized_file} ({quantized_size:.1f}MB)")
        print(f"  ğŸ“Š íŒŒì¼ í¬ê¸° ì°¨ì´: {((original_size - quantized_size) / original_size * 100):+.1f}%")
        
    except Exception as e:
        print(f"âŒ ë¹„êµ ì‹¤íŒ¨: {e}")
        print(f"\nğŸ”§ í•´ê²° ë°©ë²•:")
        print(f"1. ê²½ë¡œ í™•ì¸:")
        print(f"   - ì›ë³¸: {ORIGINAL_MODEL_PATH}")
        print(f"   - ì–‘ìí™”: {QUANTIZED_MODEL_PATH}")
        print(f"2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ: export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512")
        print(f"3. ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸: 'a cube'")


def batch_test():
    """ë¹ ë¥¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ TRELLIS ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {len(QUICK_TEST_PROMPTS)}ê°œ")
    print("=" * 50)
    
    try:
        from trellis.pipelines import TrellisTextTo3DPipeline
        from trellis.utils import postprocessing_utils
        
        results = []
        
        for i, prompt in enumerate(QUICK_TEST_PROMPTS, 1):
            print(f"\nğŸ”„ í…ŒìŠ¤íŠ¸ {i}/{len(QUICK_TEST_PROMPTS)}: '{prompt}'")
            
            result = {
                'prompt': prompt,
                'original_success': False,
                'quantized_success': False,
                'original_time': 0,
                'quantized_time': 0,
                'original_size': 0,
                'quantized_size': 0
            }
            
            # ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
            try:
                print("  ğŸ“¥ ì›ë³¸ ëª¨ë¸...")
                pipeline = TrellisTextTo3DPipeline.from_pretrained(ORIGINAL_MODEL_PATH)
                pipeline.cuda()
                
                start_time = time.time()
                outputs = pipeline.run(prompt, seed=42)
                result['original_time'] = time.time() - start_time
                
                # GLB ì €ì¥
                glb = postprocessing_utils.to_glb(outputs['gaussian'][0], outputs['mesh'][0])
                file_path = f"batch_original_{prompt.replace(' ', '_')}.glb"
                glb.export(file_path)
                result['original_size'] = Path(file_path).stat().st_size / (1024 * 1024)
                result['original_success'] = True
                
                del pipeline
                torch.cuda.empty_cache()
                print("    âœ… ì„±ê³µ")
                
            except Exception as e:
                print(f"    âŒ ì‹¤íŒ¨: {e}")
            
            # ì–‘ìí™” ëª¨ë¸ í…ŒìŠ¤íŠ¸  
            try:
                print("  ğŸ“¥ ì–‘ìí™” ëª¨ë¸...")
                pipeline = TrellisTextTo3DPipeline.from_pretrained(QUANTIZED_MODEL_PATH)
                pipeline.cuda()
                
                start_time = time.time()
                outputs = pipeline.run(prompt, seed=42)
                result['quantized_time'] = time.time() - start_time
                
                # GLB ì €ì¥
                glb = postprocessing_utils.to_glb(outputs['gaussian'][0], outputs['mesh'][0])
                file_path = f"batch_quantized_{prompt.replace(' ', '_')}.glb"
                glb.export(file_path)
                result['quantized_size'] = Path(file_path).stat().st_size / (1024 * 1024)
                result['quantized_success'] = True
                
                del pipeline
                torch.cuda.empty_cache()
                print("    âœ… ì„±ê³µ")
                
            except Exception as e:
                print(f"    âŒ ì‹¤íŒ¨: {e}")
            
            results.append(result)
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 50)
        
        success_count = sum(1 for r in results if r['original_success'] and r['quantized_success'])
        print(f"âœ… ì„±ê³µ: {success_count}/{len(results)}")
        
        if success_count > 0:
            avg_time_original = sum(r['original_time'] for r in results if r['original_success']) / success_count
            avg_time_quantized = sum(r['quantized_time'] for r in results if r['quantized_success']) / success_count
            avg_size_original = sum(r['original_size'] for r in results if r['original_success']) / success_count
            avg_size_quantized = sum(r['quantized_size'] for r in results if r['quantized_success']) / success_count
            
            time_change = ((avg_time_quantized - avg_time_original) / avg_time_original) * 100
            size_change = ((avg_size_quantized - avg_size_original) / avg_size_original) * 100
            
            print(f"â±ï¸ í‰ê·  ìƒì„±ì‹œê°„: ì›ë³¸ {avg_time_original:.1f}s â†’ ì–‘ìí™” {avg_time_quantized:.1f}s ({time_change:+.1f}%)")
            print(f"ğŸ“¦ í‰ê·  íŒŒì¼í¬ê¸°: ì›ë³¸ {avg_size_original:.1f}MB â†’ ì–‘ìí™” {avg_size_quantized:.1f}MB ({size_change:+.1f}%)")
        
        print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        for result in results:
            if result['original_success']:
                print(f"  - batch_original_{result['prompt'].replace(' ', '_')}.glb")
            if result['quantized_success']:
                print(f"  - batch_quantized_{result['prompt'].replace(' ', '_')}.glb")
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


def main():
    parser = argparse.ArgumentParser(description="TRELLIS ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë„êµ¬")
    subparsers = parser.add_subparsers(dest='command', help='í…ŒìŠ¤íŠ¸ ëª…ë ¹')
    
    # quick ëª…ë ¹
    quick_parser = subparsers.add_parser('quick', help='ë¹ ë¥¸ ë¡œë“œ í…ŒìŠ¤íŠ¸')
    
    # compare ëª…ë ¹
    compare_parser = subparsers.add_parser('compare', help='ëª¨ë¸ ë¹„êµ')
    compare_parser.add_argument('--prompt', type=str, help='í…ŒìŠ¤íŠ¸í•  í”„ë¡¬í”„íŠ¸')
    compare_parser.add_argument('--seed', type=int, default=42, help='ëœë¤ ì‹œë“œ')
    compare_parser.add_argument('--all_prompts', action='store_true', help='ëª¨ë“  í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰')
    
    # batch ëª…ë ¹
    batch_parser = subparsers.add_parser('batch', help='ë°°ì¹˜ í…ŒìŠ¤íŠ¸')
    
    args = parser.parse_args()
    
    if args.command == 'quick':
        quick_test()
        
    elif args.command == 'compare':
        if args.all_prompts:
            for i, prompt in enumerate(TEST_PROMPTS, 1):
                print(f"\n{'='*50}")
                print(f"ğŸ”„ í…ŒìŠ¤íŠ¸ {i}/{len(TEST_PROMPTS)}")
                print(f"{'='*50}")
                compare_models(prompt, args.seed)
        elif args.prompt:
            compare_models(args.prompt, args.seed)
        else:
            print("ì‚¬ìš©ë²•:")
            print("  python model_test.py compare --prompt 'a red car'")
            print("  python model_test.py compare --all_prompts")
            print(f"\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸:")
            for prompt in TEST_PROMPTS:
                print(f"  - \"{prompt}\"")
                
    elif args.command == 'batch':
        batch_test()
        
    else:
        print("TRELLIS ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë„êµ¬")
        print("=" * 30)
        print("ì‚¬ìš©ë²•:")
        print("  python model_test.py quick                    # ë¹ ë¥¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
        print("  python model_test.py compare --prompt 'a car' # ëª¨ë¸ ë¹„êµ")
        print("  python model_test.py batch                    # ë°°ì¹˜ í…ŒìŠ¤íŠ¸")
        print()
        print("âš ï¸ ê²½ë¡œ ì„¤ì •:")
        print(f"  ì›ë³¸ ëª¨ë¸: {ORIGINAL_MODEL_PATH}")
        print(f"  ì–‘ìí™” ëª¨ë¸: {QUANTIZED_MODEL_PATH}")
        print("  í•„ìš”ì‹œ ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ì—ì„œ ìˆ˜ì •í•˜ì„¸ìš”")


if __name__ == "__main__":
    main()