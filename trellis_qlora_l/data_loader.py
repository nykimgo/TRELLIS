"""
ë°ì´í„° ë¡œë” ìœ í‹¸ë¦¬í‹°

ì£¼ìš” ê¸°ëŠ¥:
- TRELLIS í›ˆë ¨ìš© ë°ì´í„°ì…‹ ë¡œë“œ
- ë¶„ì‚° í›ˆë ¨ìš© DistributedSampler ì§€ì›
- í…ìŠ¤íŠ¸-3D ìŒ ë°ì´í„° ì „ì²˜ë¦¬
"""

import os
import json
import torch
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import random

# TRELLIS ê´€ë ¨ ì„í¬íŠ¸
try:
    from trellis.utils import preprocess_text
except ImportError:
    def preprocess_text(text):
        return text.strip().lower()


class TRELLISDataset(Dataset):
    """TRELLIS í›ˆë ¨ìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, data_path: str, max_seq_length: int = 2048):
        """
        Args:
            data_path: ë°ì´í„°ì…‹ ê²½ë¡œ (JSON íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬)
            max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        """
        self.data_path = Path(data_path)
        self.max_seq_length = max_seq_length
        self.data = self._load_data()
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.data)} ìƒ˜í”Œ")
    
    def _load_data(self) -> List[Dict[str, Any]]:
        """ë°ì´í„° ë¡œë“œ"""
        if not self.data_path.exists():
            # ë”ë¯¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ì„ ë•Œ)
            print(f"âš ï¸ ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.data_path}")
            print("ğŸ“Š ë”ë¯¸ ë°ì´í„°ë¡œ í›ˆë ¨ì„ ì§„í–‰í•©ë‹ˆë‹¤")
            return self._create_dummy_data()
        
        if self.data_path.is_file() and self.data_path.suffix == '.json':
            # JSON íŒŒì¼ì—ì„œ ë¡œë“œ
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data if isinstance(data, list) else [data]
        
        elif self.data_path.is_dir():
            # ë””ë ‰í† ë¦¬ì—ì„œ ì—¬ëŸ¬ JSON íŒŒì¼ ë¡œë“œ
            data = []
            for json_file in self.data_path.glob("*.json"):
                with open(json_file, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)
                    if isinstance(file_data, list):
                        data.extend(file_data)
                    else:
                        data.append(file_data)
            return data
        
        else:
            print(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹: {self.data_path}")
            return self._create_dummy_data()
    
    def _create_dummy_data(self) -> List[Dict[str, Any]]:
        """ë”ë¯¸ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
        dummy_prompts = [
            "a red sports car",
            "a wooden chair with leather cushion",
            "a blue coffee mug on a table",
            "a small house with garden",
            "a cute cat sitting on a window",
            "a modern desk lamp",
            "a vintage bicycle",
            "a colorful umbrella",
            "a glass vase with flowers",
            "a leather backpack",
            "a digital camera",
            "a pair of running shoes",
            "a stack of books",
            "a ceramic bowl",
            "a metal water bottle"
        ]
        
        data = []
        for i in range(100):  # 100ê°œ ë”ë¯¸ ìƒ˜í”Œ
            prompt = random.choice(dummy_prompts)
            data.append({
                "text": prompt,
                "id": f"dummy_{i:03d}",
                # ì‹¤ì œë¡œëŠ” 3D ê´€ë ¨ ë°ì´í„° (ì¢Œí‘œ, ë©”ì‹œ ë“±)ê°€ í¬í•¨ë¨
                "target": self._create_dummy_3d_data()
            })
        
        return data
    
    def _create_dummy_3d_data(self) -> Dict[str, Any]:
        """ë”ë¯¸ 3D ë°ì´í„° ìƒì„±"""
        # ì‹¤ì œë¡œëŠ” TRELLISì˜ êµ¬ì¡°í™”ëœ 3D í‘œí˜„ì´ ì‚¬ìš©ë¨
        return {
            "structure": torch.randn(64, 64, 64),  # êµ¬ì¡° í‘œí˜„
            "appearance": torch.randn(128, 128, 3),  # ì™¸í˜• í‘œí˜„
            "geometry": torch.randn(1000, 3)  # ê¸°í•˜ ì •ë³´
        }
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """ë°ì´í„° ìƒ˜í”Œ ë°˜í™˜"""
        item = self.data[idx]
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = preprocess_text(item["text"])
        
        # í† í°í™” (ì‹¤ì œë¡œëŠ” TRELLISì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë” ì‚¬ìš©)
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ëœ ë²„ì „ ì‚¬ìš©
        tokens = self._tokenize_text(text)
        
        # 3D íƒ€ê²Ÿ ë°ì´í„°
        target = item.get("target", self._create_dummy_3d_data())
        
        return {
            "input_ids": tokens["input_ids"],
            "attention_mask": tokens["attention_mask"],
            "labels": target,  # 3D ìƒì„± íƒ€ê²Ÿ
            "text": text  # ì›ë³¸ í…ìŠ¤íŠ¸ (ë””ë²„ê¹…ìš©)
        }
    
    def _tokenize_text(self, text: str) -> Dict[str, torch.Tensor]:
        """í…ìŠ¤íŠ¸ í† í°í™” (ê°„ë‹¨í•œ ë”ë¯¸ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” TRELLISì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
        words = text.split()
        
        # ê°„ë‹¨í•œ ë‹¨ì–´ -> ì¸ë±ìŠ¤ ë§¤í•‘
        vocab = {
            "<pad>": 0, "<unk>": 1, "a": 2, "red": 3, "car": 4, 
            "blue": 5, "chair": 6, "house": 7, "cat": 8, "dog": 9,
            "table": 10, "book": 11, "flower": 12, "tree": 13, "sky": 14
        }
        
        # í† í° ID ìƒì„±
        token_ids = []
        for word in words[:self.max_seq_length-2]:  # íŒ¨ë”© ê³ ë ¤
            token_ids.append(vocab.get(word.lower(), 1))  # <unk>
        
        # íŒ¨ë”©
        seq_len = min(len(token_ids), self.max_seq_length)
        input_ids = token_ids[:seq_len] + [0] * (self.max_seq_length - seq_len)
        attention_mask = [1] * seq_len + [0] * (self.max_seq_length - seq_len)
        
        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long)
        }


def create_dataloader(config, world_size: int = 1, rank: int = 0) -> Tuple[DataLoader, Optional[DataLoader]]:
    """ë°ì´í„°ë¡œë” ìƒì„±"""
    
    # ë°ì´í„°ì…‹ íƒ€ì…ì— ë”°ë¥¸ ë¡œë” ì„ íƒ
    if hasattr(config, 'dataset_type') and config.dataset_type == "hssd":
        # HSSD ë°ì´í„°ì…‹ ì‚¬ìš©
        return create_hssd_dataloader(config.dataset_path, config, world_size, rank)
    
    elif config.dataset_path and config.dataset_path != "":
        # ì¼ë°˜ ë°ì´í„°ì…‹
        dataset = TRELLISDataset(
            data_path=config.dataset_path,
            max_seq_length=config.max_seq_length
        )
    else:
        # ë”ë¯¸ ë°ì´í„°ì…‹ (í…ŒìŠ¤íŠ¸ìš©)
        dataset = TRELLISDataset(
            data_path="dummy",
            max_seq_length=config.max_seq_length
        )
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    if config.dataset_split_ratio < 1.0:
        train_size = int(len(dataset) * config.dataset_split_ratio)
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
    else:
        train_dataset = dataset
        val_dataset = None
    
    # ë¶„ì‚° ìƒ˜í”ŒëŸ¬
    if world_size > 1:
        train_sampler = DistributedSampler(
            train_dataset, 
            num_replicas=world_size, 
            rank=rank,
            shuffle=True
        )
        val_sampler = DistributedSampler(
            val_dataset, 
            num_replicas=world_size, 
            rank=rank,
            shuffle=False
        ) if val_dataset else None
    else:
        train_sampler = None
        val_sampler = None
    
    # í›ˆë ¨ ë°ì´í„°ë¡œë”
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size_per_gpu,
        sampler=train_sampler,
        shuffle=(train_sampler is None),
        num_workers=config.dataloader_num_workers,
        pin_memory=config.dataloader_pin_memory,
        drop_last=True,
        collate_fn=collate_fn
    )
    
    # ê²€ì¦ ë°ì´í„°ë¡œë”
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=config.batch_size_per_gpu,
            sampler=val_sampler,
            shuffle=False,
            num_workers=config.dataloader_num_workers,
            pin_memory=config.dataloader_pin_memory,
            drop_last=False,
            collate_fn=collate_fn
        )
    
    return train_loader, val_loader


def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ë°°ì¹˜ collate í•¨ìˆ˜"""
    
    # í…ìŠ¤íŠ¸ ê´€ë ¨ ë°ì´í„°
    input_ids = torch.stack([item["input_ids"] for item in batch])
    attention_mask = torch.stack([item["attention_mask"] for item in batch])
    
    # 3D ë¼ë²¨ ë°ì´í„° (ê°„ë‹¨íˆ êµ¬ì¡°ë§Œ ë§ì¶¤)
    labels = {}
    if "labels" in batch[0]:
        for key in batch[0]["labels"]:
            if isinstance(batch[0]["labels"][key], torch.Tensor):
                labels[key] = torch.stack([item["labels"][key] for item in batch])
    
    # ì›ë³¸ í…ìŠ¤íŠ¸ (ë””ë²„ê¹…ìš©)
    texts = [item["text"] for item in batch]
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
        "texts": texts
    }


class HSSDDataset(TRELLISDataset):
    """HSSD ë°ì´í„°ì…‹ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self, hssd_path: str, split: str = "train", **kwargs):
        """
        Args:
            hssd_path: HSSD ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ
            split: ë°ì´í„° ë¶„í•  (train/val/test)
        """
        self.hssd_path = Path(hssd_path)
        self.split = split
        
        # HSSD ë°ì´í„° êµ¬ì¡° í™•ì¸
        self.expected_dirs = [
            "metadata",    # ë©”íƒ€ë°ì´í„°
            "rendered",    # ë Œë”ë§ëœ ì´ë¯¸ì§€  
            "voxelized",   # ë³µì…€ ë°ì´í„°
            "latents"      # ì¸ì½”ë”©ëœ latent
        ]
        
        super().__init__(hssd_path, **kwargs)
    
    def _load_data(self) -> List[Dict[str, Any]]:
        """HSSD ë°ì´í„° ë¡œë“œ"""
        
        # HSSD êµ¬ì¡° í™•ì¸
        if not self._validate_hssd_structure():
            print(f"âš ï¸ HSSD êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.hssd_path}")
            return self._create_dummy_data()
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ ì°¾ê¸°
        metadata_files = []
        metadata_dir = self.hssd_path / "metadata"
        
        if metadata_dir.exists():
            # splitë³„ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì°¾ê¸°
            for pattern in [f"{self.split}.json", f"{self.split}_*.json", "*.json"]:
                files = list(metadata_dir.glob(pattern))
                metadata_files.extend(files)
        
        if not metadata_files:
            print(f"âš ï¸ HSSD ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_dir}")
            return self._create_dummy_data()
        
        print(f"ğŸ“Š HSSD ë©”íƒ€ë°ì´í„° íŒŒì¼: {len(metadata_files)}ê°œ")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        data = []
        for meta_file in metadata_files:
            try:
                with open(meta_file, 'r') as f:
                    file_data = json.load(f)
                
                if isinstance(file_data, list):
                    for item in file_data:
                        processed_item = self._process_hssd_item(item)
                        if processed_item:
                            data.append(processed_item)
                else:
                    processed_item = self._process_hssd_item(file_data)
                    if processed_item:
                        data.append(processed_item)
                        
            except Exception as e:
                print(f"âš ï¸ ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {meta_file} - {e}")
                continue
        
        print(f"ğŸ“Š HSSD ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ ìƒ˜í”Œ")
        return data
    
    def _validate_hssd_structure(self) -> bool:
        """HSSD ë°ì´í„° êµ¬ì¡° ê²€ì¦"""
        if not self.hssd_path.exists():
            return False
        
        missing_dirs = []
        for dir_name in self.expected_dirs:
            if not (self.hssd_path / dir_name).exists():
                missing_dirs.append(dir_name)
        
        if missing_dirs:
            print(f"âš ï¸ HSSD ë””ë ‰í† ë¦¬ ëˆ„ë½: {missing_dirs}")
            # metadataë§Œ ìˆì–´ë„ ê¸°ë³¸ ë™ì‘ì€ ê°€ëŠ¥
            return (self.hssd_path / "metadata").exists()
        
        return True
    
    def _process_hssd_item(self, item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """HSSD í•­ëª© ì²˜ë¦¬"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if 'object_id' not in item:
                return None
            
            object_id = item['object_id']
            
            # í…ìŠ¤íŠ¸ ì„¤ëª… ì¶”ì¶œ
            text_description = self._extract_text_description(item)
            if not text_description:
                return None
            
            # 3D ë°ì´í„° ê²½ë¡œë“¤
            data_paths = self._get_hssd_data_paths(object_id)
            
            return {
                "text": text_description,
                "id": object_id,
                "target": data_paths,
                "metadata": item
            }
            
        except Exception as e:
            print(f"âš ï¸ HSSD í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_text_description(self, item: Dict[str, Any]) -> str:
        """í…ìŠ¤íŠ¸ ì„¤ëª… ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ í•„ë“œì—ì„œ í…ìŠ¤íŠ¸ ì°¾ê¸°
        text_fields = ['description', 'caption', 'text', 'prompt', 'name', 'category']
        
        for field in text_fields:
            if field in item and item[field]:
                return str(item[field]).strip()
        
        # ì¹´í…Œê³ ë¦¬ë‚˜ í´ë˜ìŠ¤ì—ì„œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„±
        if 'category' in item:
            return f"a {item['category']}"
        elif 'class' in item:
            return f"a {item['class']}"
        
        return None
    
    def _get_hssd_data_paths(self, object_id: str) -> Dict[str, Any]:
        """HSSD 3D ë°ì´í„° ê²½ë¡œë“¤ ë°˜í™˜"""
        paths = {}
        
        # ë Œë”ë§ëœ ì´ë¯¸ì§€
        rendered_dir = self.hssd_path / "rendered" / object_id
        if rendered_dir.exists():
            paths["rendered_images"] = list(rendered_dir.glob("*.png"))
        
        # ë³µì…€ ë°ì´í„°
        voxel_file = self.hssd_path / "voxelized" / f"{object_id}.npz"
        if voxel_file.exists():
            paths["voxel_data"] = voxel_file
        
        # ì¸ì½”ë”©ëœ latent
        latent_file = self.hssd_path / "latents" / f"{object_id}.pt"
        if latent_file.exists():
            paths["latent_data"] = latent_file
        
        # ë©”ì‹œ íŒŒì¼
        mesh_file = self.hssd_path / "meshes" / f"{object_id}.ply"
        if mesh_file.exists():
            paths["mesh_data"] = mesh_file
        
        return paths
    
    def _load_hssd_3d_data(self, data_paths: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤ì œ HSSD 3D ë°ì´í„° ë¡œë“œ"""
        loaded_data = {}
        
        try:
            # ë³µì…€ ë°ì´í„° ë¡œë“œ
            if "voxel_data" in data_paths:
                voxel_data = np.load(data_paths["voxel_data"])
                loaded_data["voxels"] = torch.from_numpy(voxel_data['voxels'])
            
            # Latent ë°ì´í„° ë¡œë“œ
            if "latent_data" in data_paths:
                latent_data = torch.load(data_paths["latent_data"], map_location='cpu')
                loaded_data["latents"] = latent_data
            
            # ë Œë”ë§ ì´ë¯¸ì§€ (ì²« ë²ˆì§¸ë§Œ)
            if "rendered_images" in data_paths and data_paths["rendered_images"]:
                # ì‹¤ì œë¡œëŠ” ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê²½ë¡œë§Œ ì €ì¥
                loaded_data["rendered_path"] = str(data_paths["rendered_images"][0])
                
        except Exception as e:
            print(f"âš ï¸ HSSD 3D ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë°ì´í„°
            loaded_data = self._create_dummy_3d_data()
        
        return loaded_data
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """ë°ì´í„° ìƒ˜í”Œ ë°˜í™˜ (HSSD ë²„ì „)"""
        item = self.data[idx]
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = preprocess_text(item["text"])
        tokens = self._tokenize_text(text)
        
        # 3D ë°ì´í„° ë¡œë“œ
        if isinstance(item["target"], dict) and "latent_data" in item["target"]:
            # ì‹¤ì œ HSSD ë°ì´í„° ë¡œë“œ
            target_data = self._load_hssd_3d_data(item["target"])
        else:
            # ë”ë¯¸ ë°ì´í„°
            target_data = self._create_dummy_3d_data()
        
        return {
            "input_ids": tokens["input_ids"],
            "attention_mask": tokens["attention_mask"],
            "labels": target_data,
            "text": text,
            "object_id": item["id"],
            "metadata": item.get("metadata", {})
        }


def create_hssd_dataloader(hssd_path: str, config, world_size: int = 1, rank: int = 0):
    """HSSD ë°ì´í„°ë¡œë” ìƒì„±"""
    
    # í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì…‹
    train_dataset = HSSDDataset(hssd_path, split="train", max_seq_length=config.max_seq_length)
    val_dataset = HSSDDataset(hssd_path, split="val", max_seq_length=config.max_seq_length)
    
    # ë¶„ì‚° ìƒ˜í”ŒëŸ¬
    if world_size > 1:
        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
        val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank)
    else:
        train_sampler = None
        val_sampler = None
    
    # ë°ì´í„°ë¡œë”
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size_per_gpu,
        sampler=train_sampler,
        shuffle=(train_sampler is None),
        num_workers=config.dataloader_num_workers,
        pin_memory=config.dataloader_pin_memory,
        collate_fn=collate_fn
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size_per_gpu,
        sampler=val_sampler,
        shuffle=False,
        num_workers=config.dataloader_num_workers,
        pin_memory=config.dataloader_pin_memory,
        collate_fn=collate_fn
    )
    
    return train_loader, val_loader