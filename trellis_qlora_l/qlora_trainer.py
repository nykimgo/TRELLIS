"""
TRELLIS QLoRA Ìä∏Î†àÏù¥ÎÑà

Ï£ºÏöî Í∏∞Îä•:
- QLoRAÎ•º Ïù¥Ïö©Ìïú TRELLIS Î™®Îç∏ fine-tuning
- Î∂ÑÏÇ∞ ÌõàÎ†® (DDP) ÏßÄÏõê
- Mixed Precision (FP16) ÏßÄÏõê
- Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•/Î°úÎìú
- TensorBoard Î°úÍπÖ
"""

import os
import time
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torch.utils.tensorboard import SummaryWriter
from pathlib import Path
from typing import Optional, Dict, Any
import logging

# QLoRA Í¥ÄÎ†® ÎùºÏù¥Î∏åÎü¨Î¶¨
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from transformers import BitsAndBytesConfig
import bitsandbytes as bnb

# TRELLIS Í¥ÄÎ†® ÏûÑÌè¨Ìä∏
try:
    from trellis.pipelines import TrellisTextTo3DPipeline
    from trellis.models import TrellisImageTokenizer, TrellisSparseStructureDecoder
except ImportError:
    print("‚ö†Ô∏è TRELLIS Î™®ÎìàÏùÑ ÏûÑÌè¨Ìä∏Ìï† Ïàò ÏóÜÏäµÎãàÎã§. PYTHONPATHÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.")

from qlora_config import QLoRAConfig
from utils.data_loader import create_dataloader
from utils.model_utils import get_trainable_parameters
from utils.optimizer import create_optimizer, create_scheduler
from utils.checkpoint import save_checkpoint, load_checkpoint
from utils.evaluation import evaluate_model


class TRELLISQLoRATrainer:
    """TRELLIS QLoRA Ìä∏Î†àÏù¥ÎÑà"""
    
    def __init__(self, config: QLoRAConfig, exp_dir: Path, logger: logging.Logger):
        """Ï¥àÍ∏∞Ìôî"""
        self.config = config
        self.exp_dir = exp_dir
        self.logger = logger
        
        # Î∂ÑÏÇ∞ ÌõàÎ†® ÏÑ§Ï†ï
        self.setup_distributed()
        
        # ÏãúÎìú ÏÑ§Ï†ï
        self.setup_seed()
        
        # Î™®Îç∏ Í¥ÄÎ†® Î≥ÄÏàò
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        
        # ÌõàÎ†® ÏÉÅÌÉú
        self.global_step = 0
        self.epoch = 0
        self.best_metric = float('inf')
        
        # TensorBoard
        if self.is_master and config.use_tensorboard:
            self.writer = SummaryWriter(exp_dir / "tensorboard")
        else:
            self.writer = None
    
    def setup_distributed(self):
        """Î∂ÑÏÇ∞ ÌõàÎ†® ÏÑ§Ï†ï"""
        if self.config.num_gpus > 1:
            if not dist.is_available():
                raise RuntimeError("Î∂ÑÏÇ∞ ÌõàÎ†®Ïù¥ ÏßÄÏõêÎêòÏßÄ ÏïäÏäµÎãàÎã§")
            
            # ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
            if 'RANK' not in os.environ:
                os.environ['RANK'] = '0'
                os.environ['WORLD_SIZE'] = str(self.config.num_gpus)
                os.environ['MASTER_ADDR'] = 'localhost'
                os.environ['MASTER_PORT'] = '12355'
            
            dist.init_process_group(backend='nccl')
            self.rank = dist.get_rank()
            self.world_size = dist.get_world_size()
            torch.cuda.set_device(self.rank)
            self.device = torch.device(f'cuda:{self.rank}')
        else:
            self.rank = 0
            self.world_size = 1
            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        
        self.is_master = (self.rank == 0)
        
        if self.is_master:
            self.logger.info(f"üîß Î∂ÑÏÇ∞ ÏÑ§Ï†ï: rank={self.rank}, world_size={self.world_size}")
    
    def setup_seed(self):
        """ÏãúÎìú ÏÑ§Ï†ï"""
        torch.manual_seed(self.config.seed)
        torch.cuda.manual_seed_all(self.config.seed)
        if self.config.deterministic:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
    
    def setup_model(self):
        """Î™®Îç∏ ÏÑ§Ï†ï"""
        self.logger.info(f"üì• Î™®Îç∏ Î°úÎìú: {self.config.model_path}")
        
        # BitsAndBytesConfig ÏÑ§Ï†ï
        bnb_config = BitsAndBytesConfig(**self.config.quantization_config)
        
        # TRELLIS ÌååÏù¥ÌîÑÎùºÏù∏ Î°úÎìú
        pipeline = TrellisTextTo3DPipeline.from_pretrained(
            self.config.model_path,
            quantization_config=bnb_config,
            device_map={"": self.device}
        )
        
        # QLoRA ÏÑ§Ï†ïÌï† Î©îÏù∏ Î™®Îç∏ ÏÑ†ÌÉù (ÏùºÎ∞òÏ†ÅÏúºÎ°ú sparse_structure_decoder)
        if hasattr(pipeline, 'sparse_structure_decoder'):
            base_model = pipeline.sparse_structure_decoder
        elif hasattr(pipeline.models, 'sparse_structure_decoder'):
            base_model = pipeline.models['sparse_structure_decoder']
        else:
            raise ValueError("Ï†ÅÏ†àÌïú base modelÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
        
        # LoRA ÏÑ§Ï†ï
        lora_config = LoraConfig(
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            target_modules=self.config.lora_target_modules,
            lora_dropout=self.config.lora_dropout,
            bias=self.config.lora_bias,
            task_type=TaskType.FEATURE_EXTRACTION  # 3D ÏÉùÏÑ±ÏùÄ feature extractionÏúºÎ°ú Í∞ÑÏ£º
        )
        
        # LoRA Î™®Îç∏ ÏÉùÏÑ±
        self.model = get_peft_model(base_model, lora_config)
        
        # Í∑∏ÎûòÎîîÏñ∏Ìä∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏ÌåÖ
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
        
        # Î∂ÑÏÇ∞ Î™®Îç∏Î°ú ÎûòÌïë
        if self.world_size > 1:
            self.model = DDP(self.model, device_ids=[self.rank], find_unused_parameters=True)
        
        # ÌååÎùºÎØ∏ÌÑ∞ Ï†ïÎ≥¥ Ï∂úÎ†•
        trainable_params, total_params = get_trainable_parameters(self.model)
        self.logger.info(f"üìä ÌååÎùºÎØ∏ÌÑ∞: {trainable_params:,} / {total_params:,} "
                        f"({100 * trainable_params / total_params:.2f}%)")
        
        # ÌååÏù¥ÌîÑÎùºÏù∏Ïùò Î™®Îç∏ ÍµêÏ≤¥
        if hasattr(pipeline, 'sparse_structure_decoder'):
            pipeline.sparse_structure_decoder = self.model.module if isinstance(self.model, DDP) else self.model
        
        self.pipeline = pipeline
    
    def setup_data(self):
        """Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÑ§Ï†ï"""
        self.logger.info("üìä Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÑ§Ï†ï")
        
        # Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÉùÏÑ±
        self.train_loader, self.val_loader = create_dataloader(
            self.config, self.world_size, self.rank
        )
        
        self.logger.info(f"üìä ÌõàÎ†® Îç∞Ïù¥ÌÑ∞: {len(self.train_loader)} Î∞∞Ïπò")
        if self.val_loader:
            self.logger.info(f"üìä Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞: {len(self.val_loader)} Î∞∞Ïπò")
    
    def setup_optimizer(self):
        """ÏòµÌã∞ÎßàÏù¥Ï†Ä Î∞è Ïä§ÏºÄÏ§ÑÎü¨ ÏÑ§Ï†ï"""
        self.logger.info("üîß ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÑ§Ï†ï")
        
        # ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÉùÏÑ±
        self.optimizer = create_optimizer(self.model, self.config)
        
        # Ïä§ÏºÄÏ§ÑÎü¨ ÏÉùÏÑ±
        self.scheduler = create_scheduler(self.optimizer, self.config)
        
        # Mixed Precision Scaler
        if self.config.fp16:
            self.scaler = torch.cuda.amp.GradScaler()
        
        self.logger.info(f"üîß ÏòµÌã∞ÎßàÏù¥Ï†Ä: {self.optimizer.__class__.__name__}")
        self.logger.info(f"üîß Ïä§ÏºÄÏ§ÑÎü¨: {self.scheduler.__class__.__name__}")
    
    def train_step(self, batch: Dict[str, Any]) -> Dict[str, float]:
        """Îã®Ïùº ÌõàÎ†® Ïä§ÌÖù"""
        self.model.train()
        
        # Îç∞Ïù¥ÌÑ∞Î•º GPUÎ°ú Ïù¥Îèô
        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(self.device, non_blocking=True)
        
        # Forward pass
        with torch.cuda.amp.autocast(enabled=self.config.fp16):
            outputs = self.model(**batch)
            loss = outputs.loss if hasattr(outputs, 'loss') else outputs['loss']
            loss = loss / self.config.gradient_accumulation_steps
        
        # Backward pass
        if self.config.fp16:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()
        
        return {'loss': loss.item() * self.config.gradient_accumulation_steps}
    
    def train_epoch(self):
        """ÏóêÌè¨ÌÅ¨ ÌõàÎ†®"""
        if self.world_size > 1:
            self.train_loader.sampler.set_epoch(self.epoch)
        
        total_loss = 0.0
        num_batches = 0
        
        for step, batch in enumerate(self.train_loader):
            # ÌõàÎ†® Ïä§ÌÖù
            metrics = self.train_step(batch)
            total_loss += metrics['loss']
            num_batches += 1
            
            # Gradient accumulation
            if (step + 1) % self.config.gradient_accumulation_steps == 0:
                # Gradient clipping
                if self.config.gradient_clipping > 0:
                    if self.config.fp16:
                        self.scaler.unscale_(self.optimizer)
                    
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config.gradient_clipping
                    )
                
                # Optimizer step
                if self.config.fp16:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                
                self.scheduler.step()
                self.optimizer.zero_grad()
                
                self.global_step += 1
                
                # Î°úÍπÖ
                if self.global_step % self.config.logging_steps == 0:
                    self.log_metrics({
                        'train/loss': metrics['loss'],
                        'train/lr': self.scheduler.get_last_lr()[0],
                        'train/step': self.global_step
                    })
                
                # ÌèâÍ∞Ä
                if self.global_step % self.config.eval_steps == 0:
                    eval_metrics = self.evaluate()
                    self.log_metrics(eval_metrics)
                
                # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•
                if self.global_step % self.config.save_steps == 0:
                    self.save_checkpoint()
                
                # ÏµúÎåÄ Ïä§ÌÖù ÎèÑÎã¨ Ïãú Ï¢ÖÎ£å
                if self.global_step >= self.config.max_steps:
                    return total_loss / max(num_batches, 1)
        
        return total_loss / max(num_batches, 1)
    
    def evaluate(self) -> Dict[str, float]:
        """Î™®Îç∏ ÌèâÍ∞Ä"""
        if not self.val_loader:
            return {}
        
        self.model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                # Îç∞Ïù¥ÌÑ∞Î•º GPUÎ°ú Ïù¥Îèô
                for key in batch:
                    if isinstance(batch[key], torch.Tensor):
                        batch[key] = batch[key].to(self.device, non_blocking=True)
                
                # Forward pass
                with torch.cuda.amp.autocast(enabled=self.config.fp16):
                    outputs = self.model(**batch)
                    loss = outputs.loss if hasattr(outputs, 'loss') else outputs['loss']
                
                total_loss += loss.item()
                num_batches += 1
        
        avg_loss = total_loss / max(num_batches, 1)
        
        # Î∂ÑÏÇ∞ ÌôòÍ≤ΩÏóêÏÑú ÌèâÍ∑† Í≥ÑÏÇ∞
        if self.world_size > 1:
            loss_tensor = torch.tensor(avg_loss, device=self.device)
            dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
            avg_loss = loss_tensor.item() / self.world_size
        
        metrics = {
            'eval/loss': avg_loss,
            'eval/step': self.global_step
        }
        
        # Î≤†Ïä§Ìä∏ Î™®Îç∏ Ï†ÄÏû•
        if avg_loss < self.best_metric:
            self.best_metric = avg_loss
            if self.is_master:
                self.save_best_model()
        
        return metrics
    
    def log_metrics(self, metrics: Dict[str, float]):
        """Î©îÌä∏Î¶≠ Î°úÍπÖ"""
        if not self.is_master:
            return
        
        # ÏΩòÏÜî Ï∂úÎ†•
        if 'train/loss' in metrics:
            self.logger.info(
                f"Step {self.global_step:6d} | "
                f"Loss: {metrics['train/loss']:.4f} | "
                f"LR: {metrics['train/lr']:.2e}"
            )
        
        if 'eval/loss' in metrics:
            self.logger.info(
                f"Eval Step {self.global_step:6d} | "
                f"Loss: {metrics['eval/loss']:.4f}"
            )
        
        # TensorBoard Î°úÍπÖ
        if self.writer:
            for key, value in metrics.items():
                if key != 'train/step' and key != 'eval/step':
                    self.writer.add_scalar(key, value, self.global_step)
    
    def save_checkpoint(self):
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•"""
        if not self.is_master:
            return
        
        checkpoint_path = self.exp_dir / "checkpoints" / f"checkpoint-{self.global_step}"
        checkpoint_path.mkdir(exist_ok=True)
        
        # Î™®Îç∏ ÏÉÅÌÉú Ï†ÄÏû•
        model_to_save = self.model.module if isinstance(self.model, DDP) else self.model
        model_to_save.save_pretrained(checkpoint_path)
        
        # ÌõàÎ†® ÏÉÅÌÉú Ï†ÄÏû•
        state = {
            'global_step': self.global_step,
            'epoch': self.epoch,
            'best_metric': self.best_metric,
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            'config': self.config.__dict__
        }
        
        if self.config.fp16:
            state['scaler'] = self.scaler.state_dict()
        
        torch.save(state, checkpoint_path / "training_state.pt")
        
        self.logger.info(f"üíæ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•: {checkpoint_path}")
        
        # Ïò§ÎûòÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ïÎ¶¨
        self.cleanup_checkpoints()
    
    def save_best_model(self):
        """Î≤†Ïä§Ìä∏ Î™®Îç∏ Ï†ÄÏû•"""
        best_path = self.exp_dir / "best_model"
        best_path.mkdir(exist_ok=True)
        
        model_to_save = self.model.module if isinstance(self.model, DDP) else self.model
        model_to_save.save_pretrained(best_path)
        
        self.logger.info(f"üèÜ Î≤†Ïä§Ìä∏ Î™®Îç∏ Ï†ÄÏû•: {best_path} (loss: {self.best_metric:.4f})")
    
    def cleanup_checkpoints(self):
        """Ïò§ÎûòÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ïÎ¶¨"""
        checkpoint_dir = self.exp_dir / "checkpoints"
        checkpoints = list(checkpoint_dir.glob("checkpoint-*"))
        
        if len(checkpoints) > self.config.save_total_limit:
            # Ïä§ÌÖù Î≤àÌò∏Î°ú Ï†ïÎ†¨
            checkpoints.sort(key=lambda x: int(x.name.split('-')[1]))
            
            # Ïò§ÎûòÎêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÇ≠Ï†ú
            for checkpoint in checkpoints[:-self.config.save_total_limit]:
                import shutil
                shutil.rmtree(checkpoint)
                self.logger.info(f"üóëÔ∏è Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏÇ≠Ï†ú: {checkpoint}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú"""
        checkpoint_path = Path(checkpoint_path)
        
        # Î™®Îç∏ Î°úÎìú
        self.model = PeftModel.from_pretrained(
            self.model, 
            checkpoint_path,
            device_map={"": self.device}
        )
        
        # ÌõàÎ†® ÏÉÅÌÉú Î°úÎìú
        state_path = checkpoint_path / "training_state.pt"
        if state_path.exists():
            state = torch.load(state_path, map_location=self.device)
            self.global_step = state['global_step']
            self.epoch = state['epoch']
            self.best_metric = state['best_metric']
            
            if self.optimizer:
                self.optimizer.load_state_dict(state['optimizer'])
            if self.scheduler:
                self.scheduler.load_state_dict(state['scheduler'])
            if self.config.fp16 and self.scaler:
                self.scaler.load_state_dict(state['scaler'])
        
        self.logger.info(f"üì• Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú: {checkpoint_path}")
    
    def train(self):
        """Î©îÏù∏ ÌõàÎ†® Î£®ÌîÑ"""
        self.logger.info("üöÄ ÌõàÎ†® ÏãúÏûë")
        
        # ÏÑ§Ï†ï
        self.setup_model()
        self.setup_data()
        self.setup_optimizer()
        
        # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú Ïû¨ÏãúÏûë
        if self.config.resume_from_checkpoint:
            self.load_checkpoint(self.config.resume_from_checkpoint)
        
        start_time = time.time()
        
        try:
            while self.global_step < self.config.max_steps:
                epoch_loss = self.train_epoch()
                self.epoch += 1
                
                if self.is_master:
                    self.logger.info(f"Epoch {self.epoch} ÏôÑÎ£å, ÌèâÍ∑† ÏÜêÏã§: {epoch_loss:.4f}")
                
                # ÏµúÎåÄ Ïä§ÌÖù ÎèÑÎã¨ Ïãú Ï¢ÖÎ£å
                if self.global_step >= self.config.max_steps:
                    break
        
        except KeyboardInterrupt:
            self.logger.info("‚ö†Ô∏è ÌõàÎ†®Ïù¥ Ï§ëÎã®ÎêòÏóàÏäµÎãàÎã§")
        
        finally:
            # ÏµúÏ¢Ö Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•
            if self.is_master:
                self.save_checkpoint()
            
            # Î∂ÑÏÇ∞ ÌîÑÎ°úÏÑ∏Ïä§ Ï†ïÎ¶¨
            if self.world_size > 1:
                dist.destroy_process_group()
        
        total_time = time.time() - start_time
        self.logger.info(f"‚úÖ ÌõàÎ†® ÏôÑÎ£å (ÏãúÍ∞Ñ: {total_time:.2f}Ï¥à)")
        
        if self.writer:
            self.writer.close()
    
    def evaluate_model(self):
        """Î™®Îç∏ ÌèâÍ∞Ä (Ï∂îÎ°† ÌíàÏßà ÌÖåÏä§Ìä∏)"""
        self.logger.info("üìä Î™®Îç∏ ÌèâÍ∞Ä ÏãúÏûë")
        
        # ÏÑ§Ï†ï
        self.setup_model()
        
        # Î≤†Ïä§Ìä∏ Î™®Îç∏ Î°úÎìú
        best_model_path = self.exp_dir / "best_model"
        if best_model_path.exists():
            self.load_checkpoint(best_model_path)
        
        # ÌèâÍ∞Ä Ïã§Ìñâ
        results = evaluate_model(self.pipeline, self.config)
        
        # Í≤∞Í≥º Ï†ÄÏû•
        results_path = self.exp_dir / "results" / "evaluation_results.json"
        results_path.parent.mkdir(exist_ok=True)
        
        import json
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        self.logger.info(f"üìä ÌèâÍ∞Ä ÏôÑÎ£å: {results_path}")
        return results