"""
TRELLIS QLoRA íŠ¸ë ˆì´ë„ˆ

ì£¼ìš” ê¸°ëŠ¥:
- QLoRAë¥¼ ì´ìš©í•œ TRELLIS ëª¨ë¸ fine-tuning
- ë¶„ì‚° í›ˆë ¨ (DDP) ì§€ì›
- Mixed Precision (FP16) ì§€ì›
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ
- TensorBoard ë¡œê¹…
"""

import os
import time
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torch.utils.tensorboard import SummaryWriter
from pathlib import Path
from typing import Optional, Dict, Any
import logging

# QLoRA ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from transformers import BitsAndBytesConfig
import bitsandbytes as bnb

# TRELLIS ê´€ë ¨ ì„í¬íŠ¸
try:
    from trellis.pipelines import TrellisTextTo3DPipeline
    from trellis.models import TrellisImageTokenizer, TrellisSparseStructureDecoder
except ImportError:
    print("âš ï¸ TRELLIS ëª¨ë“ˆì„ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PYTHONPATHë¥¼ í™•ì¸í•˜ì„¸ìš”.")

from qlora_config import QLoRAConfig
from utils.data_loader import create_dataloader
from utils.model_utils import get_trainable_parameters
from utils.optimizer import create_optimizer, create_scheduler
from utils.checkpoint import save_checkpoint, load_checkpoint
from utils.evaluation import evaluate_model


class TRELLISQLoRATrainer:
    """TRELLIS QLoRA íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, config: QLoRAConfig, exp_dir: Path, logger: logging.Logger):
        """ì´ˆê¸°í™”"""
        self.config = config
        self.exp_dir = exp_dir
        self.logger = logger
        
        # ë¶„ì‚° í›ˆë ¨ ì„¤ì •
        self.setup_distributed()
        
        # ì‹œë“œ ì„¤ì •
        self.setup_seed()
        
        # ëª¨ë¸ ê´€ë ¨ ë³€ìˆ˜
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        
        # í›ˆë ¨ ìƒíƒœ
        self.global_step = 0
        self.epoch = 0
        self.best_metric = float('inf')
        
        # TensorBoard
        if self.is_master and config.use_tensorboard:
            self.writer = SummaryWriter(exp_dir / "tensorboard")
        else:
            self.writer = None
    
    def setup_distributed(self):
        """ë¶„ì‚° í›ˆë ¨ ì„¤ì •"""
        if self.config.num_gpus > 1:
            if not dist.is_available():
                raise RuntimeError("ë¶„ì‚° í›ˆë ¨ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            if 'RANK' not in os.environ:
                os.environ['RANK'] = '0'
                os.environ['WORLD_SIZE'] = str(self.config.num_gpus)
                os.environ['MASTER_ADDR'] = 'localhost'
                os.environ['MASTER_PORT'] = '12355'
            
            dist.init_process_group(backend='nccl')
            self.rank = dist.get_rank()
            self.world_size = dist.get_world_size()
            torch.cuda.set_device(self.rank)
            self.device = torch.device(f'cuda:{self.rank}')
        else:
            self.rank = 0
            self.world_size = 1
            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        
        self.is_master = (self.rank == 0)
        
        if self.is_master:
            self.logger.info(f"ğŸ”§ ë¶„ì‚° ì„¤ì •: rank={self.rank}, world_size={self.world_size}")
    
    def setup_seed(self):
        """ì‹œë“œ ì„¤ì •"""
        torch.manual_seed(self.config.seed)
        torch.cuda.manual_seed_all(self.config.seed)
        if self.config.deterministic:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
    
    def setup_model(self):
        """ëª¨ë¸ ì„¤ì •"""
        self.logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë“œ: {self.config.model_path}")
        
        # BitsAndBytesConfig ì„¤ì •
        bnb_config = BitsAndBytesConfig(**self.config.quantization_config)
        
        # TRELLIS íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        pipeline = TrellisTextTo3DPipeline.from_pretrained(
            self.config.model_path,
            quantization_config=bnb_config,
            device_map={"": self.device}
        )
        
        # QLoRA ì„¤ì •í•  ë©”ì¸ ëª¨ë¸ ì„ íƒ (ì¼ë°˜ì ìœ¼ë¡œ sparse_structure_decoder)
        if hasattr(pipeline, 'sparse_structure_decoder'):
            base_model = pipeline.sparse_structure_decoder
        elif hasattr(pipeline.models, 'sparse_structure_decoder'):
            base_model = pipeline.models['sparse_structure_decoder']
        else:
            raise ValueError("ì ì ˆí•œ base modelì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # LoRA ì„¤ì •
        lora_config = LoraConfig(
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            target_modules=self.config.lora_target_modules,
            lora_dropout=self.config.lora_dropout,
            bias=self.config.lora_bias,
            task_type=TaskType.FEATURE_EXTRACTION  # 3D ìƒì„±ì€ feature extractionìœ¼ë¡œ ê°„ì£¼
        )
        
        # LoRA ëª¨ë¸ ìƒì„±
        self.model = get_peft_model(base_model, lora_config)
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
        
        # ë¶„ì‚° ëª¨ë¸ë¡œ ë˜í•‘
        if self.world_size > 1:
            self.model = DDP(self.model, device_ids=[self.rank], find_unused_parameters=True)
        
        # íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥
        trainable_params, total_params = get_trainable_parameters(self.model)
        self.logger.info(f"ğŸ“Š íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,} "
                        f"({100 * trainable_params / total_params:.2f}%)")
        
        # íŒŒì´í”„ë¼ì¸ì˜ ëª¨ë¸ êµì²´
        if hasattr(pipeline, 'sparse_structure_decoder'):
            pipeline.sparse_structure_decoder = self.model.module if isinstance(self.model, DDP) else self.model
        
        self.pipeline = pipeline
    
    def setup_data(self):
        """ë°ì´í„°ë¡œë” ì„¤ì •"""
        self.logger.info("ğŸ“Š ë°ì´í„°ë¡œë” ì„¤ì •")
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        self.train_loader, self.val_loader = create_dataloader(
            self.config, self.world_size, self.rank
        )
        
        self.logger.info(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(self.train_loader)} ë°°ì¹˜")
        if self.val_loader:
            self.logger.info(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(self.val_loader)} ë°°ì¹˜")
    
    def setup_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        self.logger.info("ğŸ”§ ì˜µí‹°ë§ˆì´ì € ì„¤ì •")
        
        # ì˜µí‹°ë§ˆì´ì € ìƒì„±
        self.optimizer = create_optimizer(self.model, self.config)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
        self.scheduler = create_scheduler(self.optimizer, self.config)
        
        # Mixed Precision Scaler
        if self.config.fp16:
            self.scaler = torch.cuda.amp.GradScaler()
        
        self.logger.info(f"ğŸ”§ ì˜µí‹°ë§ˆì´ì €: {self.optimizer.__class__.__name__}")
        self.logger.info(f"ğŸ”§ ìŠ¤ì¼€ì¤„ëŸ¬: {self.scheduler.__class__.__name__}")
    
    def train_step(self, batch: Dict[str, Any]) -> Dict[str, float]:
        """ë‹¨ì¼ í›ˆë ¨ ìŠ¤í…"""
        self.model.train()
        
        # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(self.device, non_blocking=True)
        
        # Forward pass
        with torch.cuda.amp.autocast(enabled=self.config.fp16):
            outputs = self.model(**batch)
            loss = outputs.loss if hasattr(outputs, 'loss') else outputs['loss']
            loss = loss / self.config.gradient_accumulation_steps
        
        # Backward pass
        if self.config.fp16:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()
        
        return {'loss': loss.item() * self.config.gradient_accumulation_steps}
    
    def train_epoch(self):
        """ì—í¬í¬ í›ˆë ¨"""
        if self.world_size > 1:
            self.train_loader.sampler.set_epoch(self.epoch)
        
        total_loss = 0.0
        num_batches = 0
        
        for step, batch in enumerate(self.train_loader):
            # í›ˆë ¨ ìŠ¤í…
            metrics = self.train_step(batch)
            total_loss += metrics['loss']
            num_batches += 1
            
            # Gradient accumulation
            if (step + 1) % self.config.gradient_accumulation_steps == 0:
                # Gradient clipping
                if self.config.gradient_clipping > 0:
                    if self.config.fp16:
                        self.scaler.unscale_(self.optimizer)
                    
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config.gradient_clipping
                    )
                
                # Optimizer step
                if self.config.fp16:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                
                self.scheduler.step()
                self.optimizer.zero_grad()
                
                self.global_step += 1
                
                # ë¡œê¹…
                if self.global_step % self.config.logging_steps == 0:
                    self.log_metrics({
                        'train/loss': metrics['loss'],
                        'train/lr': self.scheduler.get_last_lr()[0],
                        'train/step': self.global_step
                    })
                
                # í‰ê°€
                if self.global_step % self.config.eval_steps == 0:
                    eval_metrics = self.evaluate()
                    self.log_metrics(eval_metrics)
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if self.global_step % self.config.save_steps == 0:
                    self.save_checkpoint()
                
                # ìµœëŒ€ ìŠ¤í… ë„ë‹¬ ì‹œ ì¢…ë£Œ
                if self.global_step >= self.config.max_steps:
                    return total_loss / max(num_batches, 1)
        
        return total_loss / max(num_batches, 1)
    
    def evaluate(self) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€"""
        if not self.val_loader:
            return {}
        
        self.model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
                for key in batch:
                    if isinstance(batch[key], torch.Tensor):
                        batch[key] = batch[key].to(self.device, non_blocking=True)
                
                # Forward pass
                with torch.cuda.amp.autocast(enabled=self.config.fp16):
                    outputs = self.model(**batch)
                    loss = outputs.loss if hasattr(outputs, 'loss') else outputs['loss']
                
                total_loss += loss.item()
                num_batches += 1
        
        avg_loss = total_loss / max(num_batches, 1)
        
        # ë¶„ì‚° í™˜ê²½ì—ì„œ í‰ê·  ê³„ì‚°
        if self.world_size > 1:
            loss_tensor = torch.tensor(avg_loss, device=self.device)
            dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
            avg_loss = loss_tensor.item() / self.world_size
        
        metrics = {
            'eval/loss': avg_loss,
            'eval/step': self.global_step
        }
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥
        if avg_loss < self.best_metric:
            self.best_metric = avg_loss
            if self.is_master:
                self.save_best_model()
        
        return metrics
    
    def log_metrics(self, metrics: Dict[str, float]):
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if not self.is_master:
            return
        
        # ì½˜ì†” ì¶œë ¥
        if 'train/loss' in metrics:
            self.logger.info(
                f"Step {self.global_step:6d} | "
                f"Loss: {metrics['train/loss']:.4f} | "
                f"LR: {metrics['train/lr']:.2e}"
            )
        
        if 'eval/loss' in metrics:
            self.logger.info(
                f"Eval Step {self.global_step:6d} | "
                f"Loss: {metrics['eval/loss']:.4f}"
            )
        
        # TensorBoard ë¡œê¹…
        if self.writer:
            for key, value in metrics.items():
                if key != 'train/step' and key != 'eval/step':
                    self.writer.add_scalar(key, value, self.global_step)
    
    def save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        if not self.is_master:
            return
        
        checkpoint_path = self.exp_dir / "checkpoints" / f"checkpoint-{self.global_step}"
        checkpoint_path.mkdir(exist_ok=True)
        
        # ëª¨ë¸ ìƒíƒœ ì €ì¥
        model_to_save = self.model.module if isinstance(self.model, DDP) else self.model
        model_to_save.save_pretrained(checkpoint_path)
        
        # í›ˆë ¨ ìƒíƒœ ì €ì¥
        state = {
            'global_step': self.global_step,
            'epoch': self.epoch,
            'best_metric': self.best_metric,
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            'config': self.config.__dict__
        }
        
        if self.config.fp16:
            state['scaler'] = self.scaler.state_dict()
        
        torch.save(state, checkpoint_path / "training_state.pt")
        
        self.logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
        
        # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
        self.cleanup_checkpoints()
    
    def save_best_model(self):
        """ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥"""
        best_path = self.exp_dir / "best_model"
        best_path.mkdir(exist_ok=True)
        
        model_to_save = self.model.module if isinstance(self.model, DDP) else self.model
        model_to_save.save_pretrained(best_path)
        
        self.logger.info(f"ğŸ† ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥: {best_path} (loss: {self.best_metric:.4f})")
    
    def cleanup_checkpoints(self):
        """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
        checkpoint_dir = self.exp_dir / "checkpoints"
        checkpoints = list(checkpoint_dir.glob("checkpoint-*"))
        
        if len(checkpoints) > self.config.save_total_limit:
            # ìŠ¤í… ë²ˆí˜¸ë¡œ ì •ë ¬
            checkpoints.sort(key=lambda x: int(x.name.split('-')[1]))
            
            # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
            for checkpoint in checkpoints[:-self.config.save_total_limit]:
                import shutil
                shutil.rmtree(checkpoint)
                self.logger.info(f"ğŸ—‘ï¸ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ: {checkpoint}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint_path = Path(checkpoint_path)
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = PeftModel.from_pretrained(
            self.model, 
            checkpoint_path,
            device_map={"": self.device}
        )
        
        # í›ˆë ¨ ìƒíƒœ ë¡œë“œ
        state_path = checkpoint_path / "training_state.pt"
        if state_path.exists():
            state = torch.load(state_path, map_location=self.device)
            self.global_step = state['global_step']
            self.epoch = state['epoch']
            self.best_metric = state['best_metric']
            
            if self.optimizer:
                self.optimizer.load_state_dict(state['optimizer'])
            if self.scheduler:
                self.scheduler.load_state_dict(state['scheduler'])
            if self.config.fp16 and self.scaler:
                self.scaler.load_state_dict(state['scaler'])
        
        self.logger.info(f"ğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
    
    def train(self):
        """ë©”ì¸ í›ˆë ¨ ë£¨í”„"""
        self.logger.info("ğŸš€ í›ˆë ¨ ì‹œì‘")
        
        # ì„¤ì •
        self.setup_model()
        self.setup_data()
        self.setup_optimizer()
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘
        if self.config.resume_from_checkpoint:
            self.load_checkpoint(self.config.resume_from_checkpoint)
        
        start_time = time.time()
        
        try:
            while self.global_step < self.config.max_steps:
                epoch_loss = self.train_epoch()
                self.epoch += 1
                
                if self.is_master:
                    self.logger.info(f"Epoch {self.epoch} ì™„ë£Œ, í‰ê·  ì†ì‹¤: {epoch_loss:.4f}")
                
                # ìµœëŒ€ ìŠ¤í… ë„ë‹¬ ì‹œ ì¢…ë£Œ
                if self.global_step >= self.config.max_steps:
                    break
        
        except KeyboardInterrupt:
            self.logger.info("âš ï¸ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        finally:
            # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if self.is_master:
                self.save_checkpoint()
            
            # ë¶„ì‚° í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
            if self.world_size > 1:
                dist.destroy_process_group()
        
        total_time = time.time() - start_time
        self.logger.info(f"âœ… í›ˆë ¨ ì™„ë£Œ (ì‹œê°„: {total_time:.2f}ì´ˆ)")
        
        if self.writer:
            self.writer.close()
    
    def evaluate_model(self):
        """ëª¨ë¸ í‰ê°€ (ì¶”ë¡  í’ˆì§ˆ í…ŒìŠ¤íŠ¸)"""
        self.logger.info("ğŸ“Š ëª¨ë¸ í‰ê°€ ì‹œì‘")
        
        # ì„¤ì •
        self.setup_model()
        
        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ
        best_model_path = self.exp_dir / "best_model"
        if best_model_path.exists():
            self.load_checkpoint(best_model_path)
        
        # í‰ê°€ ì‹¤í–‰
        results = evaluate_model(self.pipeline, self.config)
        
        # ê²°ê³¼ ì €ì¥
        results_path = self.exp_dir / "results" / "evaluation_results.json"
        results_path.parent.mkdir(exist_ok=True)
        
        import json
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        self.logger.info(f"ğŸ“Š í‰ê°€ ì™„ë£Œ: {results_path}")
        return results