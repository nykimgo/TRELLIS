"""
ëª¨ë¸ í‰ê°€ ìœ í‹¸ë¦¬í‹°

ì£¼ìš” ê¸°ëŠ¥:
- CLIP Score ê³„ì‚°
- FID, KID ë“± ìƒì„± í’ˆì§ˆ í‰ê°€
- Chamfer Distance ê³„ì‚°
- 3D ëª¨ë¸ í’ˆì§ˆ ë©”íŠ¸ë¦­
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import time

try:
    import clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("âš ï¸ CLIP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. pip install clip-by-openai")

try:
    from pytorch3d.loss import chamfer_distance
    from pytorch3d.structures import Pointclouds
    PYTORCH3D_AVAILABLE = True
except ImportError:
    PYTORCH3D_AVAILABLE = False
    print("âš ï¸ PyTorch3Dê°€ ì—†ìŠµë‹ˆë‹¤. 3D ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

try:
    from torchmetrics.image.fid import FrechetInceptionDistance
    from torchmetrics.image.kid import KernelInceptionDistance
    TORCHMETRICS_AVAILABLE = True
except ImportError:
    TORCHMETRICS_AVAILABLE = False
    print("âš ï¸ TorchMetricsê°€ ì—†ìŠµë‹ˆë‹¤. FID/KID ê³„ì‚°ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


class TRELLISEvaluator:
    """TRELLIS ëª¨ë¸ í‰ê°€ê¸°"""
    
    def __init__(self, device: str = "cuda"):
        self.device = device
        self.clip_model = None
        self.clip_preprocess = None
        
        # CLIP ëª¨ë¸ ë¡œë“œ
        if CLIP_AVAILABLE:
            self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=device)
            self.clip_model.eval()
        
        # FID/KID ê³„ì‚°ê¸°
        if TORCHMETRICS_AVAILABLE:
            self.fid = FrechetInceptionDistance(feature=2048).to(device)
            self.kid = KernelInceptionDistance(subset_size=100).to(device)
    
    def evaluate_model(self, model, test_prompts: List[str], config) -> Dict[str, float]:
        """ëª¨ë¸ ì „ì²´ í‰ê°€"""
        print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        
        results = {}
        generated_samples = []
        
        # ìƒ˜í”Œ ìƒì„±
        for i, prompt in enumerate(test_prompts):
            print(f"ğŸ”„ ìƒì„± ì¤‘ ({i+1}/{len(test_prompts)}): {prompt}")
            
            try:
                # 3D ìƒì„±
                outputs = model.run(prompt, seed=42 + i)
                generated_samples.append({
                    'prompt': prompt,
                    'outputs': outputs,
                    'index': i
                })
            except Exception as e:
                print(f"âŒ ìƒì„± ì‹¤íŒ¨: {prompt} - {e}")
                continue
        
        if not generated_samples:
            print("âŒ ìƒì„±ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"âœ… {len(generated_samples)}ê°œ ìƒ˜í”Œ ìƒì„± ì™„ë£Œ")
        
        # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        if CLIP_AVAILABLE:
            results['clip_score'] = self.calculate_clip_score(generated_samples)
        
        if PYTORCH3D_AVAILABLE:
            results['chamfer_distance'] = self.calculate_chamfer_distance(generated_samples)
        
        # ë Œë”ë§ í’ˆì§ˆ í‰ê°€ (FID/KID)
        if TORCHMETRICS_AVAILABLE:
            fid_score, kid_score = self.calculate_image_metrics(generated_samples)
            results['fid_score'] = fid_score
            results['kid_score'] = kid_score
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        results['inference_time'] = self.measure_inference_time(model, test_prompts[:5])
        
        print("ğŸ“Š í‰ê°€ ì™„ë£Œ!")
        for key, value in results.items():
            print(f"  {key}: {value:.4f}")
        
        return results
    
    def calculate_clip_score(self, samples: List[Dict]) -> float:
        """CLIP Score ê³„ì‚°"""
        if not CLIP_AVAILABLE or not self.clip_model:
            return 0.0
        
        print("ğŸ” CLIP Score ê³„ì‚° ì¤‘...")
        clip_scores = []
        
        for sample in samples:
            try:
                prompt = sample['prompt']
                outputs = sample['outputs']
                
                # 3Dì—ì„œ 2D ë Œë”ë§ ìƒì„± (ê°„ë‹¨í•œ ì˜ˆì‹œ)
                rendered_images = self.render_3d_to_2d(outputs)
                
                if rendered_images is None:
                    continue
                
                # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
                text_tokens = clip.tokenize([prompt]).to(self.device)
                text_features = self.clip_model.encode_text(text_tokens)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # ì´ë¯¸ì§€ ì¸ì½”ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°
                image_scores = []
                for img in rendered_images:
                    img_preprocessed = self.clip_preprocess(img).unsqueeze(0).to(self.device)
                    img_features = self.clip_model.encode_image(img_preprocessed)
                    img_features = img_features / img_features.norm(dim=-1, keepdim=True)
                    
                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                    similarity = torch.cosine_similarity(text_features, img_features)
                    image_scores.append(similarity.item())
                
                if image_scores:
                    clip_scores.append(max(image_scores))  # ìµœê³  ì ìˆ˜ ì‚¬ìš©
                
            except Exception as e:
                print(f"âš ï¸ CLIP Score ê³„ì‚° ì˜¤ë¥˜: {e}")
                continue
        
        avg_clip_score = np.mean(clip_scores) if clip_scores else 0.0
        print(f"ğŸ“Š CLIP Score: {avg_clip_score:.4f}")
        return avg_clip_score
    
    def calculate_chamfer_distance(self, samples: List[Dict]) -> float:
        """Chamfer Distance ê³„ì‚°"""
        if not PYTORCH3D_AVAILABLE:
            return 0.0
        
        print("ğŸ” Chamfer Distance ê³„ì‚° ì¤‘...")
        cd_scores = []
        
        for sample in samples:
            try:
                outputs = sample['outputs']
                
                # 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì¶”ì¶œ
                pred_points = self.extract_point_cloud(outputs)
                if pred_points is None:
                    continue
                
                # ì°¸ì¡° í¬ì¸íŠ¸í´ë¼ìš°ë“œ (ë”ë¯¸ - ì‹¤ì œë¡œëŠ” GT ë°ì´í„° ì‚¬ìš©)
                ref_points = self.generate_reference_points(sample['prompt'])
                
                if pred_points is not None and ref_points is not None:
                    # PyTorch3D í¬ì¸íŠ¸í´ë¼ìš°ë“œ ìƒì„±
                    pred_pc = Pointclouds(points=[pred_points])
                    ref_pc = Pointclouds(points=[ref_points])
                    
                    # Chamfer Distance ê³„ì‚°
                    cd, _ = chamfer_distance(pred_pc, ref_pc)
                    cd_scores.append(cd.item())
                
            except Exception as e:
                print(f"âš ï¸ Chamfer Distance ê³„ì‚° ì˜¤ë¥˜: {e}")
                continue
        
        avg_cd = np.mean(cd_scores) if cd_scores else 0.0
        print(f"ğŸ“Š Chamfer Distance: {avg_cd:.4f}")
        return avg_cd
    
    def calculate_image_metrics(self, samples: List[Dict]) -> Tuple[float, float]:
        """FID/KID ê³„ì‚°"""
        if not TORCHMETRICS_AVAILABLE:
            return 0.0, 0.0
        
        print("ğŸ” FID/KID ê³„ì‚° ì¤‘...")
        
        try:
            # ìƒì„±ëœ ì´ë¯¸ì§€ë“¤
            generated_images = []
            for sample in samples:
                rendered = self.render_3d_to_2d(sample['outputs'])
                if rendered:
                    generated_images.extend(rendered)
            
            if len(generated_images) < 10:
                print("âš ï¸ FID/KID ê³„ì‚°ì— ì¶©ë¶„í•œ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0.0, 0.0
            
            # ì‹¤ì œ ì´ë¯¸ì§€ (ë”ë¯¸ - ì‹¤ì œë¡œëŠ” ì‹¤ì œ ë°ì´í„°ì…‹ ì‚¬ìš©)
            real_images = self.generate_reference_images(len(generated_images))
            
            # í…ì„œë¡œ ë³€í™˜
            gen_tensor = torch.stack([self.image_to_tensor(img) for img in generated_images])
            real_tensor = torch.stack([self.image_to_tensor(img) for img in real_images])
            
            # FID ê³„ì‚°
            self.fid.update(real_tensor, real=True)
            self.fid.update(gen_tensor, real=False)
            fid_score = self.fid.compute().item()
            
            # KID ê³„ì‚°
            self.kid.update(real_tensor, real=True)
            self.kid.update(gen_tensor, real=False)
            kid_score = self.kid.compute()[0].item()
            
            print(f"ğŸ“Š FID: {fid_score:.4f}, KID: {kid_score:.4f}")
            return fid_score, kid_score
            
        except Exception as e:
            print(f"âš ï¸ FID/KID ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0, 0.0
    
    def measure_inference_time(self, model, test_prompts: List[str]) -> float:
        """ì¶”ë¡  ì‹œê°„ ì¸¡ì •"""
        print("â±ï¸ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘...")
        
        times = []
        for prompt in test_prompts:
            try:
                torch.cuda.synchronize()
                start_time = time.time()
                
                _ = model.run(prompt, seed=42)
                
                torch.cuda.synchronize()
                end_time = time.time()
                
                times.append(end_time - start_time)
                
            except Exception as e:
                print(f"âš ï¸ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì˜¤ë¥˜: {e}")
                continue
        
        avg_time = np.mean(times) if times else 0.0
        print(f"ğŸ“Š í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.2f}ì´ˆ")
        return avg_time
    
    def render_3d_to_2d(self, outputs: Dict) -> Optional[List]:
        """3D ì¶œë ¥ì„ 2D ì´ë¯¸ì§€ë¡œ ë Œë”ë§ (ë”ë¯¸ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” TRELLISì˜ ë Œë”ë§ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ë”ë¯¸ ì´ë¯¸ì§€ ë°˜í™˜
        try:
            from PIL import Image
            dummy_image = Image.new('RGB', (224, 224), color='white')
            return [dummy_image] * 4  # 4ê°œ ë·°
        except:
            return None
    
    def extract_point_cloud(self, outputs: Dict) -> Optional[torch.Tensor]:
        """3D ì¶œë ¥ì—ì„œ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì¶”ì¶œ (ë”ë¯¸ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” TRELLISì˜ ë©”ì‹œ/ê°€ìš°ì‹œì•ˆì—ì„œ í¬ì¸íŠ¸ ì¶”ì¶œ
        try:
            # ë”ë¯¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ (1000ê°œ ì )
            points = torch.randn(1000, 3).to(self.device)
            return points
        except:
            return None
    
    def generate_reference_points(self, prompt: str) -> Optional[torch.Tensor]:
        """ì°¸ì¡° í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„± (ë”ë¯¸ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” GT ë°ì´í„° ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸ì˜ ì¶œë ¥ ì‚¬ìš©
        try:
            # ë”ë¯¸ ì°¸ì¡° í¬ì¸íŠ¸
            points = torch.randn(1000, 3).to(self.device)
            return points
        except:
            return None
    
    def generate_reference_images(self, count: int) -> List:
        """ì°¸ì¡° ì´ë¯¸ì§€ ìƒì„± (ë”ë¯¸ êµ¬í˜„)"""
        from PIL import Image
        images = []
        for _ in range(count):
            img = Image.new('RGB', (224, 224), color='gray')
            images.append(img)
        return images
    
    def image_to_tensor(self, image) -> torch.Tensor:
        """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        import torchvision.transforms as transforms
        
        transform = transforms.Compose([
            transforms.Resize((299, 299)),  # Inception í¬ê¸°
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        return transform(image).to(self.device)


def evaluate_model(pipeline, config) -> Dict[str, float]:
    """ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    test_prompts = [
        "a red sports car",
        "a wooden chair",
        "a blue coffee mug",
        "a small house",
        "a cute cat",
        "a modern desk lamp",
        "a vintage bicycle",
        "a glass vase with flowers"
    ]
    
    evaluator = TRELLISEvaluator()
    results = evaluator.evaluate_model(pipeline, test_prompts, config)
    
    return results


def calculate_model_efficiency(model: nn.Module) -> Dict[str, Any]:
    """ëª¨ë¸ íš¨ìœ¨ì„± ë©”íŠ¸ë¦­ ê³„ì‚°"""
    from utils.model_utils import get_trainable_parameters, get_model_memory_usage
    
    trainable_params, total_params = get_trainable_parameters(model)
    memory_info = get_model_memory_usage(model)
    
    efficiency = {
        'total_parameters': total_params,
        'trainable_parameters': trainable_params,
        'trainable_ratio': trainable_params / total_params if total_params > 0 else 0,
        'memory_mb': memory_info['total_mb'],
        'parameters_per_mb': total_params / memory_info['total_mb'] if memory_info['total_mb'] > 0 else 0
    }
    
    return efficiency


def compare_models(original_results: Dict, qlora_results: Dict) -> Dict[str, Any]:
    """ì›ë³¸ ëª¨ë¸ê³¼ QLoRA ëª¨ë¸ ë¹„êµ"""
    
    comparison = {
        'metrics_comparison': {},
        'efficiency_gain': {},
        'quality_retention': {}
    }
    
    # ë©”íŠ¸ë¦­ ë¹„êµ
    for metric in ['clip_score', 'chamfer_distance', 'fid_score', 'inference_time']:
        if metric in original_results and metric in qlora_results:
            original_val = original_results[metric]
            qlora_val = qlora_results[metric]
            
            # ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­ë“¤ (CD, FID, ì¶”ë¡ ì‹œê°„)
            if metric in ['chamfer_distance', 'fid_score', 'inference_time']:
                improvement = (original_val - qlora_val) / original_val * 100
            else:  # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­ (CLIP score)
                improvement = (qlora_val - original_val) / original_val * 100
            
            comparison['metrics_comparison'][metric] = {
                'original': original_val,
                'qlora': qlora_val,
                'improvement_percent': improvement
            }
    
    return comparison


def generate_evaluation_report(results: Dict[str, float], config, save_path: Path):
    """í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
    
    report_lines = []
    report_lines.append("=" * 50)
    report_lines.append("TRELLIS QLoRA í‰ê°€ ë¦¬í¬íŠ¸")
    report_lines.append("=" * 50)
    report_lines.append("")
    
    # ëª¨ë¸ ì •ë³´
    report_lines.append("ğŸ”§ ëª¨ë¸ ì •ë³´:")
    report_lines.append(f"  ëª¨ë¸: {config.model_name}")
    report_lines.append(f"  LoRA rank: {config.lora_rank}")
    report_lines.append(f"  LoRA alpha: {config.lora_alpha}")
    report_lines.append(f"  í›ˆë ¨ ìŠ¤í…: {config.max_steps}")
    report_lines.append("")
    
    # í‰ê°€ ê²°ê³¼
    report_lines.append("ğŸ“Š í‰ê°€ ê²°ê³¼:")
    for metric, value in results.items():
        if metric == 'clip_score':
            report_lines.append(f"  CLIP Score: {value:.4f} (â†‘ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        elif metric == 'chamfer_distance':
            report_lines.append(f"  Chamfer Distance: {value:.4f} (â†“ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        elif metric == 'fid_score':
            report_lines.append(f"  FID Score: {value:.4f} (â†“ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        elif metric == 'kid_score':
            report_lines.append(f"  KID Score: {value:.4f} (â†“ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        elif metric == 'inference_time':
            report_lines.append(f"  ì¶”ë¡  ì‹œê°„: {value:.2f}ì´ˆ (â†“ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        else:
            report_lines.append(f"  {metric}: {value:.4f}")
    report_lines.append("")
    
    # í’ˆì§ˆ í‰ê°€
    report_lines.append("ğŸ¯ í’ˆì§ˆ í‰ê°€:")
    if 'clip_score' in results:
        clip_score = results['clip_score']
        if clip_score >= 0.8:
            report_lines.append("  âœ… í…ìŠ¤íŠ¸-3D ì¼ì¹˜ë„: ìš°ìˆ˜")
        elif clip_score >= 0.6:
            report_lines.append("  ğŸŸ¡ í…ìŠ¤íŠ¸-3D ì¼ì¹˜ë„: ì–‘í˜¸")
        else:
            report_lines.append("  âŒ í…ìŠ¤íŠ¸-3D ì¼ì¹˜ë„: ê°œì„  í•„ìš”")
    
    if 'fid_score' in results:
        fid_score = results['fid_score']
        if fid_score <= 50:
            report_lines.append("  âœ… ë Œë”ë§ í’ˆì§ˆ: ìš°ìˆ˜")
        elif fid_score <= 100:
            report_lines.append("  ğŸŸ¡ ë Œë”ë§ í’ˆì§ˆ: ì–‘í˜¸")
        else:
            report_lines.append("  âŒ ë Œë”ë§ í’ˆì§ˆ: ê°œì„  í•„ìš”")
    
    report_lines.append("")
    report_lines.append("=" * 50)
    
    # íŒŒì¼ ì €ì¥
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"ğŸ“„ í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥: {save_path}")


class QualityMetrics:
    """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° í´ë˜ìŠ¤"""
    
    @staticmethod
    def calculate_mesh_quality(mesh_vertices: torch.Tensor, mesh_faces: torch.Tensor) -> Dict[str, float]:
        """ë©”ì‹œ í’ˆì§ˆ ë©”íŠ¸ë¦­"""
        if not PYTORCH3D_AVAILABLE:
            return {}
        
        try:
            # ê¸°ë³¸ ë©”ì‹œ í†µê³„
            num_vertices = mesh_vertices.shape[0]
            num_faces = mesh_faces.shape[0]
            
            # ì—£ì§€ ê¸¸ì´ ë¶„í¬
            edges = []
            for face in mesh_faces:
                for i in range(3):
                    v1, v2 = face[i], face[(i+1)%3]
                    edge_length = torch.norm(mesh_vertices[v1] - mesh_vertices[v2])
                    edges.append(edge_length.item())
            
            edge_stats = {
                'mean_edge_length': np.mean(edges),
                'std_edge_length': np.std(edges),
                'min_edge_length': np.min(edges),
                'max_edge_length': np.max(edges)
            }
            
            return {
                'num_vertices': num_vertices,
                'num_faces': num_faces,
                **edge_stats
            }
            
        except Exception as e:
            print(f"âš ï¸ ë©”ì‹œ í’ˆì§ˆ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {}
    
    @staticmethod
    def calculate_point_cloud_quality(points: torch.Tensor) -> Dict[str, float]:
        """í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í’ˆì§ˆ ë©”íŠ¸ë¦­"""
        try:
            num_points = points.shape[0]
            
            # ì¤‘ì‹¬ì 
            centroid = torch.mean(points, dim=0)
            
            # ê²½ê³„ ìƒì
            bbox_min = torch.min(points, dim=0)[0]
            bbox_max = torch.max(points, dim=0)[0]
            bbox_size = bbox_max - bbox_min
            
            # ë°€ë„ (k-nearest neighbors ê¸°ë°˜)
            from sklearn.neighbors import NearestNeighbors
            nbrs = NearestNeighbors(n_neighbors=10).fit(points.cpu().numpy())
            distances, _ = nbrs.kneighbors(points.cpu().numpy())
            avg_density = np.mean(distances[:, 1:])  # ìê¸° ìì‹  ì œì™¸
            
            return {
                'num_points': num_points,
                'bbox_volume': torch.prod(bbox_size).item(),
                'avg_point_density': avg_density,
                'centroid_x': centroid[0].item(),
                'centroid_y': centroid[1].item(),
                'centroid_z': centroid[2].item()
            }
            
        except Exception as e:
            print(f"âš ï¸ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í’ˆì§ˆ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {}


def run_comprehensive_evaluation(pipeline, config, save_dir: Path) -> Dict[str, Any]:
    """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€"""
    
    print("ğŸ” ì¢…í•© í‰ê°€ ì‹œì‘...")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # ê¸°ë³¸ í‰ê°€
    basic_results = evaluate_model(pipeline, config)
    
    # íš¨ìœ¨ì„± í‰ê°€
    if hasattr(pipeline, 'sparse_structure_decoder'):
        model = pipeline.sparse_structure_decoder
    else:
        model = next(iter(pipeline.models.values()))
    
    efficiency_results = calculate_model_efficiency(model)
    
    # ì¢…í•© ê²°ê³¼
    comprehensive_results = {
        'quality_metrics': basic_results,
        'efficiency_metrics': efficiency_results,
        'evaluation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'config_summary': {
            'model_name': config.model_name,
            'lora_rank': config.lora_rank,
            'lora_alpha': config.lora_alpha,
            'max_steps': config.max_steps
        }
    }
    
    # ê²°ê³¼ ì €ì¥
    import json
    results_file = save_dir / "comprehensive_evaluation.json"
    with open(results_file, 'w') as f:
        json.dump(comprehensive_results, f, indent=2)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report_file = save_dir / "evaluation_report.txt"
    generate_evaluation_report(basic_results, config, report_file)
    
    print("âœ… ì¢…í•© í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“Š ê²°ê³¼ ì €ì¥: {save_dir}")
    
    return comprehensive_results