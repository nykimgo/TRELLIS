# TRELLIS QLoRA Fine-tuning

TRELLIS ëª¨ë¸ì„ ìœ„í•œ QLoRA (Quantized Low-Rank Adaptation) fine-tuning êµ¬í˜„ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

- **QLoRA ê¸°ë°˜ fine-tuning**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ 4-bit ì–‘ìí™” + LoRA
- **ë¶„ì‚° í›ˆë ¨**: Multi-GPU (RTX 4090 Ã— 4) ì§€ì›
- **Mixed Precision**: FP16 ìë™ í˜¼í•© ì •ë°€ë„
- **ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬**: ìë™ ì €ì¥/ë¡œë“œ, EMA ì§€ì›
- **TensorBoard ë¡œê¹…**: ì‹¤ì‹œê°„ í›ˆë ¨ ëª¨ë‹ˆí„°ë§
- **ì¢…í•© í‰ê°€**: CLIP Score, FID, Chamfer Distance ë“±

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
trellis_qlora/
â”œâ”€â”€ main.py                    # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ qlora_config.py           # ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ qlora_trainer.py          # QLoRA íŠ¸ë ˆì´ë„ˆ
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ qlora_config.yaml     # ê¸°ë³¸ ì„¤ì • íŒŒì¼
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_loader.py        # ë°ì´í„°ë¡œë”
â”‚   â”œâ”€â”€ model_utils.py        # ëª¨ë¸ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ optimizer.py          # ì˜µí‹°ë§ˆì´ì €/ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”œâ”€â”€ logger.py             # ë¡œê¹… ìœ í‹¸ë¦¬í‹°
â”‚   â””â”€â”€ evaluation.py         # í‰ê°€ ë©”íŠ¸ë¦­
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_qlora.sh          # ë‹¨ì¼ GPU í›ˆë ¨
â”‚   â”œâ”€â”€ run_distributed.sh    # ë¶„ì‚° í›ˆë ¨
â”‚   â””â”€â”€ evaluate.py           # ëª¨ë¸ í‰ê°€
â””â”€â”€ README.md                 # ì‚¬ìš© ê°€ì´ë“œ
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# TRELLIS í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
cd /path/to/TRELLIS
mkdir trellis_qlora
cd trellis_qlora

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install peft bitsandbytes transformers accelerate
pip install tensorboard wandb  # ë¡œê¹… (ì„ íƒì‚¬í•­)
```

### 2. ê¸°ë³¸ í›ˆë ¨

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í›ˆë ¨
python main.py --model text-large

# ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ í›ˆë ¨
python main.py --config configs/qlora_config.yaml --model text-large --rank 32 --alpha 64

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python main.py --model text-large --max_steps 1000 --experiment_name quick_test
```

### 3. ë¶„ì‚° í›ˆë ¨ (4 GPUs)

```bash
# ë¶„ì‚° í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
bash scripts/run_distributed.sh

# ë˜ëŠ” torchrun ì‚¬ìš©
torchrun --nproc_per_node=4 main.py --model text-large --num_gpus 4
```

### 4. ëª¨ë¸ í‰ê°€

```bash
# ë‹¨ì¼ ëª¨ë¸ í‰ê°€
python scripts/evaluate.py --model_path ./qlora_experiments/best_model

# ì›ë³¸ vs QLoRA ë¹„êµ
python scripts/evaluate.py --compare \
    --original_path /home/sr/TRELLIS/microsoft/TRELLIS-text-large \
    --qlora_path ./qlora_experiments/best_model
```

## âš™ï¸ ì„¤ì • ì˜µì…˜

### ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|--------|------|
| `lora_rank` | 16 | LoRA rank (ë‚®ì„ìˆ˜ë¡ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ) |
| `lora_alpha` | 32 | LoRA scaling factor |
| `learning_rate` | 1e-4 | í•™ìŠµë¥  |
| `batch_size_per_gpu` | 2 | GPUë‹¹ ë°°ì¹˜ í¬ê¸° |
| `max_steps` | 10000 | ìµœëŒ€ í›ˆë ¨ ìŠ¤í… |
| `gradient_checkpointing` | true | ë©”ëª¨ë¦¬ ì ˆì•½ |

### QLoRA ì„¤ì •

```yaml
# ì–‘ìí™” ì„¤ì •
quantization_config:
  load_in_4bit: true                    # 4-bit ì–‘ìí™”
  bnb_4bit_compute_dtype: "float16"     # ê³„ì‚° ì •ë°€ë„
  bnb_4bit_use_double_quant: true       # ì´ì¤‘ ì–‘ìí™”
  bnb_4bit_quant_type: "nf4"           # ì–‘ìí™” íƒ€ì…

# LoRA íƒ€ê²Ÿ ëª¨ë“ˆ
lora_target_modules:
  - "q_proj"      # Query projection
  - "k_proj"      # Key projection  
  - "v_proj"      # Value projection
  - "o_proj"      # Output projection
```

## ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­

### í’ˆì§ˆ ë©”íŠ¸ë¦­
- **CLIP Score**: í…ìŠ¤íŠ¸-3D ì¼ì¹˜ë„ (â†‘ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **FID Score**: ë Œë”ë§ í’ˆì§ˆ (â†“ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **Chamfer Distance**: 3D ê¸°í•˜ ì •í™•ë„ (â†“ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)

### íš¨ìœ¨ì„± ë©”íŠ¸ë¦­
- **íŒŒë¼ë¯¸í„° ìˆ˜**: í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: GPU ë©”ëª¨ë¦¬ (MB)
- **ì¶”ë¡  ì‹œê°„**: ìƒ˜í”Œë‹¹ ìƒì„± ì‹œê°„ (ì´ˆ)

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹

```python
# utils/data_loader.py ìˆ˜ì •
class CustomDataset(TRELLISDataset):
    def _load_data(self):
        # ì»¤ìŠ¤í…€ ë°ì´í„° ë¡œë”© ë¡œì§
        return your_data_list
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```bash
# ë‚®ì€ rank (ë©”ëª¨ë¦¬ ì ˆì•½)
python main.py --rank 8 --alpha 16 --experiment_name low_rank

# ë†’ì€ rank (ì„±ëŠ¥ ìš°ì„ )
python main.py --rank 64 --alpha 128 --experiment_name high_rank

# í•™ìŠµë¥  ì‹¤í—˜
python main.py --learning_rate 2e-4 --experiment_name high_lr
```

### ëª¨ë‹ˆí„°ë§

```bash
# TensorBoard ì‹¤í–‰
tensorboard --logdir ./qlora_experiments/tensorboard

# ë¡œê·¸ í™•ì¸
tail -f ./qlora_experiments/logs/training.log
```

## ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼ ì˜ˆì‹œ

### RTX 4090 Ã— 4 ê¸°ì¤€

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ë©”ëª¨ë¦¬ | í›ˆë ¨ì‹œê°„ | CLIP Score |
|------|----------|--------|----------|------------|
| ì›ë³¸ (FP32) | 1.1B | ~18GB | - | 0.85 |
| QLoRA (r=16) | 16M | ~12GB | 2h | 0.82 |
| QLoRA (r=32) | 32M | ~13GB | 2.5h | 0.84 |

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

```
QLoRA ì¥ì :
âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 33% ê°ì†Œ
âœ… í›ˆë ¨ íŒŒë¼ë¯¸í„° 98% ê°ì†Œ  
âœ… ì¶”ë¡  ì†ë„ 15% í–¥ìƒ
ğŸŸ¡ í’ˆì§ˆ ì•½ê°„ ì €í•˜ (CLIP: 0.85â†’0.82)
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜

1. **GPU ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```bash
   export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
   # ë˜ëŠ” batch_size ì¤„ì´ê¸°
   ```

2. **ë¶„ì‚° í›ˆë ¨ ì˜¤ë¥˜**
   ```bash
   # ë°©í™”ë²½ í¬íŠ¸ í™•ì¸
   export MASTER_PORT=12356  # ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
   ```

3. **TRELLIS ì„í¬íŠ¸ ì˜¤ë¥˜**
   ```bash
   export PYTHONPATH="${PYTHONPATH}:$(pwd)/.."
   ```

### ì„±ëŠ¥ ìµœì í™”

```yaml
# ë©”ëª¨ë¦¬ ìµœì í™”
gradient_checkpointing: true
batch_size_per_gpu: 1
gradient_accumulation_steps: 8

# ì†ë„ ìµœì í™”
dataloader_num_workers: 8
dataloader_pin_memory: true
fp16: true
```

## ğŸ”„ ì›Œí¬í”Œë¡œìš°

### 1. ì‹¤í—˜ ì„¤ê³„
```bash
# ì„¤ì • íŒŒì¼ ë³µì‚¬ ë° ìˆ˜ì •
cp configs/qlora_config.yaml configs/my_experiment.yaml
# my_experiment.yaml í¸ì§‘
```

### 2. í›ˆë ¨ ì‹¤í–‰
```bash
# í›ˆë ¨ ì‹œì‘
python main.py --config configs/my_experiment.yaml
```

### 3. ê²°ê³¼ ë¶„ì„
```bash
# í‰ê°€ ì‹¤í–‰
python scripts/evaluate.py --model_path ./qlora_experiments/best_model

# TensorBoard í™•ì¸
tensorboard --logdir ./qlora_experiments/tensorboard
```

### 4. ëª¨ë¸ ë¹„êµ
```bash
# ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ ë¹„êµ
python scripts/evaluate.py --compare \
    --original_path /path/to/original \
    --qlora_path ./qlora_experiments/exp1/best_model
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [QLoRA ë…¼ë¬¸](https://arxiv.org/abs/2305.14314)
- [PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/huggingface/peft)
- [TRELLIS ê³µì‹ ì €ì¥ì†Œ](https://github.com/microsoft/TRELLIS)
- [BitsAndBytes ë¬¸ì„œ](https://github.com/TimDettmers/bitsandbytes)

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

ë²„ê·¸ ë¦¬í¬íŠ¸ë‚˜ ê°œì„  ì œì•ˆì€ ì–¸ì œë‚˜ í™˜ì˜í•©ë‹ˆë‹¤!

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ìˆìŠµë‹ˆë‹¤.