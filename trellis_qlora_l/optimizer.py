"""
λ¨λΈ μ ν‹Έλ¦¬ν‹° ν•¨μλ“¤

μ£Όμ” κΈ°λ¥:
- λ¨λΈ νλΌλ―Έν„° λ¶„μ„
- LoRA λ¨λ“ κ΄€λ¦¬
- λ¨λΈ μ •λ³΄ μ¶λ ¥
"""

import torch
import torch.nn as nn
from typing import Tuple, Dict, List, Any
from peft import PeftModel, LoraConfig


def get_trainable_parameters(model: nn.Module) -> Tuple[int, int]:
    """ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„° μ κ³„μ‚°"""
    trainable_params = 0
    total_params = 0
    
    for param in model.parameters():
        total_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    
    return trainable_params, total_params


def print_model_info(model: nn.Module, model_name: str = "Model"):
    """λ¨λΈ μ •λ³΄ μ¶λ ¥"""
    trainable_params, total_params = get_trainable_parameters(model)
    trainable_percentage = 100 * trainable_params / total_params if total_params > 0 else 0
    
    print(f"\nπ“ {model_name} μ •λ³΄:")
    print(f"  μ „μ²΄ νλΌλ―Έν„°: {total_params:,}")
    print(f"  ν›λ ¨ νλΌλ―Έν„°: {trainable_params:,}")
    print(f"  ν›λ ¨ λΉ„μ¨: {trainable_percentage:.2f}%")
    
    # λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¶”μ •
    if total_params > 0:
        # FP32 κΈ°μ¤€ λ©”λ¨λ¦¬ (λ°”μ΄νΈ)
        memory_fp32 = total_params * 4 / (1024**2)  # MB
        memory_fp16 = total_params * 2 / (1024**2)  # MB
        print(f"  λ©”λ¨λ¦¬ (FP32): {memory_fp32:.1f} MB")
        print(f"  λ©”λ¨λ¦¬ (FP16): {memory_fp16:.1f} MB")


def analyze_lora_modules(model: nn.Module) -> Dict[str, Any]:
    """LoRA λ¨λ“ λ¶„μ„"""
    lora_info = {
        "lora_modules": [],
        "base_modules": [],
        "total_lora_params": 0,
        "total_base_params": 0
    }
    
    for name, module in model.named_modules():
        if hasattr(module, 'lora_A') or hasattr(module, 'lora_B'):
            # LoRA λ¨λ“
            lora_params = sum(p.numel() for p in module.parameters() if 'lora' in str(p))
            lora_info["lora_modules"].append({
                "name": name,
                "type": type(module).__name__,
                "params": lora_params
            })
            lora_info["total_lora_params"] += lora_params
        
        elif len(list(module.parameters())) > 0:
            # λ² μ΄μ¤ λ¨λ“
            base_params = sum(p.numel() for p in module.parameters())
            lora_info["base_modules"].append({
                "name": name,
                "type": type(module).__name__,
                "params": base_params
            })
            lora_info["total_base_params"] += base_params
    
    return lora_info


def print_lora_info(model: nn.Module):
    """LoRA μ •λ³΄ μ¶λ ¥"""
    lora_info = analyze_lora_modules(model)
    
    print(f"\nπ”§ LoRA λ¨λ“ λ¶„μ„:")
    print(f"  LoRA λ¨λ“ μ: {len(lora_info['lora_modules'])}")
    print(f"  LoRA νλΌλ―Έν„°: {lora_info['total_lora_params']:,}")
    print(f"  λ² μ΄μ¤ νλΌλ―Έν„°: {lora_info['total_base_params']:,}")
    
    if lora_info['total_lora_params'] > 0:
        efficiency = lora_info['total_lora_params'] / (lora_info['total_lora_params'] + lora_info['total_base_params']) * 100
        print(f"  LoRA ν¨μ¨μ„±: {efficiency:.2f}%")
    
    # μƒμ„ LoRA λ¨λ“λ“¤ μ¶λ ¥
    if lora_info['lora_modules']:
        print(f"\n  μ£Όμ” LoRA λ¨λ“:")
        sorted_modules = sorted(lora_info['lora_modules'], key=lambda x: x['params'], reverse=True)
        for module in sorted_modules[:5]:  # μƒμ„ 5κ°λ§
            print(f"    - {module['name']}: {module['params']:,} νλΌλ―Έν„°")


def freeze_base_model(model: nn.Module):
    """λ² μ΄μ¤ λ¨λΈ νλΌλ―Έν„° λ™κ²°"""
    for name, param in model.named_parameters():
        if 'lora' not in name.lower():
            param.requires_grad = False
    
    print("π”’ λ² μ΄μ¤ λ¨λΈ νλΌλ―Έν„° λ™κ²° μ™„λ£")


def unfreeze_lora_parameters(model: nn.Module):
    """LoRA νλΌλ―Έν„°λ§ ν•΄λ™"""
    for name, param in model.named_parameters():
        if 'lora' in name.lower():
            param.requires_grad = True
    
    print("π”“ LoRA νλΌλ―Έν„° ν•΄λ™ μ™„λ£")


def get_model_memory_usage(model: nn.Module) -> Dict[str, float]:
    """λ¨λΈ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ κ³„μ‚°"""
    param_memory = 0
    buffer_memory = 0
    
    for param in model.parameters():
        param_memory += param.numel() * param.element_size()
    
    for buffer in model.buffers():
        buffer_memory += buffer.numel() * buffer.element_size()
    
    total_memory = param_memory + buffer_memory
    
    return {
        "parameters_mb": param_memory / (1024**2),
        "buffers_mb": buffer_memory / (1024**2),
        "total_mb": total_memory / (1024**2)
    }


def compare_models(original_model: nn.Module, lora_model: nn.Module):
    """μ›λ³Έ λ¨λΈκ³Ό LoRA λ¨λΈ λΉ„κµ"""
    print("\nπ“ λ¨λΈ λΉ„κµ:")
    
    # νλΌλ―Έν„° μ λΉ„κµ
    orig_trainable, orig_total = get_trainable_parameters(original_model)
    lora_trainable, lora_total = get_trainable_parameters(lora_model)
    
    print(f"  μ›λ³Έ λ¨λΈ:")
    print(f"    - μ „μ²΄: {orig_total:,}")
    print(f"    - ν›λ ¨: {orig_trainable:,}")
    
    print(f"  LoRA λ¨λΈ:")
    print(f"    - μ „μ²΄: {lora_total:,}")
    print(f"    - ν›λ ¨: {lora_trainable:,}")
    
    # λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λΉ„κµ
    orig_memory = get_model_memory_usage(original_model)
    lora_memory = get_model_memory_usage(lora_model)
    
    print(f"  λ©”λ¨λ¦¬ μ‚¬μ©λ‰:")
    print(f"    - μ›λ³Έ: {orig_memory['total_mb']:.1f} MB")
    print(f"    - LoRA: {lora_memory['total_mb']:.1f} MB")
    
    # ν¨μ¨μ„± κ³„μ‚°
    param_reduction = (1 - lora_trainable / orig_trainable) * 100 if orig_trainable > 0 else 0
    memory_reduction = (1 - lora_memory['total_mb'] / orig_memory['total_mb']) * 100 if orig_memory['total_mb'] > 0 else 0
    
    print(f"  ν¨μ¨μ„±:")
    print(f"    - ν›λ ¨ νλΌλ―Έν„° κ°μ†: {param_reduction:.1f}%")
    print(f"    - λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ°¨μ΄: {memory_reduction:.1f}%")


def find_target_modules(model: nn.Module, module_types: List[str] = None) -> List[str]:
    """LoRA μ μ© κ°€λ¥ν• νƒ€κ² λ¨λ“ μ°ΎκΈ°"""
    if module_types is None:
        module_types = ["Linear", "Conv1d", "Conv2d", "Conv3d"]
    
    target_modules = []
    
    for name, module in model.named_modules():
        module_type = type(module).__name__
        if module_type in module_types:
            # μΌλ°μ μΈ transformer ν¨ν„΄ ν™•μΈ
            if any(pattern in name.lower() for pattern in ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']):
                target_modules.append(name.split('.')[-1])  # λ§μ§€λ§‰ λ¶€λ¶„λ§
    
    # μ¤‘λ³µ μ κ±°
    target_modules = list(set(target_modules))
    
    print(f"π― λ°κ²¬λ νƒ€κ² λ¨λ“: {target_modules}")
    return target_modules


def calculate_lora_parameters(rank: int, input_dim: int, output_dim: int) -> int:
    """LoRA νλΌλ―Έν„° μ κ³„μ‚°"""
    # LoRA: W = W_0 + BA (B: output_dim x rank, A: rank x input_dim)
    lora_params = rank * (input_dim + output_dim)
    return lora_params


def estimate_lora_memory(model: nn.Module, lora_config: LoraConfig) -> Dict[str, float]:
    """LoRA λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¶”μ •"""
    total_lora_params = 0
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and any(target in name for target in lora_config.target_modules):
            in_features = module.in_features
            out_features = module.out_features
            lora_params = calculate_lora_parameters(lora_config.r, in_features, out_features)
            total_lora_params += lora_params
    
    # λ©”λ¨λ¦¬ κ³„μ‚° (FP16 κΈ°μ¤€)
    lora_memory_mb = total_lora_params * 2 / (1024**2)
    
    return {
        "lora_parameters": total_lora_params,
        "lora_memory_mb": lora_memory_mb
    }


def validate_lora_config(model: nn.Module, lora_config: LoraConfig) -> bool:
    """LoRA μ„¤μ • κ²€μ¦"""
    print("π” LoRA μ„¤μ • κ²€μ¦...")
    
    # νƒ€κ² λ¨λ“ μ΅΄μ¬ ν™•μΈ
    found_modules = []
    for name, module in model.named_modules():
        if any(target in name for target in lora_config.target_modules):
            found_modules.append(name)
    
    if not found_modules:
        print(f"β νƒ€κ² λ¨λ“μ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {lora_config.target_modules}")
        return False
    
    print(f"β… λ°κ²¬λ νƒ€κ² λ¨λ“: {len(found_modules)}κ°")
    
    # λ­ν¬ κ²€μ¦
    if lora_config.r <= 0:
        print(f"β μλ»λ LoRA rank: {lora_config.r}")
        return False
    
    # μ•ν κ²€μ¦
    if lora_config.lora_alpha <= 0:
        print(f"β μλ»λ LoRA alpha: {lora_config.lora_alpha}")
        return False
    
    print("β… LoRA μ„¤μ • κ²€μ¦ μ™„λ£")
    return True


def save_model_info(model: nn.Module, save_path: str):
    """λ¨λΈ μ •λ³΄λ¥Ό νμΌλ΅ μ €μ¥"""
    import json
    from pathlib import Path
    
    trainable_params, total_params = get_trainable_parameters(model)
    memory_info = get_model_memory_usage(model)
    lora_info = analyze_lora_modules(model)
    
    model_info = {
        "parameters": {
            "total": total_params,
            "trainable": trainable_params,
            "trainable_percentage": (trainable_params / total_params * 100) if total_params > 0 else 0
        },
        "memory": memory_info,
        "lora": {
            "modules_count": len(lora_info['lora_modules']),
            "lora_parameters": lora_info['total_lora_params'],
            "base_parameters": lora_info['total_base_params']
        },
        "architecture": str(model)
    }
    
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, indent=2, ensure_ascii=False)
    
    print(f"π’Ύ λ¨λΈ μ •λ³΄ μ €μ¥: {save_path}")


class ModelProfiler:
    """λ¨λΈ ν”„λ΅νμΌλ§ ν΄λμ¤"""
    
    def __init__(self, model: nn.Module):
        self.model = model
        self.hooks = []
        self.activations = {}
        self.gradients = {}
    
    def register_hooks(self):
        """ν›… λ“±λ΅"""
        def forward_hook(name):
            def hook(module, input, output):
                if isinstance(output, torch.Tensor):
                    self.activations[name] = {
                        'shape': output.shape,
                        'memory_mb': output.numel() * output.element_size() / (1024**2)
                    }
            return hook
        
        def backward_hook(name):
            def hook(module, grad_input, grad_output):
                if grad_output[0] is not None:
                    self.gradients[name] = {
                        'shape': grad_output[0].shape,
                        'memory_mb': grad_output[0].numel() * grad_output[0].element_size() / (1024**2)
                    }
            return hook
        
        for name, module in self.model.named_modules():
            if len(list(module.children())) == 0:  # leaf modules only
                self.hooks.append(module.register_forward_hook(forward_hook(name)))
                self.hooks.append(module.register_backward_hook(backward_hook(name)))
    
    def clear_hooks(self):
        """ν›… μ κ±°"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
    
    def get_profile_summary(self) -> Dict[str, Any]:
        """ν”„λ΅νμΌλ§ μ”μ•½"""
        total_activation_memory = sum(info['memory_mb'] for info in self.activations.values())
        total_gradient_memory = sum(info['memory_mb'] for info in self.gradients.values())
        
        return {
            'activation_memory_mb': total_activation_memory,
            'gradient_memory_mb': total_gradient_memory,
            'total_memory_mb': total_activation_memory + total_gradient_memory,
            'module_count': len(self.activations)
        }
    
    def __enter__(self):
        self.register_hooks()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.clear_hooks()